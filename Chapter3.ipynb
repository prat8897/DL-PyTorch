{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hyFTafKTD09x",
        "zyURq634FG6w",
        "M-yQO6oQFZGR",
        "198nHSu-IUVb"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMWPka2b14OanCw9Kys9hho",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prat8897/DL_PyTorch/blob/master/Chapter3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRKm8CKW-7nH",
        "colab_type": "text"
      },
      "source": [
        "This chapter covers\n",
        "- Understanding tensors, the basic data structure in PyTorch\n",
        "- Indexing and operating on tensors\n",
        "- Interoperating with NumPy multidimensional\n",
        "arrays\n",
        "- Moving computations to the GPU for speed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytAfWK3k_n9E",
        "colab_type": "text"
      },
      "source": [
        "Deep learning really consists of building a system that can transform data from one representation to another.\n",
        "\n",
        "In the context of deep learning, *tensors* refer to the generalization of vectors and matrices to an arbitrary number of dimensions. Another name for the same concept is *multidimensional array*. The dimensionality of a tensor coincides with the number of indexes used to refer to scalar values within the tensor.\n",
        "\n",
        "PyTorch features seamless interoperability with NumPy, which brings with it first-class integration with the rest of the scientific libraries in Python, such as SciPy (www.scipy.org), Scikit-learn (https://scikit-learn .org), and Pandas (https://pandas.pydata.org).\n",
        "\n",
        "Compared to NumPy arrays, PyTorch tensors have a few superpowers, such as the ability to perform very fast operations on graphical processing units (GPUs), distribute operations on multiple devices or machines, and keep track of the graph of computations that created them. These are all important features when implementing a modern deep learning library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyFTafKTD09x",
        "colab_type": "text"
      },
      "source": [
        "## From Python lists to PyTorch tensors\n",
        "\n",
        "Let’s see list indexing in action so we can compare it to tensor indexing. Take a list of three numbers in Python ([code/p1ch3/1_tensors.ipynb](https://github.com/deep-learning-with-pytorch/dlwpt-code/blob/master/p1ch3/1_tensors.ipynb)):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fcu1QBk-1ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = [1.0, 2.0, 1.0]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--7xmWvpEFwg",
        "colab_type": "text"
      },
      "source": [
        "We can access the first element of the list using the corresponding zero-based index:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR7wlyVqED-7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1ba9296-a223-4ade-b6dd-3a718a5a38b6"
      },
      "source": [
        "a[0]"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA3b4gPVEJNs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "624f667d-e708-48d1-c9c3-7bc527e9920c"
      },
      "source": [
        "a[2] = 3.0\n",
        "a"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0, 2.0, 3.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGgzVPeqEXig",
        "colab_type": "text"
      },
      "source": [
        "It is not unusual for simple Python programs dealing with vectors of numbers, such as the coordinates of a 2D line, to use Python lists to store the vectors. As we will see in the following chapter, using the more efficient tensor data structure, many types of data—from images to time series, and even sentences—can be represented. By defining operations over tensors, some of which we’ll explore in this chapter, we can slice and manipulate data expressively and efficiently at the same time, even from a high- level (and not particularly fast) language such as Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyURq634FG6w",
        "colab_type": "text"
      },
      "source": [
        "## Constructing Our First Tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxI92sXzEIpp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "06a02013-6a05-4083-c81a-86c894bd920e"
      },
      "source": [
        "import torch #imports torch module\n",
        "\n",
        "a = torch.ones(3) #Creates a 1-dimensional tensor with size 3, filled with 1s\n",
        "a"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HMJdpmPEuIr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82eb227f-8ff0-4f38-fc14-1ca736face54"
      },
      "source": [
        "a[1]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uww08GnEyaj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "deda745b-b518-4e97-bec0-d765125342f3"
      },
      "source": [
        "float(a[1])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWpxyXlQE0WS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e94665ba-ae40-4b2f-c817-bd244ca71465"
      },
      "source": [
        "a[2] = 2.0\n",
        "a"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 2.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bjFoXkuE8Pp",
        "colab_type": "text"
      },
      "source": [
        "After importing the `torch` module, we call a function that creates a (one-dimensional) tensor of size `3` filled with the value `1.0`. We can access an element using its zero-based index or assign a new value to it. Although on the surface this example doesn’t differ much from a list of number objects, under the hood things are completely different."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-yQO6oQFZGR",
        "colab_type": "text"
      },
      "source": [
        "## The essence of tensors\n",
        "\n",
        "Python `lists` or `tuples` of numbers are collections of Python objects that are individually allocated in memory. PyTorch `tensors` or `NumPy arrays`, on the other hand, are views over (typically) contiguous memory blocks containing unboxed C numeric types rather than Python objects. Each element is a 32-bit (4-byte) float in this case. This means storing a 1D tensor of 1,000,000 float numbers will require exactly 4,000,000 contiguous bytes, plus a small overhead for the metadata (such as dimensions and numeric type).\n",
        "\n",
        "\n",
        "Say we have a list of coordinates we’d like to use to represent a geometrical object: perhaps a 2D triangle with vertices at coordinates `(4, 1)`, `(5, 3)`, and `(2, 1)`. The example is not particularly pertinent to deep learning, but it’s easy to follow. Instead of having coordinates as numbers in a Python list, as we did earlier, we can use a one-dimensional tensor by storing Xs in the even indices and Ys in the odd indices, like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz0xa81RE3Ha",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5dfa8223-fbaa-4608-fb38-7718aedeb851"
      },
      "source": [
        "points = torch.zeros(6)\n",
        "points[0] = 4.0\n",
        "points[1] = 1.0\n",
        "points[2] = 5.0\n",
        "points[3] = 3.0\n",
        "points[4] = 2.0\n",
        "points[5] = 1.0\n",
        "\n",
        "points"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4., 1., 5., 3., 2., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNWUR_7pGofr",
        "colab_type": "text"
      },
      "source": [
        "We can also pass a Python list to the constructor, to the same effect:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4FXZgRQGjz7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "32af837e-2981-4d84-a05b-985b37e416e9"
      },
      "source": [
        "points = torch.tensor([4.0, 1.0, 5.0, 3.0, 2.0, 1.0])\n",
        "points"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4., 1., 5., 3., 2., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr_fadz3Gt_N",
        "colab_type": "text"
      },
      "source": [
        "To get the coordinates of the first point, we do the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucHA-bK4Gq9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "892f17a5-5365-4b3b-b7e9-38efd680d68b"
      },
      "source": [
        "float(points[0]), float(points[1])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4.0, 1.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeTeg9xsG14A",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "This is OK, although it would be practical to have the first index refer to individual 2D points rather than point coordinates. For this, we can use a 2D tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSzXYegNGvrq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c3eedc3b-59d1-4b13-ed26-ca45e47e36d7"
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "points"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4., 1.],\n",
              "        [5., 3.],\n",
              "        [2., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik72ws2FG7kU",
        "colab_type": "text"
      },
      "source": [
        "Here, we pass a list of lists to the constructor. We can ask the tensor about its shape:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSZQJzy8G5eQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26f596c8-4b5f-4905-a9f3-2e0ccbd7f129"
      },
      "source": [
        "points.shape\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPAFQuAaHBlT",
        "colab_type": "text"
      },
      "source": [
        "This informs us about the size of the tensor along each dimension i.e we have 2 dimensions each for a total of 3 points.\n",
        "\n",
        "\n",
        "We could also use zeros or ones to initialize the tensor, providing the size as a tuple:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNLDOF3hG9fH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1f761f51-efc1-4b38-f17b-f577d7353162"
      },
      "source": [
        "points = torch.zeros(3, 2)\n",
        "points"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLfObW_MHlsv",
        "colab_type": "text"
      },
      "source": [
        "Now we can access an individual element in the tensor using two indices:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKf0o2O9Hj3C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b6507506-290e-4ce0-8644-29960b7f0224"
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "points"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4., 1.],\n",
              "        [5., 3.],\n",
              "        [2., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H1WGIp6HoaB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd99ad42-0160-4c42-efe6-367b5c6c7fc0"
      },
      "source": [
        "points[0, 1]"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp_T2xvKHvs8",
        "colab_type": "text"
      },
      "source": [
        "This returns the Y-coordinate of the zeroth point in our dataset. We can also access the first element in the tensor as we did before to get the 2D coordinates of the first point:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUOXeDgKHtfJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70abdfd7-574f-4b35-8ede-8914b9f58cfc"
      },
      "source": [
        "points[0]"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqgC8xqxH8Mb",
        "colab_type": "text"
      },
      "source": [
        "The output is another tensor that presents a different view of the same underlying data. The new tensor is a 1D tensor of size 2, referencing the values of the first row in the points tensor. Does this mean a new chunk of memory was allocated, values were copied into it, and the new memory was returned wrapped in a new tensor object? No, because that would be very inefficient, especially if we had millions of points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "198nHSu-IUVb",
        "colab_type": "text"
      },
      "source": [
        "## Indexing tensors\n",
        "\n",
        "What if we need to obtain a tensor containing all points but the first? That’s easy using range indexing notation, which also applies to standard Python lists. Here’s a reminder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSJqtbETH1hI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "49b989b1-9b7e-4c92-a36f-a7e386d7d015"
      },
      "source": [
        "some_list = list(range(6))\n",
        "some_list"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGB6FTqKIk3R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "408a508f-efcd-45d9-df53-27913d882879"
      },
      "source": [
        "some_list[:] #All elements of the list"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeQQYNN8Io1I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "67eb8a19-b46c-4fb1-cad9-952e95e62d05"
      },
      "source": [
        "some_list[1:4] #From element 1 inclusive to element 4 exclusive"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIPMdKoNIurX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c67a5c4a-e892-43b2-f426-c6563b5c0686"
      },
      "source": [
        "some_list[1:] #From element 1 inclusive to the end of the list"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wZKnpxII0Q_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "828993e5-e71f-48a2-fa3c-7734264905df"
      },
      "source": [
        "some_list[:4] #From the start of the list to element 4 exclusive"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "741r0asEI5Le",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d0b2800e-9f4d-4978-8030-200922a68452"
      },
      "source": [
        "some_list[:-1] #From the start of the list to one before the last element"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUU7yXICJAgV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a50a4290-fadb-4ba3-8064-2d523c6c0e3e"
      },
      "source": [
        "some_list[1:4:2] #From element 1 inclusive to element 4 exclusive in steps of 2"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GdGMQlsJOIw",
        "colab_type": "text"
      },
      "source": [
        "To achieve our goal (What if we need to obtain a tensor containing all points but the first?), we can use the same notation for PyTorch tensors, with the added benefit that, just as in NumPy and other Python scientific libraries, we can use range indexing for each of the tensor’s dimensions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTkEYo8RJKkq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "ab4749eb-3456-4749-9c10-932015890f9d"
      },
      "source": [
        "points"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4., 1.],\n",
              "        [5., 3.],\n",
              "        [2., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21Xh3mBJJfmA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "11e40302-de91-446e-f521-094d6a498d0d"
      },
      "source": [
        "points[1:] #All rows after the first; implicitly all columns"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[5., 3.],\n",
              "        [2., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okCLtJX0JjWx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "9f6de2f8-71c3-4812-cd74-5d348eaa470d"
      },
      "source": [
        "points[1:, :] #All rows after the first; explicitly all columns"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[5., 3.],\n",
              "        [2., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sd88g6BJshv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a6e51138-87e5-4ab3-fa6b-6c47f6fbbe61"
      },
      "source": [
        "points[1:, 0] #All rows after the first; first column"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([5., 2.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtLArBekJx73",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "7fc85f48-d806-4129-e537-6b105c5a2dd8"
      },
      "source": [
        "points[None] #Adds a dimension of size 1, just like unsqueeze"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[4., 1.],\n",
              "         [5., 3.],\n",
              "         [2., 1.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lklPLYErJ9cF",
        "colab_type": "text"
      },
      "source": [
        "## Named tensors\n",
        "\n",
        "The dimensions (or axes) of our tensors usually index something like pixel locations or color channels. This means when we want to index into a tensor, we need to remember the ordering of the dimensions and write our indexing accordingly. As data is transformed through multiple tensors, keeping track of which dimension contains what data can be error-prone.\n",
        "\n",
        "To make things concrete, imagine that we have a 3D tensor like `img_t` from Chapter 2 (we will use dummy data for simplicity here), and we want to convert it to gray-scale. We looked up typical weights for the colors to derive a single brightness value:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmNU3dCyJ3l-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "0cdc30f1-b7d6-41e7-db8f-34df84c3fd89"
      },
      "source": [
        "img_t = torch.randn(3, 5, 5) # shape [channels, rows, columns]\n",
        "weights = torch.tensor([0.2126, 0.7152, 0.0722])\n",
        "img_t"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.4908, -0.9236, -1.8775, -0.1125,  0.3821],\n",
              "         [ 1.7600,  1.9684,  0.0066, -1.1974,  2.4375],\n",
              "         [-1.9211, -0.4184, -1.6118, -0.3680, -1.1099],\n",
              "         [-2.0732,  0.2650, -0.1579, -1.2324, -0.1139],\n",
              "         [-0.8073,  1.1427, -0.5344,  0.2231,  0.9041]],\n",
              "\n",
              "        [[-0.2148, -0.3572, -0.6933,  0.6115, -0.3495],\n",
              "         [-1.3418, -1.8093, -0.1345, -0.1675,  0.9695],\n",
              "         [ 0.3177,  0.3681,  1.6774,  2.0129, -0.6489],\n",
              "         [ 2.0833,  0.2066,  0.2774, -0.4641,  2.6546],\n",
              "         [ 0.0495, -0.9592,  1.2016,  1.0830,  1.2897]],\n",
              "\n",
              "        [[-0.2775, -2.1811, -0.9999,  0.1867, -1.6738],\n",
              "         [-1.1349,  0.2197, -0.0189, -0.2675,  1.5478],\n",
              "         [ 0.6625, -0.0145, -1.5767, -1.5972,  0.8957],\n",
              "         [-0.6628,  0.3079,  0.2111, -2.1949,  0.1105],\n",
              "         [ 0.6951,  0.7535,  1.0956, -0.9477,  1.9169]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqfvBtFWKuDl",
        "colab_type": "text"
      },
      "source": [
        "We also often want our code to generalize—for example, from grayscale images represented as 2D tensors with height and width dimensions to color images adding a third channel dimension (as in RGB), or from a single image to a batch of images. In Chapter 2, we introduced an additional batch dimension in `batch_t`; here we pretend to have a batch of 2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0qGJspjKYnB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "465b6705-d7f7-4143-ee03-8cfef1dea962"
      },
      "source": [
        "batch_t = torch.randn(2, 3, 5, 5) # shape [batch, channels, rows, columns]\n",
        "batch_t"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-2.2263, -2.0457, -0.6210,  0.5475, -1.7689],\n",
              "          [-2.1972, -0.1958, -0.6730,  1.6901,  0.3577],\n",
              "          [ 0.7147, -0.6981,  1.1645,  0.1658, -1.1968],\n",
              "          [-1.3056, -0.5509, -0.9069,  0.3316,  0.2204],\n",
              "          [-0.3466, -0.5804, -1.1529, -0.0614,  0.4614]],\n",
              "\n",
              "         [[ 0.0676,  0.3888, -0.9560, -1.9303, -1.9234],\n",
              "          [-0.8344, -0.8868, -0.1357, -0.1018,  1.3273],\n",
              "          [-1.6775,  0.0770, -0.4906, -0.5011, -0.1115],\n",
              "          [-0.3862,  0.5039, -1.3555, -1.6612, -0.2368],\n",
              "          [-0.1960,  1.2312, -0.9096, -1.5027, -0.3757]],\n",
              "\n",
              "         [[ 1.1961, -2.2081, -0.7627, -0.4595,  0.2581],\n",
              "          [ 1.5618,  0.3051, -0.0406, -1.3396,  0.1423],\n",
              "          [-0.3616, -1.6760,  0.7677,  1.4149, -0.9568],\n",
              "          [ 0.4904, -1.0562, -0.5624, -0.8620,  0.0377],\n",
              "          [-0.2996, -0.4733, -1.4669,  0.2975, -0.1048]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0209,  0.1161,  0.0762,  0.0722, -1.4618],\n",
              "          [-1.0744, -0.7705,  0.2472, -0.0775,  0.2777],\n",
              "          [-1.4294, -0.0107,  0.2003, -0.0404,  1.3810],\n",
              "          [ 0.4265,  0.1014, -0.9682,  0.4960,  0.2425],\n",
              "          [-0.9712,  1.0997,  0.0182,  1.4172, -0.2557]],\n",
              "\n",
              "         [[-1.2763,  0.8557, -0.1951,  0.6286,  0.1661],\n",
              "          [ 0.5515,  0.4940, -2.3214,  1.1872,  0.4665],\n",
              "          [-1.6352, -0.0869,  1.4317, -0.3168,  0.8967],\n",
              "          [ 0.2738, -0.7794, -0.9361, -1.4405,  0.2701],\n",
              "          [ 1.6651, -0.3142, -2.0537, -2.4802,  0.2850]],\n",
              "\n",
              "         [[-1.0125, -0.9052,  1.1828, -0.7246,  1.5714],\n",
              "          [-0.6011,  0.2834,  0.3309,  0.2379,  0.3814],\n",
              "          [ 0.5445, -0.6440, -1.4819,  0.7215, -0.5156],\n",
              "          [ 1.2717, -0.6424,  1.7608, -0.2223, -0.2073],\n",
              "          [-0.4959,  0.4111,  0.2914,  1.3652, -0.7249]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Il3PwaBK7Xc",
        "colab_type": "text"
      },
      "source": [
        "So sometimes the RGB channels are in dimension 0, and sometimes they are in dimension 1. But we can generalize by counting from the end: they are always in dimension –3, the third from the end. The lazy, unweighted [mean](https://pytorch.org/docs/master/generated/torch.mean.html) (R+G+B)/3 can thus be written as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2S5D6W4K2-k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "98af59b5-adfd-40de-9eac-496eb947290f"
      },
      "source": [
        "img_gray_naive = img_t.mean(-3)\n",
        "batch_gray_naive = batch_t.mean(-3)\n",
        "img_gray_naive, img_gray_naive.shape"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.3277, -1.1539, -1.1902,  0.2286, -0.5471],\n",
              "         [-0.2389,  0.1263, -0.0489, -0.5442,  1.6516],\n",
              "         [-0.3136, -0.0216, -0.5037,  0.0159, -0.2877],\n",
              "         [-0.2175,  0.2598,  0.1102, -1.2971,  0.8837],\n",
              "         [-0.0209,  0.3123,  0.5876,  0.1195,  1.3702]]), torch.Size([5, 5]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgTy4XtHaRie",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "fe123beb-808f-4e06-f01e-55de5bf65480"
      },
      "source": [
        " batch_gray_naive, batch_gray_naive.shape"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[-0.3209, -1.2883, -0.7799, -0.6141, -1.1447],\n",
              "          [-0.4899, -0.2592, -0.2831,  0.0829,  0.6091],\n",
              "          [-0.4414, -0.7657,  0.4806,  0.3599, -0.7550],\n",
              "          [-0.4005, -0.3678, -0.9416, -0.7305,  0.0071],\n",
              "          [-0.2807,  0.0592, -1.1765, -0.4222, -0.0064]],\n",
              " \n",
              "         [[-0.7560,  0.0222,  0.3546, -0.0079,  0.0919],\n",
              "          [-0.3747,  0.0023, -0.5811,  0.4492,  0.3752],\n",
              "          [-0.8400, -0.2472,  0.0501,  0.1214,  0.5874],\n",
              "          [ 0.6574, -0.4401, -0.0478, -0.3889,  0.1018],\n",
              "          [ 0.0660,  0.3989, -0.5814,  0.1007, -0.2319]]]),\n",
              " torch.Size([2, 5, 5]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0kgld0FMyV-",
        "colab_type": "text"
      },
      "source": [
        "But now we have the weight, too. PyTorch will allow us to multiply things that are the same shape, as well as shapes where one operand is of size 1 in a given dimension. It also appends leading dimensions of size 1 automatically. This is a feature called [broadcasting](https://stackoverflow.com/questions/51371070/how-does-pytorch-broadcasting-work). `batch_t` of shape `(2, 3, 5, 5)` is multiplied by *unsqueezed_weights* of shape `(3, 1, 1)`, resulting in a tensor of shape `(2, 3, 5, 5)`, from which we can then sum the third dimension from the end (the three channels):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_wsXQ90fG5t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1bc22a3-561c-4327-9775-95672e0e4f82"
      },
      "source": [
        "weights"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2126, 0.7152, 0.0722])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNTEtmPpLCML",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "04781869-9a15-4315-8a94-50cb207330a6"
      },
      "source": [
        "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze_(-1)\n",
        "unsqueezed_weights"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.2126]],\n",
              "\n",
              "        [[0.7152]],\n",
              "\n",
              "        [[0.0722]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A957UfLAdPVM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "f236cfed-178e-4f97-f31d-a1db26557eeb"
      },
      "source": [
        "img_weights = (img_t * unsqueezed_weights)\n",
        "img_weights"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.0435e-01, -1.9635e-01, -3.9916e-01, -2.3924e-02,  8.1233e-02],\n",
              "         [ 3.7418e-01,  4.1847e-01,  1.3974e-03, -2.5457e-01,  5.1820e-01],\n",
              "         [-4.0843e-01, -8.8961e-02, -3.4267e-01, -7.8228e-02, -2.3596e-01],\n",
              "         [-4.4076e-01,  5.6331e-02, -3.3562e-02, -2.6202e-01, -2.4212e-02],\n",
              "         [-1.7163e-01,  2.4293e-01, -1.1361e-01,  4.7432e-02,  1.9222e-01]],\n",
              "\n",
              "        [[-1.5363e-01, -2.5548e-01, -4.9585e-01,  4.3736e-01, -2.4994e-01],\n",
              "         [-9.5968e-01, -1.2940e+00, -9.6168e-02, -1.1982e-01,  6.9342e-01],\n",
              "         [ 2.2722e-01,  2.6323e-01,  1.1997e+00,  1.4396e+00, -4.6411e-01],\n",
              "         [ 1.4900e+00,  1.4773e-01,  1.9842e-01, -3.3189e-01,  1.8986e+00],\n",
              "         [ 3.5423e-02, -6.8604e-01,  8.5939e-01,  7.7458e-01,  9.2238e-01]],\n",
              "\n",
              "        [[-2.0034e-02, -1.5747e-01, -7.2195e-02,  1.3483e-02, -1.2085e-01],\n",
              "         [-8.1943e-02,  1.5865e-02, -1.3617e-03, -1.9317e-02,  1.1175e-01],\n",
              "         [ 4.7832e-02, -1.0461e-03, -1.1384e-01, -1.1532e-01,  6.4671e-02],\n",
              "         [-4.7853e-02,  2.2231e-02,  1.5238e-02, -1.5847e-01,  7.9756e-03],\n",
              "         [ 5.0186e-02,  5.4403e-02,  7.9106e-02, -6.8425e-02,  1.3840e-01]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwuA6hzifk-b",
        "colab_type": "text"
      },
      "source": [
        "`batch_t` of shape `(2, 3, 5, 5)` is multiplied by unsqueezed_weights of shape `(3, 1, 1)`, resulting in a tensor of shape `(2, 3, 5, 5)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4jYdpfLepOL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "b3d0ef6a-4c61-4a11-b1ea-89774e809055"
      },
      "source": [
        "batch_weights = (batch_t * unsqueezed_weights)\n",
        "batch_weights, batch_weights.shape"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[[-0.4733, -0.4349, -0.1320,  0.1164, -0.3761],\n",
              "           [-0.4671, -0.0416, -0.1431,  0.3593,  0.0760],\n",
              "           [ 0.1520, -0.1484,  0.2476,  0.0353, -0.2544],\n",
              "           [-0.2776, -0.1171, -0.1928,  0.0705,  0.0469],\n",
              "           [-0.0737, -0.1234, -0.2451, -0.0131,  0.0981]],\n",
              " \n",
              "          [[ 0.0483,  0.2781, -0.6837, -1.3806, -1.3756],\n",
              "           [-0.5968, -0.6343, -0.0970, -0.0728,  0.9493],\n",
              "           [-1.1997,  0.0551, -0.3509, -0.3584, -0.0797],\n",
              "           [-0.2762,  0.3604, -0.9694, -1.1881, -0.1694],\n",
              "           [-0.1402,  0.8806, -0.6506, -1.0747, -0.2687]],\n",
              " \n",
              "          [[ 0.0864, -0.1594, -0.0551, -0.0332,  0.0186],\n",
              "           [ 0.1128,  0.0220, -0.0029, -0.0967,  0.0103],\n",
              "           [-0.0261, -0.1210,  0.0554,  0.1022, -0.0691],\n",
              "           [ 0.0354, -0.0763, -0.0406, -0.0622,  0.0027],\n",
              "           [-0.0216, -0.0342, -0.1059,  0.0215, -0.0076]]],\n",
              " \n",
              " \n",
              "         [[[ 0.0045,  0.0247,  0.0162,  0.0154, -0.3108],\n",
              "           [-0.2284, -0.1638,  0.0525, -0.0165,  0.0590],\n",
              "           [-0.3039, -0.0023,  0.0426, -0.0086,  0.2936],\n",
              "           [ 0.0907,  0.0216, -0.2058,  0.1055,  0.0516],\n",
              "           [-0.2065,  0.2338,  0.0039,  0.3013, -0.0544]],\n",
              " \n",
              "          [[-0.9128,  0.6120, -0.1395,  0.4496,  0.1188],\n",
              "           [ 0.3944,  0.3533, -1.6602,  0.8491,  0.3336],\n",
              "           [-1.1695, -0.0621,  1.0240, -0.2266,  0.6413],\n",
              "           [ 0.1958, -0.5574, -0.6695, -1.0302,  0.1931],\n",
              "           [ 1.1909, -0.2247, -1.4688, -1.7738,  0.2038]],\n",
              " \n",
              "          [[-0.0731, -0.0654,  0.0854, -0.0523,  0.1135],\n",
              "           [-0.0434,  0.0205,  0.0239,  0.0172,  0.0275],\n",
              "           [ 0.0393, -0.0465, -0.1070,  0.0521, -0.0372],\n",
              "           [ 0.0918, -0.0464,  0.1271, -0.0160, -0.0150],\n",
              "           [-0.0358,  0.0297,  0.0210,  0.0986, -0.0523]]]]),\n",
              " torch.Size([2, 3, 5, 5]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZzYt2I0fwfX",
        "colab_type": "text"
      },
      "source": [
        "from which we can then sum the third dimension from the end (the three channels):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DENWOqHxf5k8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "00f130d3-304f-4acf-f852-b77835b799a3"
      },
      "source": [
        "img_gray_weighted = img_weights.sum(-3)\n",
        "img_gray_weighted, img_gray_weighted.shape"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.2780, -0.6093, -0.9672,  0.4269, -0.2896],\n",
              "         [-0.6674, -0.8597, -0.0961, -0.3937,  1.3234],\n",
              "         [-0.1334,  0.1732,  0.7432,  1.2461, -0.6354],\n",
              "         [ 1.0014,  0.2263,  0.1801, -0.7524,  1.8823],\n",
              "         [-0.0860, -0.3887,  0.8249,  0.7536,  1.2530]]), torch.Size([5, 5]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktV9zaJBfDnj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "650fb827-afa0-485e-a1af-8a441817ebab"
      },
      "source": [
        "batch_gray_weighted = batch_weights.sum(-3)\n",
        "batch_gray_weighted, batch_gray_weighted.shape"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[-0.3386, -0.3162, -0.8708, -1.2973, -1.7330],\n",
              "          [-0.9511, -0.6539, -0.2430,  0.1898,  1.0356],\n",
              "          [-1.0739, -0.2143, -0.0478, -0.2210, -0.4033],\n",
              "          [-0.5184,  0.1670, -1.2029, -1.1798, -0.1198],\n",
              "          [-0.2355,  0.7230, -1.0016, -1.0663, -0.1782]],\n",
              " \n",
              "         [[-0.9814,  0.5713, -0.0379,  0.4126, -0.0786],\n",
              "          [ 0.1226,  0.2100, -1.5838,  0.8498,  0.4202],\n",
              "          [-1.4340, -0.1109,  0.9596, -0.1831,  0.8977],\n",
              "          [ 0.3783, -0.5823, -0.7482, -0.9408,  0.2297],\n",
              "          [ 0.9486,  0.0388, -1.4439, -1.3740,  0.0971]]]),\n",
              " torch.Size([2, 5, 5]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSAZ2G1xfTbh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a937870-4881-41db-a682-449dafdc458d"
      },
      "source": [
        "batch_weights.shape, batch_t.shape, unsqueezed_weights.shape"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 3, 5, 5]), torch.Size([2, 3, 5, 5]), torch.Size([3, 1, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09GwcmDkgZFH",
        "colab_type": "text"
      },
      "source": [
        "Because this gets messy quickly—and for the sake of efficiency—the PyTorch function `einsum` (adapted from NumPy) specifies an indexing mini-language giving index names to dimensions for sums of such products. As often in Python, broadcasting—a form of summarizing unnamed things—is done using three dots '...'; but don’t worry too much about `einsum`, because we will not use it in the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJAJ6ds6gJxn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c18d8ed8-f9fb-4057-8a59-4d26e4ff0996"
      },
      "source": [
        "img_gray_weighted_fancy = torch.einsum('...chw,c->...hw', img_t, weights)\n",
        "batch_gray_weighted_fancy = torch.einsum('...chw,c->...hw', batch_t, weights)\n",
        "batch_gray_weighted_fancy.shape"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 5, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UItrsogAg0CY",
        "colab_type": "text"
      },
      "source": [
        "As we can see, there is quite a lot of bookkeeping involved. This is error-prone, especially when the locations where tensors are created and used are far apart in our code. This has caught the eye of practitioners, and so it has been suggested that the dimension be given a name instead.\n",
        "PyTorch 1.3 added named tensors as an experimental feature (see https://pytorch.org/tutorials/intermediate/named_tensor_tutorial.html and https://pytorch.org/docs/stable/named_tensor.html). Tensor factory functions such as `tensor` and `rand` take a `names` argument. The names should be a sequence of strings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmNagbowgpcF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75ef5f61-d1ec-4ace-f6d0-72f8cb53d936"
      },
      "source": [
        "weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels'])\n",
        "weights_named"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2126, 0.7152, 0.0722], names=('channels',))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY9TbkmDhMGQ",
        "colab_type": "text"
      },
      "source": [
        "Note: PyTorch recommends that Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE9YL-anhddl",
        "colab_type": "text"
      },
      "source": [
        "When we already have a tensor and want to add names (but not change existing ones), we can call the method `refine_names` on it. Similar to indexing, the ellipsis (...) allows you to leave out any number of dimensions. With the rename sibling method, you can also overwrite or drop (by passing in None) existing names:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGUQL18MhEQR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a7fbe551-31cb-4c57-a0fe-fb9179cb2b5e"
      },
      "source": [
        "img_named = img_t.refine_names(..., 'channels', 'rows', 'columns')\n",
        "batch_named = batch_t.refine_names(..., 'channels', 'rows', 'columns')\n",
        "print(\"img named:\", img_named.shape, img_named.names)\n",
        "print(\"batch named:\", batch_named.shape, batch_named.names)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "img named: torch.Size([3, 5, 5]) ('channels', 'rows', 'columns')\n",
            "batch named: torch.Size([2, 3, 5, 5]) (None, 'channels', 'rows', 'columns')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYFavXzVh34_",
        "colab_type": "text"
      },
      "source": [
        "For operations with two inputs, in addition to the usual dimension checks—whether sizes are the same, or if one is 1 and can be broadcast to the other—PyTorch will now check the names for us. So far, it does not automatically align dimensions, so we need to do this explicitly. The method `align_as` returns a tensor with missing dimensions added and existing ones permuted to the right order:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjVTKD5uhofM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ffd6d158-fcf7-4fc8-b56f-fdf744a5ae27"
      },
      "source": [
        "weights_aligned = weights_named.align_as(img_named)\n",
        "weights_aligned.shape, weights_aligned.names"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 1, 1]), ('channels', 'rows', 'columns'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48DXdNUPiLV1",
        "colab_type": "text"
      },
      "source": [
        "Functions accepting dimension arguments, like `sum`, also take named dimensions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMgHamnbiI0I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "edc54d0d-de02-4930-f441-3a2945a4eb77"
      },
      "source": [
        "gray_named = (img_named * weights_aligned).sum('channels')\n",
        "gray_named.shape, gray_named.names"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5, 5]), ('rows', 'columns'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIulhr3hiVwh",
        "colab_type": "text"
      },
      "source": [
        "If we try to combine dimensions with different names, we get an error:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0LPZQZEiOk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#gray_named = (img_named[..., :3] * weights_named).sum('channels')"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxymlLPzidbZ",
        "colab_type": "text"
      },
      "source": [
        "If we want to use tensors outside functions that operate on named tensors, we need to drop the names by renaming them to None. The following gets us back into the world of unnamed dimensions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asouU73jiYuN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8839abe-6bf4-4e84-bb42-a33dacacd62d"
      },
      "source": [
        "gray_plain = gray_named.rename(None)\n",
        "gray_plain.shape, gray_plain.names"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5, 5]), (None, None))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGxIIYJgipnZ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Given the experimental nature of this feature at the time of writing, and to avoid mucking around with indexing and alignment, we will stick to unnamed in the remainder of the book. Named tensors have the potential to eliminate many sources of alignment errors, which—if the PyTorch forum is any indication—can be a source of headaches. It will be interesting to see how widely they will be adopted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uUUhfR9issT",
        "colab_type": "text"
      },
      "source": [
        "## Tensor Element Types\n",
        "\n",
        "So far, we have covered the basics of how tensors work, but we have not yet touched on what kinds of numeric types we can store in a Tensor. As we hinted at in section 3.2, using the standard Python numeric types can be suboptimal for several reasons:\n",
        "- Numbers in Python are objects. Whereas a floating-point number might require only, for instance, 32 bits to be represented on a computer, Python will convert it into a full-fledged Python object with reference counting, and so on. This operation, called boxing, is not a problem if we need to store a small number of numbers, but allocating millions gets very inefficient.\n",
        "- Lists in Python are meant for sequential collections of objects. There are no operations defined for, say, efficiently taking the dot product of two vectors, or summing vec- tors together. Also, Python lists have no way of optimizing the layout of their con- tents in memory, as they are indexable collections of pointers to Python objects (of any kind, not just numbers). Finally, Python lists are one-dimensional, and although we can create lists of lists, this is again very inefficient.\n",
        "- The Python interpreter is slow compared to optimized, compiled code. Performing math- ematical operations on large collections of numerical data can be much faster using optimized code written in a compiled, low-level language like C.\n",
        "\n",
        "For these reasons, data science libraries rely on NumPy or introduce dedicated data structures like PyTorch tensors, which provide efficient low-level implementations of numerical data structures and related operations on them, wrapped in a convenient high-level API. To enable this, the objects within a tensor must all be numbers of the same type, and PyTorch must keep track of this numeric type."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH-v7Ox6kOlo",
        "colab_type": "text"
      },
      "source": [
        "### Specifying the numeric type with dtype\n",
        "\n",
        "\n",
        "The `dtype` argument to tensor constructors (that is, functions like tensor, zeros, and ones) specifies the numerical data (d) type that will be contained in the tensor. The data type specifies the possible values the tensor can hold (integers versus floating-point numbers) and the number of bytes per value. The `dtype` argument is deliberately similar to the standard NumPy argument of the same name. Here's a list of all the possible values for the `dtype` argument:\n",
        "- `torch.float32` or `torch.float`: 32-bit floating-point\n",
        "- `torch.float64` or `torch.double`: 64-bit, double-precision floating-point\n",
        "- `torch.float16` or `torch.half`: 16-bit, half-precision floating-point\n",
        "- `torch.int8`: signed 8-bit integers\n",
        "- `torch.uint8`: unsigned 8-bit integers\n",
        "- `torch.int16` or `torch.short`: signed 16-bit integers\n",
        "- `torch.int32` or `torch.int`: signed 32-bit integers\n",
        "- `torch.int64` or `torch.long`: signed 64-bit integers\n",
        "- `torch.bool`: Boolean\n",
        "\n",
        "The default data type for tensors is 32-bit floating-point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc01rGx8lxo1",
        "colab_type": "text"
      },
      "source": [
        "### A `dtype` for every occasion\n",
        "\n",
        "As we will see in future chapters, computations happening in neural networks are typically executed with 32-bit floating-point precision. Higher precision, like 64-bit, will not buy improvements in the accuracy of a model and will require more memory and computing time. The 16-bit floating-point, half-precision data type is not present natively in standard CPUs, but it is offered on modern GPUs. It is possible to switch to half-precision to decrease the footprint of a neural network model if needed, with a minor impact on accuracy.\n",
        "\n",
        "\n",
        "Tensors can be used as indexes in other tensors. In this case, PyTorch expects indexing tensors to have a 64-bit integer data type. Creating a tensor with integers as arguments, such as using `torch.tensor([2, 2])`, will create a 64-bit integer tensor by default. As such, we’ll spend most of our time dealing with `float32` and `int64`.\n",
        "\n",
        "Finally, predicates on tensors, such as `points > 1.0`, produce `bool` tensors indicating whether each individual element satisfies the condition. These are the numeric types in a nutshell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBqw1sD2mNr2",
        "colab_type": "text"
      },
      "source": [
        "### Managing a tensor’s `dtype` attribute\n",
        "\n",
        "In order to allocate a tensor of the right numeric type, we can specify the proper `dtype` as an argument to the constructor. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vl1Z5LbmQQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "double_points = torch.ones(10, 2, dtype=torch.double)\n",
        "short_points = torch.tensor([[1, 2], [3, 4]], dtype=torch.short)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66KJqY7tmbPL",
        "colab_type": "text"
      },
      "source": [
        "We can find out about the dtype for a tensor by accessing the corresponding attribute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz8r6-DLmZOJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9f6ec3a3-cb2b-495a-9541-4a562bcc2128"
      },
      "source": [
        "short_points.dtype"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.int16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DN_kx6hmgJB",
        "colab_type": "text"
      },
      "source": [
        "We can also cast the output of a tensor creation function to the right type using the corresponding casting method, such as"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "San3aZBsmdnw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "beb7ceaf-e34c-4968-806b-091163197b0c"
      },
      "source": [
        "double_points = torch.zeros(10, 2).double()\n",
        "short_points = torch.ones(10, 2).short()\n",
        "double_points.dtype, short_points.dtype"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.float64, torch.int16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKYNhaz8mzHs",
        "colab_type": "text"
      },
      "source": [
        "or the more convenient to method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JPJ9EqAmkRw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d317ff2d-04ad-4c73-dbc7-f28d96b19364"
      },
      "source": [
        "double_points = torch.zeros(10, 2).to(torch.double)\n",
        "short_points = torch.ones(10, 2).to(dtype=torch.short)\n",
        "double_points.dtype, short_points.dtype"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.float64, torch.int16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWjjBB7UnKBH",
        "colab_type": "text"
      },
      "source": [
        "## The tensor API\n",
        "\n",
        "It is worth taking a look at the tensor operations that PyTorch offers. It would be of little use to list them all here. Instead, we’re going to get a general feel for the API and establish a few directions on where to find things in the online documentation at http://pytorch.org/docs.\n",
        "\n",
        "First, the vast majority of operations on and between tensors are available in the torch module and can also be called as methods of a tensor object.\n",
        "\n",
        "For instance, the transpose function we encountered earlier can be used from the torch module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qnb7TG9m2mL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c08b29fb-c971-486e-b248-7e2d4222d0e2"
      },
      "source": [
        "a = torch.ones(3, 2)\n",
        "a_t = torch.transpose(a, 0, 1)\n",
        "a.shape, a_t.shape"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 2]), torch.Size([2, 3]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f5BWealnvDR",
        "colab_type": "text"
      },
      "source": [
        "or as a method of the a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viBID5UMnrzW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86ddb2d5-e325-4356-99f3-b5f42dc5dbd2"
      },
      "source": [
        "a = torch.ones(3, 2)\n",
        "a_t = a.transpose(0, 1)\n",
        "a.shape, a_t.shape"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 2]), torch.Size([2, 3]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmztDT28n0Ra",
        "colab_type": "text"
      },
      "source": [
        "There is no difference between the two forms; they can be used interchangeably.\n",
        "\n",
        "The PyTorch docs are exhaustive and well organized, with the tensor operations divided into groups:\n",
        "\n",
        "- *Creation ops* — Functions for constructing a tensor, like ones and from_numpy\n",
        "- *Indexing, slicing, joining, mutating ops*—Functions for changing the shape, stride, or content of a tensor, like transpose\n",
        "- *Math ops*—Functions for manipulating the content of the tensor through computations\n",
        "\n",
        "    * *Pointwise ops*-Functions for obtaining a new tensor by applying a function to each element independently, like abs and cos.\n",
        "\n",
        "    * *Reduction ops*-Functions for computing aggregate values by iterating through tensors, like mean, std, and norm\n",
        "    * Comparison ops—Functions for evaluating numerical predicates over tensors, like equal and max\n",
        "\n",
        "    * *Spectral ops*—Functions for transforming in and operating in the frequency domain, like stft and hamming_window\n",
        "\n",
        "    * *Other operations* —Special functions operating on vectors, like cross, or matrices, like trace\n",
        "\n",
        "    * *BLAS and LAPACK operations*—Functions following the Basic Linear Algebra Subprograms (BLAS) specification for scalar, vector-vector, matrix-vector, and matrix-matrix operations\n",
        "\n",
        "- *Random sampling*—Functions for generating values by drawing randomly from\n",
        "probability distributions, like randn and normal\n",
        "- *Serialization* —Functions for saving and loading tensors, like load and save\n",
        "- Parallelism—Functions for controlling the number of threads for parallel CPU\n",
        "execution, like set_num_threads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCXfFOSZHRqx",
        "colab_type": "text"
      },
      "source": [
        "### Indexing into storage\n",
        "\n",
        "Let’s see how indexing into the storage works in practice with our 2D points. The storage for a given tensor is accessible using the `.storage` property:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYc1zyIHHVuS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "edd72e13-93af-44c9-fa43-e231c65f8783"
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "points.storage()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 4.0\n",
              " 1.0\n",
              " 5.0\n",
              " 3.0\n",
              " 2.0\n",
              " 1.0\n",
              "[torch.FloatStorage of size 6]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCrALkw5HjUv",
        "colab_type": "text"
      },
      "source": [
        "Even though the tensor reports itself as having three rows and two columns, the storage under the hood is a contiguous array of size 6. In this sense, the tensor just knows how to translate a pair of indices into a location in the storage.\n",
        "\n",
        "We can also index into a storage manually. For instance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZLUl82vHfUo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6756655a-eb72-41b7-c025-cc8405cb22f6"
      },
      "source": [
        "points_storage = points.storage()\n",
        "points_storage[0]"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhH_3R6_HoAv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b05532c-90cb-4a7a-e8ef-552b2af359c6"
      },
      "source": [
        "points.storage()[1]"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFyyu1e0Ht9P",
        "colab_type": "text"
      },
      "source": [
        "We can’t index a storage of a 2D tensor using two indices. The layout of a storage is always one-dimensional, regardless of the dimensionality of any and all tensors that might refer to it.\n",
        "\n",
        "At this point, it shouldn’t come as a surprise that changing the value of a storage leads to changing the content of its referring tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrltiWpoHpxn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c0ebf9b5-44e4-4f63-cd22-ea9c79d727b2"
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "print(points)\n",
        "points_storage = points.storage()\n",
        "points_storage[0] = 2.0\n",
        "print(points)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4., 1.],\n",
            "        [5., 3.],\n",
            "        [2., 1.]])\n",
            "tensor([[2., 1.],\n",
            "        [5., 3.],\n",
            "        [2., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLEn0hc5H8AJ",
        "colab_type": "text"
      },
      "source": [
        "### Modifying stored values: In-place operations\n",
        "\n",
        "In addition to the operations on tensors introduced in the previous section, a small number of operations exist only as methods of the `Tensor` object. They are recognizable from a trailing underscore in their name, like `zero_` , which indicates that the method operates in place by modifying the input instead of creating a new output tensor and returning it. For instance, the `zero_` method zeros out all the elements of the input. Any method **without** the trailing underscore leaves the source tensor unchanged and instead returns a new tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv80ZOJmH1J_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9b5ac2ed-20f0-4973-dada-62f10a2ff652"
      },
      "source": [
        "a = torch.ones(3, 2)\n",
        "a"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kaRH0MgLVsO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "345acbfc-15f7-4ea4-c5eb-13e03386e77e"
      },
      "source": [
        "a.zero_()\n",
        "a"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDp9rHK6Lia9",
        "colab_type": "text"
      },
      "source": [
        "### Tensor metadata: Size, offset, and stride\n",
        "\n",
        "In order to index into a storage, tensors rely on a few pieces of information that, together with their storage, unequivocally define them: ***size, offset, and stride***.\n",
        "\n",
        "The `size` (or `shape`, in NumPy parlance) is a tuple indicating how many elements across each dimension the tensor represents.\n",
        "\n",
        "The `storage` offset is the index in the storage corresponding to the first element in the tensor.\n",
        "\n",
        "The `stride` is the number of elements in the storage that need to be skipped over to obtain the next element along each dimension.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3GxRhmlMb3g",
        "colab_type": "text"
      },
      "source": [
        "### Views of another tensor’s storage\n",
        "\n",
        "We can get the second point in the tensor by providing the corresponding index:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8019K7ZzLdLG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a21d9864-6e59-451d-f0b4-c6c7c3714279"
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "second_point = points[1]\n",
        "second_point.storage_offset()"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDfJVmI6MiOr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "85df5c08-43ea-423c-bcad-d86bcbc90a38"
      },
      "source": [
        "second_point.size()"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrmY1QeUMsr0",
        "colab_type": "text"
      },
      "source": [
        "The resulting tensor has `offset` 2 in the storage (since we need to skip the first point, which has two items), and the size is an instance of the `Size` class containing one element, since the tensor is one-dimensional.\n",
        "\n",
        "It’s important to note that this is the same information contained in the `shape` property of tensor objects:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJFmaYHzMpaN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92027df9-391f-4cff-cf27-302803855b74"
      },
      "source": [
        "second_point.shape"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8GH1IXsM477",
        "colab_type": "text"
      },
      "source": [
        "The `stride` is a tuple indicating the number of elements in the storage that have to be skipped when the index is increased by 1 in each dimension. For instance, our `points` tensor has a stride of `(2, 1)`:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOD9Lit7M3Mi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c795897a-fdb5-4916-f44e-039f6bc4ad30"
      },
      "source": [
        "points.stride()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blg9MpGMOcBL",
        "colab_type": "text"
      },
      "source": [
        "Accessing an element `i`, `j` in a 2D tensor results in accessing the `storage_offset + stride[0] * i + stride[1] * j` element in the storage. The offset will usually be zero; if this tensor is a view of a storage created to hold a larger tensor, the offset might be a positive value.\n",
        "\n",
        "This indirection between `Tensor` and `Storage` makes some operations inexpensive, like transposing a tensor or extracting a subtensor, because they do not lead to memory reallocations. Instead, they consist of allocating a new Tensor object with a different value for `size`, `storage` offset, or `stride`.\n",
        "\n",
        "We already extracted a subtensor when we indexed a specific point and saw the `storage` offset increasing. Let’s see what happens to the `size` and `stride` as well:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9t5imcANA2Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "673c96cf-db8e-4468-c947-14e2ab5e1929"
      },
      "source": [
        "print(points)\n",
        "second_point = points[1]\n",
        "second_point.size()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4., 1.],\n",
            "        [5., 3.],\n",
            "        [2., 1.]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5UIbuRmP37F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24fee65d-add0-4bf4-9f42-0c177633ab3e"
      },
      "source": [
        "second_point.storage_offset()"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHjW6AXwQiJA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c65bb000-2730-4705-fb6a-2cf0d7410031"
      },
      "source": [
        "second_point.stride()"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y6xZeyaQsF_",
        "colab_type": "text"
      },
      "source": [
        "The bottom line is that the subtensor has one less dimension, as we would expect, while still indexing the same storage as the original `points` tensor. This also means changing the subtensor will have a side effect on the original tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob1ccI1TQpiW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "52a70fc3-3842-4d97-8e03-faa33a3a4eca"
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "print(points)\n",
        "second_point = points[1]\n",
        "second_point[0] = 10.0\n",
        "print(points)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4., 1.],\n",
            "        [5., 3.],\n",
            "        [2., 1.]])\n",
            "tensor([[ 4.,  1.],\n",
            "        [10.,  3.],\n",
            "        [ 2.,  1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5NA3UjZS6D4",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "This might not always be desirable, so we can eventually clone the subtensor into a new tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4zF5SBSSrRO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "39a08301-113b-4fb4-91de-e2d6ca502820"
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "print(points)\n",
        "second_point = points[1].clone()\n",
        "second_point[0] = 10.0\n",
        "print(points)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4., 1.],\n",
            "        [5., 3.],\n",
            "        [2., 1.]])\n",
            "tensor([[4., 1.],\n",
            "        [5., 3.],\n",
            "        [2., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wwvG2AxTKxV",
        "colab_type": "text"
      },
      "source": [
        "### Transposing without copying\n",
        "\n",
        "Let’s try transposing now. Let’s take our `points` tensor, which has individual points in the rows and `X` and `Y` coordinates in the columns, and turn it around so that individual points are in the columns. We take this opportunity to introduce the `t` function, a shorthand alternative to `transpose` for two-dimensional tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67I9CbjgS-7L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e359cc0f-6f97-4883-b25e-5adbbd95f535"
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "points"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4., 1.],\n",
              "        [5., 3.],\n",
              "        [2., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idbbYGvJTcuX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ba86a3db-dcb8-4e34-cc1b-f7ad7f7ec7a9"
      },
      "source": [
        "points_t = points.t() #transpose\n",
        "points_t"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4., 5., 2.],\n",
              "        [1., 3., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvKnZ-HHT1vK",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We can easily verify that the two tensors share the same storage\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2xBWoUuTqh1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f8dbc985-e567-4771-92e4-ab84555c619f"
      },
      "source": [
        "id(points.storage()) == id(points_t.storage())"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xpj8SJgT5YR",
        "colab_type": "text"
      },
      "source": [
        "and that they differ only in shape and stride:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI2Gav0MT2oZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f61a8400-fc21-472e-8be8-bb09875a120a"
      },
      "source": [
        "points.stride()"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykuXi5pWT7-q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "65ac4baa-f268-4e95-81dd-7c3db2a28f69"
      },
      "source": [
        "points_t.stride()"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOd72Da-UUqh",
        "colab_type": "text"
      },
      "source": [
        "This tells us that increasing the first index by one in points—for example, going from points `[0,0]` to points `[1,0]` —will skip along the storage by two elements, while increasing the second index—from points `[0,0]` to points `[0,1]`—will skip along the storage by one. In other words, the storage holds the elements in the tensor sequentially row by row.\n",
        "\n",
        "We can transpose `points` into `points_t`. We change the order of the elements in the `stride`. After that, increasing the row (the first index of the tensor) will skip along the storage by one, just like when we were moving along columns in points. This is the very definition of transposing. No new memory is allocated: transposing is obtained only by creating a new `Tensor` instance with different `stride` ordering than the original."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BdZSJYlcDf5",
        "colab_type": "text"
      },
      "source": [
        "### Transposing in Higher Dimensions\n",
        "\n",
        "Transposing in PyTorch is not limited to matrices. We can transpose a multidimensional array by specifying the two dimensions along which transposing (flipping shape and stride) should occur:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Hs2ZVRwcJhP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "1b1f5cef-8941-473b-897d-bac61953e801"
      },
      "source": [
        "some_t = torch.ones(3, 4, 5) # 3 matrices with 4 rows and 5 columns -> 3, 4, 5\n",
        "some_t, some_t.shape, some_t.stride()"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[1., 1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1., 1.]],\n",
              " \n",
              "         [[1., 1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1., 1.]],\n",
              " \n",
              "         [[1., 1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1., 1.]]]), torch.Size([3, 4, 5]), (20, 5, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTv65TBQciv4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "b6d025cd-26ef-43a8-aa7f-dda9201fb288"
      },
      "source": [
        "transpose_t = some_t.transpose(0, 1) #flip 3 and 4 to get 4 matrices with 3 rows and 5 columns -> 4, 3, 5\n",
        "transpose_t, transpose_t.shape, transpose_t.stride()"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[1., 1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1., 1.]],\n",
              " \n",
              "         [[1., 1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1., 1.]],\n",
              " \n",
              "         [[1., 1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1., 1.]],\n",
              " \n",
              "         [[1., 1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1., 1.]]]), torch.Size([4, 3, 5]), (5, 20, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWT7qDljcnOl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "163665a0-52f9-434a-db81-f31e54dfe1e9"
      },
      "source": [
        "transpose_t = some_t.transpose(0, 2) #flip 3 and 5 to get 5 matrices with 4 rows and 3 columns -> 5, 4, 3\n",
        "transpose_t, transpose_t.shape, transpose_t.stride()"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[1., 1., 1.],\n",
              "          [1., 1., 1.],\n",
              "          [1., 1., 1.],\n",
              "          [1., 1., 1.]],\n",
              " \n",
              "         [[1., 1., 1.],\n",
              "          [1., 1., 1.],\n",
              "          [1., 1., 1.],\n",
              "          [1., 1., 1.]],\n",
              " \n",
              "         [[1., 1., 1.],\n",
              "          [1., 1., 1.],\n",
              "          [1., 1., 1.],\n",
              "          [1., 1., 1.]],\n",
              " \n",
              "         [[1., 1., 1.],\n",
              "          [1., 1., 1.],\n",
              "          [1., 1., 1.],\n",
              "          [1., 1., 1.]],\n",
              " \n",
              "         [[1., 1., 1.],\n",
              "          [1., 1., 1.],\n",
              "          [1., 1., 1.],\n",
              "          [1., 1., 1.]]]), torch.Size([5, 4, 3]), (1, 5, 20))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g0K2r2Gdxix",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "178bdf3e-d7cc-40b9-de20-eb377f239766"
      },
      "source": [
        "transpose_t = some_t.transpose(1, 2) #flip 4 and 5 to get 3 matrices with 5 rows and 4 columns -> 3, 5, 4\n",
        "transpose_t, transpose_t.shape, transpose_t.stride()"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1.]],\n",
              " \n",
              "         [[1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1.]],\n",
              " \n",
              "         [[1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1.],\n",
              "          [1., 1., 1., 1.]]]), torch.Size([3, 5, 4]), (20, 1, 5))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0PIYTSZeo1k",
        "colab_type": "text"
      },
      "source": [
        "### Contiguous tensors\n",
        "\n",
        "A tensor whose values are laid out in the storage starting from the rightmost dimension onward (that is, moving along rows for a 2D tensor) is defined as `contiguous`. Contiguous tensors are convenient because we can visit them efficiently in order without jumping around in the storage (improving data locality improves performance because of the way memory access works on modern CPUs). This advantage of course depends on the way algorithms visit it.\n",
        "\n",
        "Some tensor operations in PyTorch only work on contiguous tensors, such as `view`, which we’ll encounter in the next chapter. In that case, PyTorch will throw an informative exception and require us to call contiguous explicitly. It’s worth noting that calling contiguous will do nothing (and will not hurt performance) if the tensor is already contiguous.\n",
        "\n",
        "In our case, points is contiguous, while its transpose is not:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpsyTTB1fNls",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f8148321-d110-43aa-8cba-df6b1185c315"
      },
      "source": [
        "points.is_contiguous()"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WYwB4qifhHi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "44a04cdc-b21c-43b7-a3f0-ab997620ae71"
      },
      "source": [
        "points_t.is_contiguous()"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCzmuaP6frLD",
        "colab_type": "text"
      },
      "source": [
        "We can obtain a new contiguous tensor from a non-contiguous one using the contiguous method. The content of the tensor will be the same, but the stride will change, as will the storage:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKjPhtonfj1K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "5b0a52bf-a4f2-4b24-ff69-f18b37b5abca"
      },
      "source": [
        "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
        "print(points)\n",
        "points_t = points.t()\n",
        "print(points_t)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4., 1.],\n",
            "        [5., 3.],\n",
            "        [2., 1.]])\n",
            "tensor([[4., 5., 2.],\n",
            "        [1., 3., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsf_5BmYf0CX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "ac658382-52a2-4990-c409-095f4fbe7e20"
      },
      "source": [
        "points_t.storage()"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 4.0\n",
              " 1.0\n",
              " 5.0\n",
              " 3.0\n",
              " 2.0\n",
              " 1.0\n",
              "[torch.FloatStorage of size 6]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbJhqNy7f4Ue",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "64ca622b-c732-420a-aa7f-d2ecf6ea8f87"
      },
      "source": [
        "points_t.stride()"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMqFzBr4f6ne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f3f15c4f-c09f-4582-fa8c-ff71cdd07589"
      },
      "source": [
        "points_t_cont = points_t.contiguous() #new contiguous tensor\n",
        "points_t_cont"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4., 5., 2.],\n",
              "        [1., 3., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEAoEZ_YgETX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "4998cb9b-0fab-4a6c-9db8-db7421655993"
      },
      "source": [
        "points_t_cont.storage()"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 4.0\n",
              " 5.0\n",
              " 2.0\n",
              " 1.0\n",
              " 3.0\n",
              " 1.0\n",
              "[torch.FloatStorage of size 6]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXksQbyzgAX2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bc8bde84-aa94-401b-d949-51a382ae5aff"
      },
      "source": [
        "points_t_cont.stride()"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRhCkDbHg1_0",
        "colab_type": "text"
      },
      "source": [
        "Notice that the `storage` has been reshuffled in order for elements to be laid out row- by-row in the new storage. The `stride` has been changed to reflect the new layout.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpAzeoBVgIdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Mx1eIzfg_tP",
        "colab_type": "text"
      },
      "source": [
        "## Moving tensors to the GPU\n",
        "\n",
        "So far in this chapter, when we’ve talked about storage, we’ve meant memory on the CPU. PyTorch tensors also can be stored on a different kind of processor: a graphics processing unit (GPU). Every PyTorch tensor can be transferred to (one of) the GPU(s) in order to perform massively parallel, fast computations. All operations that will be performed on the tensor will be carried out using GPU-specific routines that come with PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vKPSwbxhe4O",
        "colab_type": "text"
      },
      "source": [
        "### Managing a tensor’s device attribute\n",
        "\n",
        "In addition to `dtype`, a PyTorch `Tensor` also has the notion of `device`, which is where on the computer the tensor data is placed. Here is how we can create a tensor on the GPU by specifying the corresponding argument to the constructor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tDiMZS8hB98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "points_gpu = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device='cuda')"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK1b_kaOifz2",
        "colab_type": "text"
      },
      "source": [
        "If the above line doesn't work on Google Colab because no CUDA-capable device is detected, refer [this link](https://stackoverflow.com/a/55723109/3252158).\n",
        "\n",
        "We could instead copy a tensor created on the CPU onto the GPU using the to method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PJxn427hteH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "points_gpu = points.to(device='cuda')"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8vvfOZ3jr08",
        "colab_type": "text"
      },
      "source": [
        "Doing so returns a new tensor that has the same numerical data, but stored in the RAM of the GPU, rather than in regular system RAM. Now that the data is stored locally on the GPU, we’ll start to see the speedups mentioned earlier when performing mathematical operations on the tensor. In almost all cases, CPU- and GPU-based tensors expose the same user-facing API, making it much easier to write code that is agnostic to where, exactly, the heavy number crunching is running.\n",
        "\n",
        "If our machine has more than one GPU, we can also decide on which GPU we allocate the tensor by passing a zero-based integer identifying the GPU on the machine, such as"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcSvmu1xilSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "points_gpu = points.to(device='cuda:0')"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKCTjnFvkbmK",
        "colab_type": "text"
      },
      "source": [
        "At this point, any operation performed on the tensor, such as multiplying all elements by a constant, is carried out on the GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtnROPVJkZfP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "5c213de2-a2e0-4631-a8db-954a22e3f15e"
      },
      "source": [
        "print(points)\n",
        "points = 2 * points #Multiplication performed on the CPU\n",
        "points_gpu = 2 * points.to(device='cuda') #Multiplication performed on the GPU\n",
        "print(points)\n",
        "print(points_gpu)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[32.,  8.],\n",
            "        [40., 24.],\n",
            "        [16.,  8.]])\n",
            "tensor([[64., 16.],\n",
            "        [80., 48.],\n",
            "        [32., 16.]])\n",
            "tensor([[128.,  32.],\n",
            "        [160.,  96.],\n",
            "        [ 64.,  32.]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuP-_EhglBbf",
        "colab_type": "text"
      },
      "source": [
        "Note that the `points_gpu` tensor is not brought back to the CPU once the result has been computed. Here’s what happened in this line:\n",
        "\n",
        "1.  The `points` tensor is copied to the GPU.\n",
        "2.  A new tensor is allocated on the GPU and used to store the result of the multiplication.\n",
        "3.  A handle to that GPU tensor is returned.\n",
        "\n",
        "Therefore, if we also add a constant to the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0-UDlgSki4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "points_gpu = points_gpu + 4"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAaytXTnlWC0",
        "colab_type": "text"
      },
      "source": [
        "the addition is still performed on the GPU, and no information flows to the CPU (unless we print or access the resulting tensor). In order to move the tensor back to the CPU, we need to provide a `cpu` argument to the `to` method, such as"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx1ynBqYlUCe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a3d44e00-23ed-4f89-f51e-001924ad8eb3"
      },
      "source": [
        "points_cpu = points_gpu.to(device='cpu')\n",
        "print(points_cpu)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[132.,  36.],\n",
            "        [164., 100.],\n",
            "        [ 68.,  36.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PKww3bslnhJ",
        "colab_type": "text"
      },
      "source": [
        "We can also use the shorthand methods `cpu` and `cuda` instead of the `to` method to achieve the same goal:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tojZQOa1leY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "points_gpu = points.cuda() #Defaults to GPU index 0\n",
        "points_gpu = points.cuda(0) #GPU\n",
        "points_cpu = points_gpu.cpu() #CPU"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6xns9n_l6tl",
        "colab_type": "text"
      },
      "source": [
        "It’s also worth mentioning that by using the `to` method, we can change the placement and the data type simultaneously by providing both `device` and `dtype` as arguments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmKBl0-ZmNyh",
        "colab_type": "text"
      },
      "source": [
        "## NumPy interoperability\n",
        "\n",
        "PyTorch tensors can be converted to NumPy arrays and vice versa very efficiently. By doing so, we can take advantage of the huge swath of functionality in the wider Python ecosystem that has built up around the NumPy array type. This zero-copy interoperability with NumPy arrays is due to the storage system working with the Python buffer protocol (https://docs.python.org/3/c-api/buffer.html).\n",
        "\n",
        "To get a NumPy array out of our `points` tensor, we just call"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl_XkBIUmPC1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "cde417b0-3208-47bf-8938-48b00a7217fc"
      },
      "source": [
        "points = torch.ones(3, 4)\n",
        "points_np = points.numpy()\n",
        "points_np"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71bt1IWrmtxp",
        "colab_type": "text"
      },
      "source": [
        "which will return a NumPy multidimensional array of the right `size`, `shape`, and numerical `type`. Interestingly, the returned array shares the same underlying buffer with the tensor `storage`. This means the `numpy` method can be effectively executed at basically no cost, as long as the data sits in CPU RAM. It also means modifying the NumPy array will lead to a change in the originating tensor. If the tensor is allocated on the GPU, PyTorch will make a copy of the content of the tensor into a NumPy array allocated on the CPU.\n",
        "\n",
        "Conversely, we can obtain a PyTorch tensor from a NumPy array this way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4JJmTyFmpFS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a6898931-70ae-4ff6-ec0e-e36aba4e52d3"
      },
      "source": [
        "points = torch.from_numpy(points_np)\n",
        "points"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckm8RUHInHDG",
        "colab_type": "text"
      },
      "source": [
        "which will use the same buffer-sharing strategy we just described.\n",
        "\n",
        "Note: while the default numeric type in PyTorch is `32-bit floating-point`, for NumPy it is `64-bit`. As discussed in \"*A `dtype` for every occasion*\" in *Tensor Element Types*, we usually want to use 32-bit floating-points, so we need to make sure we have tensors of `dtype torch .float` after converting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lklSU7zXn0PY",
        "colab_type": "text"
      },
      "source": [
        "## Serializing tensors\n",
        "\n",
        "Creating a tensor on the fly is all well and good, but if the data inside is valuable, we will want to save it to a file and load it back at some point. After all, we don’t want to have to retrain a model from scratch every time we start running our program! PyTorch uses `pickle` under the hood to serialize the `tensor` object, plus dedicated serialization code for the storage. Here’s how we can save our `points` tensor to an `ourpoints.t` file:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN-01s1JnC1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(points, '/content/ourpoints.t')"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tVEQgAOoV-o",
        "colab_type": "text"
      },
      "source": [
        "As an alternative, we can pass a file descriptor in lieu of the filename:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZNYsrEVoPad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/ourpoints.t','wb') as f:\n",
        "  torch.save(points, f)"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19_25NJFofGq",
        "colab_type": "text"
      },
      "source": [
        "Loading our points back is similarly a one-liner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwYNkOCTocvT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "cde91129-3568-451a-d915-297aaf515864"
      },
      "source": [
        "points = torch.load('/content/ourpoints.t')\n",
        "print(points)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT7JZRKkonTu",
        "colab_type": "text"
      },
      "source": [
        "or, equivalently,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYnC7EXLokQ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e5b91304-4ca2-4547-f442-dd89f92b403c"
      },
      "source": [
        "with open('/content/ourpoints.t','rb') as f:\n",
        "  points = torch.load(f)\n",
        "\n",
        "print(points)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnSv_rvRowjw",
        "colab_type": "text"
      },
      "source": [
        "While we can quickly save tensors this way if we only want to load them with PyTorch, the file format itself is not interoperable: we can’t read the tensor with software other than PyTorch. Depending on the use case, this may or may not be a limitation, but we should learn how to save tensors interoperably for those times when it is. We’ll look next at how to do so."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmaVBi4co27w",
        "colab_type": "text"
      },
      "source": [
        "### Serializing to HDF5 with h5py\n",
        "\n",
        "Every use case is unique, but we suspect needing to save tensors interoperably will be more common when introducing PyTorch into existing systems that already rely on different libraries. New projects probably won’t need to do this as often.\n",
        "\n",
        "For those cases when you need to, however, you can use the HDF5 format and library (www.hdfgroup.org/solutions/hdf5). HDF5 is a portable, widely supported format for representing serialized multidimensional arrays, organized in a nested key- value dictionary. Python supports HDF5 through the h5py library (www.h5py.org), which accepts and returns data in the form of NumPy arrays.\n",
        "\n",
        "We can install h5py using"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORoXdLHLosQw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a0ee1f16-1ccb-407f-b997-938ef1654ce3"
      },
      "source": [
        "!pip install h5py"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng-JM7IbpXks",
        "colab_type": "text"
      },
      "source": [
        "At this point, we can save our `points` tensor by converting it to a NumPy array (at no cost, as we noted earlier) and passing it to the `create_dataset` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEaxxPJtpDOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py\n",
        "f = h5py.File('/content/ourpoints.hdf5', 'w')\n",
        "dset = f.create_dataset('coords', data=points.numpy())\n",
        "f.close()"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d77QeEjapmYk",
        "colab_type": "text"
      },
      "source": [
        "Here '`coords`' is a key into the HDF5 file. We can have other keys—even nested ones. One of the interesting things in HDF5 is that we can index the dataset while on disk and access only the elements we’re interested in.\n",
        "\n",
        "Let’s suppose we want to load just the last two points in our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a-Ee86SpleZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = h5py.File('/content/ourpoints.hdf5', 'r')\n",
        "dset = f['coords']\n",
        "last_points = dset[-2:]"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBLItzvOqFBZ",
        "colab_type": "text"
      },
      "source": [
        "The data is not loaded when the file is opened or the dataset is required. Rather, the data stays on disk until we request the second and last rows in the dataset. At that point, `h5py` accesses those two columns and returns a NumPy array-like object encapsulating that region in that dataset that behaves like a NumPy array and has the same API.\n",
        "\n",
        "Owing to this fact, we can pass the returned object to the `torch.from_numpy` function to obtain a tensor directly. Note that in this case, the data is copied over to the tensor’s storage:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z7Ftu1NqAFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "last_points = torch.from_numpy(dset[-2:])\n",
        "f.close()"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAb6KxQpr5y8",
        "colab_type": "text"
      },
      "source": [
        "Once we’re finished loading data, we close the file. Closing the HDFS file invalidates the datasets, and trying to access dset afterward will give an exception. As long as we stick to the order shown here, we are fine and can now work with the `last_points` tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPIgbMF4r_SI",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Now we have covered everything we need to get started with representing everything in floats. We’ll cover other aspects of tensors—such as creating views of tensors; indexing tensors with other tensors; and broadcasting, which simplifies performing element-wise operations between tensors of different sizes or shapes—as needed along the way.\n",
        "\n",
        "In chapter 4, we will learn how to represent real-world data in PyTorch. We will start with simple tabular data and move on to something more elaborate. In the process, we will get to know more about tensors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFiVg1CHsUIk",
        "colab_type": "text"
      },
      "source": [
        "## Exercises\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOLOL0VlskB-",
        "colab_type": "text"
      },
      "source": [
        "1. Create a tensor `a` from `list(range(9))`. Predict and then check the `size`, `offset`, and `stride`.\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFxE_irRtAzi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a84c443-6699-417e-9f23-264151fcb680"
      },
      "source": [
        "a = torch.Tensor(list(range(9)))\n",
        "a"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-DOFYHAwDmH",
        "colab_type": "text"
      },
      "source": [
        "The size should be 9. Offset should be 0. Stride should be 1 because the tensor is one-dimensional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVnDJmqDucaV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c0479984-3fce-4d47-edc0-f0120e5f2713"
      },
      "source": [
        "a.size(), a.storage_offset(), a.stride()"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([9]), 0, (1,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNqvgujds1AV",
        "colab_type": "text"
      },
      "source": [
        "  a. Create a new tensor using `b = a.view(3, 3)`. What does view do? Check that `a` and `b` share the same storage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyWb6iynsjSL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9a93b8c5-c085-4581-afbc-ecc6dc920dc0"
      },
      "source": [
        "b = a.view(3, 3)\n",
        "b"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 1., 2.],\n",
              "        [3., 4., 5.],\n",
              "        [6., 7., 8.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-yccKFjvDwm",
        "colab_type": "text"
      },
      "source": [
        "The `view` shows us the same tensor with a different `shape`. To check that `a` and `b` share the same storage:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTc5NwDrvgQ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56c60827-c7a8-49d7-c67e-fdfbc2d00491"
      },
      "source": [
        "id(a.storage()) == id(b.storage())"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4G4G1k9tCTz",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "b. Create a tensor `c = b[1:,1:]`. Predict and then check the `size`, `offset`, and `stride`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqLGBq1RtI8e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "55143d3a-a5cd-4c94-e5d2-8a71613a8074"
      },
      "source": [
        "c = b[1:,1:]\n",
        "c"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4., 5.],\n",
              "        [7., 8.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz-e3Qn4wXpV",
        "colab_type": "text"
      },
      "source": [
        "The size should be 2x2, offset should be 4, and stride should be (3, 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7F5ImFDwBeE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bfc0fbe5-54ae-4267-bdd3-72eefb070fb5"
      },
      "source": [
        "c.size(), c.storage_offset(), c.stride()"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 2]), 4, (3, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYi6c2kWwyxa",
        "colab_type": "text"
      },
      "source": [
        "2. Pick a mathematical operation like `cosine` or `square root`. Can you find a corresponding function in the torch library?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rltmh0IbxC_z",
        "colab_type": "text"
      },
      "source": [
        "[Tan function](https://pytorch.org/docs/master/generated/torch.tan.html#torch-tan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OuNn1RoxnaQ",
        "colab_type": "text"
      },
      "source": [
        "a. Apply the function element-wise to `a`. Why does it return an error?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4BoLokowlWX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f4d2bad-d14b-43ca-d5ba-2b2bad7241ed"
      },
      "source": [
        "a"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERRI0Knjxvzl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "888f9cc2-9887-4228-9fb1-9cbec78c475d"
      },
      "source": [
        "import math\n",
        "\n",
        "#print(math.tan(a))"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-177-04b7b6cbe815>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NFXf4Bi0cpu",
        "colab_type": "text"
      },
      "source": [
        "It returns an error because only one element tensors can be converted to Python scalars."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fk-nLZv1L5b",
        "colab_type": "text"
      },
      "source": [
        "b. What operation is required to make the function work?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHku21Mpx0hb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4662c1a9-d23d-4c1a-f9eb-44985ddd8dfc"
      },
      "source": [
        "torch.tan(a)"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000,  1.5574, -2.1850, -0.1425,  1.1578, -3.3805, -0.2910,  0.8714,\n",
              "        -6.7997])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBc67zEB1S0a",
        "colab_type": "text"
      },
      "source": [
        "c. Is there a version of your function that operates in place?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd7Wu7T41RgH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "aa8fb367-da44-412c-f063-fdb0cea4e976"
      },
      "source": [
        "a_elementwise = []\n",
        "for element in a:\n",
        "  a_elementwise.append(math.tan(element))\n",
        "\n",
        "print(a_elementwise)"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0, 1.5574077246549023, -2.185039863261519, -0.1425465430742778, 1.1578212823495775, -3.380515006246586, -0.29100619138474915, 0.8714479827243188, -6.799711455220379]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weKPp1LL1rlf",
        "colab_type": "text"
      },
      "source": [
        "## Summary\n",
        "\n",
        "- Neural networks transform floating-point representations into other floating- point representations. The starting and ending representations are typically human interpretable, but the intermediate representations are less so.\n",
        "- These floating-point representations are stored in tensors.\n",
        "- Tensors are multidimensional arrays; they are the basic data structure in\n",
        "PyTorch.\n",
        "- PyTorch has a comprehensive standard library for tensor creation, manipulation, and mathematical operations.\n",
        "- Tensors can be serialized to disk and loaded back.\n",
        "- All tensor operations in PyTorch can execute on the CPU as well as on the GPU,\n",
        "with no change in the code.\n",
        "- PyTorch uses a trailing underscore to indicate that a function operates in place on a tensor (for example, Tensor.sqrt_)."
      ]
    }
  ]
}