{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rEUfyCpPTqQX"
      ],
      "authorship_tag": "ABX9TyMpYI3wGD4nonlCMjzX39x3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prat8897/DL_PyTorch/blob/master/Chapter4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEUfyCpPTqQX",
        "colab_type": "text"
      },
      "source": [
        "## Real-world data representation using tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qgdfqr5TvXv",
        "colab_type": "text"
      },
      "source": [
        "This chapter covers\n",
        "- Representing real-world data as PyTorch tensors\n",
        "- Working with a range of data types\n",
        "- Loading data from a file\n",
        "- Converting data to tensors\n",
        "- Shaping tensors so they can be used as inputs for neural network models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKw_NFiQR4WY",
        "colab_type": "text"
      },
      "source": [
        "## Continuous, ordinal, and categorical values\n",
        "\n",
        "We should be aware of three different kinds of numerical values as we attempt to make sense of our data. The first kind is `continuous` values. These are the most intuitive when represented as numbers. They are strictly ordered, and a difference between various values has a strict meaning. Stating that package `A` is 2 kilograms heavier than package `B`, or that package `B` came from 100 miles farther away than `A` has a fixed meaning, regardless of whether package `A` is 3 kilograms or 10, or if `B` came from 200 miles away or 2,000. If you’re counting or measuring something with units, it’s probably a `continuous` value.\n",
        "The literature actually divides continuous values further: in the previous examples, it makes sense to say something is twice as heavy or three times farther away, so those values are said to be on a ratio scale. The time of day, on the other hand, does have the notion of difference, but it is not reasonable to claim that 6:00 is twice as late as 3:00; so time of day only offers aninterval scale.\n",
        "\n",
        "\n",
        "Next we have `ordinal` values. The strict ordering we have with continuous values remains, but the fixed relationship between values no longer applies. A good example of this is ordering a small, medium, or large drink, with small mapped to the value 1, medium 2, and large 3. The large drink is bigger than the medium, in the same way that 3 is bigger than 2, but it doesn’t tell us anything about how much bigger. If we were to convert our 1, 2, and 3 to the actual volumes (say, 8, 12, and 24 fluid ounces), then they would switch to being interval values. It’s important to remember that we can’t “do math” on the values outside of ordering them; trying to average large = 3 and small = 1 does not result in a medium drink!\n",
        "\n",
        "\n",
        "Finally, `categorical` values have neither ordering nor numerical meaning to their values. These are often just enumerations of possibilities assigned arbitrary numbers. Assigning water to 1, coffee to 2, soda to 3, and milk to 4 is a good example. There’s no real logic to placing water first and milk last; they simply need distinct values to differentiate them. We could assign coffee to 10 and milk to –3, and there would be no significant change (though assigning values in the range `0..N – 1` will have advantages for one-hot encoding and the embeddings we’ll discuss later). Because the numerical values bear no meaning, they are said to be on a `nominal` scale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY3ExcQUUhDv",
        "colab_type": "text"
      },
      "source": [
        "## Working with images\n",
        "\n",
        "### Loading an image file\n",
        "\n",
        "Images come in several different file formats, but luckily there are plenty of ways to load images in Python. Let’s start by loading a PNG image using the imageio module ([code/p1ch4/1_image_dog.ipynb](https://github.com/deep-learning-with-pytorch/dlwpt-code/blob/master/p1ch4/1_image_dog.ipynb)).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpcxfaAtXU3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "torch.set_printoptions(edgeitems=2, threshold=50)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxI04mP5T5uK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1117c43-bc0d-4dd1-b80d-b690a2e6b581"
      },
      "source": [
        "import imageio\n",
        "img_arr = imageio.imread('/content/bobby.jpg')\n",
        "img_arr.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(720, 1280, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy7J9qZVXpTy",
        "colab_type": "text"
      },
      "source": [
        "At this point, img is a NumPy array-like object with three dimensions: two spatial dimensions, width and height; and a third dimension corresponding to the red, green, and blue channels. Any library that outputs a NumPy array will suffice to obtain a PyTorch tensor. The only thing to watch out for is the layout of the dimensions. PyTorch modules dealing with image data require tensors to be laid out as C × H × W : `channels`, `height`, and `width`, respectively.\n",
        "\n",
        "We can use the tensor’s `permute` method with the old dimensions for each new dimension to get to an appropriate layout. Given an input tensor H × W × C as obtained previously, we get a proper layout by having channel 2 first and then channels 0 and 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSxLQ4xLXQgN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "36f3922d-dc25-42d0-b6c3-a864a19ced19"
      },
      "source": [
        "img = torch.from_numpy(img_arr)\n",
        "out = img.permute(2, 0, 1)\n",
        "out, out.shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 77,  77,  ..., 117, 116],\n",
              "          [ 75,  76,  ..., 117, 116],\n",
              "          ...,\n",
              "          [215, 216,  ..., 174, 174],\n",
              "          [215, 216,  ..., 158, 158]],\n",
              " \n",
              "         [[ 45,  45,  ...,  77,  76],\n",
              "          [ 43,  44,  ...,  77,  76],\n",
              "          ...,\n",
              "          [165, 166,  ..., 124, 124],\n",
              "          [165, 166,  ..., 107, 107]],\n",
              " \n",
              "         [[ 22,  22,  ...,  51,  51],\n",
              "          [ 20,  21,  ...,  51,  50],\n",
              "          ...,\n",
              "          [ 78,  79,  ...,  55,  55],\n",
              "          [ 78,  79,  ...,  41,  41]]], dtype=torch.uint8),\n",
              " torch.Size([3, 720, 1280]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnJ2-7-uYuW6",
        "colab_type": "text"
      },
      "source": [
        "As a slightly more efficient alternative to using stack to build up the tensor, we can pre-allocate a tensor of appropriate size and fill it with images loaded from a directory, like so:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABce6cEWX3ko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 3\n",
        "batch = torch.zeros(batch_size, 3, 256, 256, dtype=torch.uint8)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0ZbHv35Yzz2",
        "colab_type": "text"
      },
      "source": [
        "This indicates that our batch will consist of three RGB images 256 pixels in height and 256 pixels in width. Notice the type of the tensor: we’re expecting each color to be represented as an 8-bit integer, as in most photographic formats from standard consumer cameras. We can now load all PNG images from an input directory and store them in the tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7miwfjUYxuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "data_dir = '/content/image-cats/'\n",
        "filenames = [name for name in os.listdir(data_dir) if os.path.splitext(name)[-1] == '.png']\n",
        "\n",
        "for i, filename in enumerate(filenames):\n",
        "  img_arr = imageio.imread(os.path.join(data_dir, filename))\n",
        "  img_t = torch.from_numpy(img_arr)\n",
        "  img_t = img_t.permute(2, 0, 1) #H × W × C\n",
        "  img_t = img_t[:3] #only keep first 3 channels\n",
        "  batch[i] = img_t"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcfNYS3BF99M",
        "colab_type": "text"
      },
      "source": [
        "### Normalizing the data\n",
        "\n",
        "We mentioned earlier that neural networks usually work with floating-point tensors as their input. Neural networks exhibit the best training performance when the input data ranges roughly from `0` to `1`, or from `-1` to `1` (this is an effect of how their building blocks are defined).\n",
        "\n",
        "\n",
        "So a typical thing we’ll want to do is cast a tensor to floating-point and normalize the values of the pixels. Casting to floating-point is easy, but normalization is trickier, as it depends on what range of the input we decide should lie between `0` and `1` (or `-1` and `1`). One possibility is to just divide the values of the pixels by `255` (the maximum representable number in 8-bit unsigned):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApBA44IHaJGD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d0633a3-c01c-4d99-9298-0cd3bc0fd7b6"
      },
      "source": [
        "print(batch)\n",
        "batch = batch.float()\n",
        "batch /= 255.0  #normalize between 0 and 1\n",
        "print(batch)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[156, 152,  ..., 149, 158],\n",
            "          [174, 134,  ..., 136, 138],\n",
            "          ...,\n",
            "          [129, 130,  ..., 121, 114],\n",
            "          [129, 123,  ..., 121, 120]],\n",
            "\n",
            "         [[139, 135,  ..., 135, 147],\n",
            "          [160, 119,  ..., 122, 124],\n",
            "          ...,\n",
            "          [111, 111,  ..., 112, 105],\n",
            "          [111, 104,  ..., 110, 111]],\n",
            "\n",
            "         [[129, 123,  ..., 132, 145],\n",
            "          [155, 110,  ..., 119, 121],\n",
            "          ...,\n",
            "          [108, 108,  ..., 117, 110],\n",
            "          [107,  98,  ..., 115, 116]]],\n",
            "\n",
            "\n",
            "        [[[238, 238,  ..., 215, 215],\n",
            "          [238, 238,  ..., 215, 215],\n",
            "          ...,\n",
            "          [214, 213,  ..., 190, 192],\n",
            "          [214, 213,  ..., 190, 192]],\n",
            "\n",
            "         [[195, 195,  ..., 175, 175],\n",
            "          [195, 195,  ..., 175, 175],\n",
            "          ...,\n",
            "          [128, 127,  ..., 103, 105],\n",
            "          [128, 127,  ..., 103, 105]],\n",
            "\n",
            "         [[137, 137,  ..., 126, 126],\n",
            "          [137, 137,  ..., 126, 126],\n",
            "          ...,\n",
            "          [ 79,  78,  ...,  69,  71],\n",
            "          [ 79,  78,  ...,  69,  72]]],\n",
            "\n",
            "\n",
            "        [[[202, 193,  ...,  13,  12],\n",
            "          [199, 192,  ...,  14,  14],\n",
            "          ...,\n",
            "          [ 75,  68,  ...,  36,  37],\n",
            "          [ 85, 103,  ...,  37,  38]],\n",
            "\n",
            "         [[151, 139,  ...,   9,   8],\n",
            "          [151, 140,  ...,  11,  11],\n",
            "          ...,\n",
            "          [ 33,  26,  ...,  26,  27],\n",
            "          [ 40,  58,  ...,  27,  28]],\n",
            "\n",
            "         [[ 68,  53,  ...,   6,   5],\n",
            "          [ 67,  54,  ...,   6,   6],\n",
            "          ...,\n",
            "          [ 11,   2,  ...,  17,  18],\n",
            "          [ 19,  37,  ...,  18,  19]]]], dtype=torch.uint8)\n",
            "tensor([[[[0.6118, 0.5961,  ..., 0.5843, 0.6196],\n",
            "          [0.6824, 0.5255,  ..., 0.5333, 0.5412],\n",
            "          ...,\n",
            "          [0.5059, 0.5098,  ..., 0.4745, 0.4471],\n",
            "          [0.5059, 0.4824,  ..., 0.4745, 0.4706]],\n",
            "\n",
            "         [[0.5451, 0.5294,  ..., 0.5294, 0.5765],\n",
            "          [0.6275, 0.4667,  ..., 0.4784, 0.4863],\n",
            "          ...,\n",
            "          [0.4353, 0.4353,  ..., 0.4392, 0.4118],\n",
            "          [0.4353, 0.4078,  ..., 0.4314, 0.4353]],\n",
            "\n",
            "         [[0.5059, 0.4824,  ..., 0.5176, 0.5686],\n",
            "          [0.6078, 0.4314,  ..., 0.4667, 0.4745],\n",
            "          ...,\n",
            "          [0.4235, 0.4235,  ..., 0.4588, 0.4314],\n",
            "          [0.4196, 0.3843,  ..., 0.4510, 0.4549]]],\n",
            "\n",
            "\n",
            "        [[[0.9333, 0.9333,  ..., 0.8431, 0.8431],\n",
            "          [0.9333, 0.9333,  ..., 0.8431, 0.8431],\n",
            "          ...,\n",
            "          [0.8392, 0.8353,  ..., 0.7451, 0.7529],\n",
            "          [0.8392, 0.8353,  ..., 0.7451, 0.7529]],\n",
            "\n",
            "         [[0.7647, 0.7647,  ..., 0.6863, 0.6863],\n",
            "          [0.7647, 0.7647,  ..., 0.6863, 0.6863],\n",
            "          ...,\n",
            "          [0.5020, 0.4980,  ..., 0.4039, 0.4118],\n",
            "          [0.5020, 0.4980,  ..., 0.4039, 0.4118]],\n",
            "\n",
            "         [[0.5373, 0.5373,  ..., 0.4941, 0.4941],\n",
            "          [0.5373, 0.5373,  ..., 0.4941, 0.4941],\n",
            "          ...,\n",
            "          [0.3098, 0.3059,  ..., 0.2706, 0.2784],\n",
            "          [0.3098, 0.3059,  ..., 0.2706, 0.2824]]],\n",
            "\n",
            "\n",
            "        [[[0.7922, 0.7569,  ..., 0.0510, 0.0471],\n",
            "          [0.7804, 0.7529,  ..., 0.0549, 0.0549],\n",
            "          ...,\n",
            "          [0.2941, 0.2667,  ..., 0.1412, 0.1451],\n",
            "          [0.3333, 0.4039,  ..., 0.1451, 0.1490]],\n",
            "\n",
            "         [[0.5922, 0.5451,  ..., 0.0353, 0.0314],\n",
            "          [0.5922, 0.5490,  ..., 0.0431, 0.0431],\n",
            "          ...,\n",
            "          [0.1294, 0.1020,  ..., 0.1020, 0.1059],\n",
            "          [0.1569, 0.2275,  ..., 0.1059, 0.1098]],\n",
            "\n",
            "         [[0.2667, 0.2078,  ..., 0.0235, 0.0196],\n",
            "          [0.2627, 0.2118,  ..., 0.0235, 0.0235],\n",
            "          ...,\n",
            "          [0.0431, 0.0078,  ..., 0.0667, 0.0706],\n",
            "          [0.0745, 0.1451,  ..., 0.0706, 0.0745]]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3RBqicVGwPT",
        "colab_type": "text"
      },
      "source": [
        "Another possibility is to compute the *mean* and *standard deviation* of the input data and scale it so that the output has `zero` mean and unit standard deviation across each channel:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVi2G5GyGr7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_channels = batch.shape[1]\n",
        "for c in range(n_channels):\n",
        "  mean = torch.mean(batch[:, c])\n",
        "  std = torch.std(batch[:, c])\n",
        "  batch[:, c] = (batch[:, c] - mean) / std"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKRJJezCJj_q",
        "colab_type": "text"
      },
      "source": [
        "Here, we normalize just a single batch of images because we do not know yet how to operate on an entire dataset. In working with images, it is good practice to compute the *mean* and *standard deviation* on all the training data in advance and then subtract and divide by these fixed, precomputed quantities.\n",
        "\n",
        "We can perform several other operations on inputs, such as geometric transforma- tions like rotations, scaling, and cropping. These may help with training or may be required to make an arbitrary input conform to the input requirements of a network, like the size of the image. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4RzAtUpJufu",
        "colab_type": "text"
      },
      "source": [
        "## 3D images: Volumetric data\n",
        "\n",
        "### Loading a specialized format\n",
        "\n",
        "Let’s load a sample CT scan using the volread function in the imageio module, which takes a directory as an argument and assembles all Digital Imaging and Communi- cations in Medicine (DICOM) files2 in a series in a NumPy 3D array [(code/p1ch4/ 2_volumetric_ct.ipynb)](https://github.com/deep-learning-with-pytorch/dlwpt-code/blob/master/p1ch4/2_volumetric_ct.ipynb).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t-rsJmWKcDn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3dc88848-1c36-4eae-e66b-516a3e104360"
      },
      "source": [
        "import imageio\n",
        "dir_path = \"/content/2-LUNG 3.0  B70f-04083\"\n",
        "vol_arr = imageio.volread(dir_path, 'DICOM')\n",
        "vol_arr.shape"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading DICOM (examining files): 1/99 files (1.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b99/99 files (100.0%)\n",
            "  Found 1 correct series.\n",
            "Reading DICOM (loading data): 65/99  (65.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b99/99  (100.0%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99, 512, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzF21AxFMjC8",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The layout is different from what PyTorch expects, due to having no channel information. So we’ll have to make room for the channel dimension using `unsqueeze`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6kXezbPMLFx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d4a69b6-405d-4cbf-8e41-88ed1b0e825a"
      },
      "source": [
        "vol = torch.from_numpy(vol_arr).float()\n",
        "vol = torch.unsqueeze(vol, 0)\n",
        "vol.shape"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 99, 512, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yfv5w3GMwb7",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "At this point we could assemble a 5D dataset by stacking multiple volumes along the `batch` direction, just as we did in the previous section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6Y15w4WNL2L",
        "colab_type": "text"
      },
      "source": [
        "## Representing tabular data\n",
        "\n",
        "The simplest form of data we’ll encounter on a machine learning job is sitting in a spreadsheet, CSV file, or database. Whatever the medium, it’s a table containing one row per sample (or record), where columns contain one piece of information about our sample.\n",
        "\n",
        "At first we are going to assume there’s no meaning to the order in which samples appear in the table: such a table is a collection of independent samples, unlike a time series, for instance, in which samples are related by a time dimension.\n",
        "\n",
        "Columns may contain numerical values, like temperatures at specific locations; or labels, like a string expressing an attribute of the sample, like “blue.” Therefore, tabular data is typically not homogeneous: different columns don’t have the same type. We might have a column showing the weight of apples and another encoding their color in a label.\n",
        "PyTorch tensors, on the other hand, are homogeneous. Information in PyTorch is typically encoded as a number, typically `floating-point` (though `integer` types and `Boolean` are supported as well). This numeric encoding is deliberate, since neural networks are mathematical entities that take real numbers as inputs and produce real numbers as output through successive application of matrix multiplications and nonlinear functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7yIVp3NNj_E",
        "colab_type": "text"
      },
      "source": [
        "### Using a real-world dataset\n",
        "\n",
        "Our first job as deep learning practitioners is to encode heterogeneous, real-world data into a tensor of floating-point numbers, ready for consumption by a neural network. A large number of tabular datasets are freely available on the internet; see, for instance, https://github.com/caesar0301/awesome-public-datasets.\n",
        "\n",
        "Let’s start with something fun: wine! The Wine Quality dataset is a freely available table containing chemical characterizations of samples of vinho verde, a wine from north Portugal, together with a sensory quality score. The dataset for white wines can be downloaded here: http://mng.bz/90Ol. For convenience, we also created a copy of the dataset on the Deep Learning with PyTorch Git repository, under data/p1ch4/tabular-wine.\n",
        "\n",
        "\n",
        "The file contains a comma-separated collection of values organized in 12 columns preceded by a header line containing the column names. The first 11 columns contain values of chemical variables, and the last column contains the sensory quality score from 0 (very bad) to 10 (excellent). These are the column names in the order they appear in the dataset:\n",
        "\n",
        "  - fixed acidity\n",
        "  - volatile acidity\n",
        "  - citric acid\n",
        "  - residual sugar\n",
        "  - chlorides\n",
        "  - free sulfur dioxide\n",
        "  - total sulfur dioxide\n",
        "  - density\n",
        "  - pH\n",
        "  - sulphates\n",
        "  - alcohol\n",
        "  - quality\n",
        "  \n",
        "\n",
        "A possible machine learning task on this dataset is predicting the quality score from chemical characterization alone. We’re hoping to find a relationship between one of the chemical columns in our data and the quality column. Here, we’re expecting to see quality increase as sulfur decreases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIF9RBW-OfOa",
        "colab_type": "text"
      },
      "source": [
        "### Loading a wine data tensor\n",
        "\n",
        "We need to be able to examine the data in a more usable way than opening the file in a text editor. Let’s see how we can load the data using Python and then turn it into a PyTorch tensor. Python offers several options for quickly loading a CSV file. Three popular options are\n",
        "\n",
        "\n",
        "- The csv module that ships with Python\n",
        "- NumPy\n",
        "- Pandas\n",
        "\n",
        "Let's use NumPy for this example. We can load our file and turn the resulting NumPy array into a PyTorch tensor [(code/p1ch4/3_tabular_wine.ipynb)](https://github.com/deep-learning-with-pytorch/dlwpt-code/blob/master/p1ch4/3_tabular_wine.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REHFDco6Mpmc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "55c71b83-f373-4260-c6f1-165536c64ae0"
      },
      "source": [
        "import csv\n",
        "wine_path = \"/content/winequality-white.csv\"\n",
        "wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=\";\",skiprows=1)\n",
        "wineq_numpy"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 7.  ,  0.27,  0.36, ...,  0.45,  8.8 ,  6.  ],\n",
              "       [ 6.3 ,  0.3 ,  0.34, ...,  0.49,  9.5 ,  6.  ],\n",
              "       [ 8.1 ,  0.28,  0.4 , ...,  0.44, 10.1 ,  6.  ],\n",
              "       ...,\n",
              "       [ 6.5 ,  0.24,  0.19, ...,  0.46,  9.4 ,  6.  ],\n",
              "       [ 5.5 ,  0.29,  0.3 , ...,  0.38, 12.8 ,  7.  ],\n",
              "       [ 6.  ,  0.21,  0.38, ...,  0.32, 11.8 ,  6.  ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZVsaaHcPVuV",
        "colab_type": "text"
      },
      "source": [
        "Here we just prescribe what the type of the 2D array should be (`32-bit floating-point`), the `delimiter` used to separate values in each row, and the fact that the first line should not be read since it contains the column names. Let’s check that all the data has been read:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlSQku8wPPzV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "64789deb-d6cc-40ad-d4bf-7bf6ae8ac975"
      },
      "source": [
        "col_list = next(csv.reader(open(wine_path), delimiter=';'))\n",
        "wineq_numpy.shape, col_list"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4898, 12),\n",
              " ['fixed acidity',\n",
              "  'volatile acidity',\n",
              "  'citric acid',\n",
              "  'residual sugar',\n",
              "  'chlorides',\n",
              "  'free sulfur dioxide',\n",
              "  'total sulfur dioxide',\n",
              "  'density',\n",
              "  'pH',\n",
              "  'sulphates',\n",
              "  'alcohol',\n",
              "  'quality'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Psn3XVVPm9Y",
        "colab_type": "text"
      },
      "source": [
        "and proceed to convert the `NumPy` array to a `PyTorch` tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZdtZDY3Pd0r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88a03c69-a1c2-4233-c231-e8b9e16117b2"
      },
      "source": [
        "wineq = torch.from_numpy(wineq_numpy)\n",
        "wineq.shape, wineq.dtype"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4898, 12]), torch.float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sliz55IIPwyo",
        "colab_type": "text"
      },
      "source": [
        "At this point, we have a floating-point `torch.Tensor` containing all the columns, including the last, which refers to the `quality` score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrXuscm5Q29v",
        "colab_type": "text"
      },
      "source": [
        "### Representing scores\n",
        "\n",
        "We could treat the score as a `continuous` variable, keep it as a real number, and perform a regression task, or treat it as a `label` and try to guess the `label` from the chemical analysis in a classification task. In both approaches, we will typically remove the score from the tensor of input data and keep it in a separate tensor, so that we can use the score as the ground truth without it being input to our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7L36ssAPsmQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "abce4bdf-e3de-491b-a13a-58722a9526de"
      },
      "source": [
        "data = wineq[:, :-1] #Select all rows and all columns except the last\n",
        "data, data.shape"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 7.0000,  0.2700,  ...,  0.4500,  8.8000],\n",
              "         [ 6.3000,  0.3000,  ...,  0.4900,  9.5000],\n",
              "         ...,\n",
              "         [ 5.5000,  0.2900,  ...,  0.3800, 12.8000],\n",
              "         [ 6.0000,  0.2100,  ...,  0.3200, 11.8000]]), torch.Size([4898, 11]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfmdQ6ccScC4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4eeabedb-d580-41fe-84cf-bb0db48cee02"
      },
      "source": [
        "target = wineq[:, -1] #Select all rows and the last columns\n",
        "target, target.shape"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([6., 6.,  ..., 7., 6.]), torch.Size([4898]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLB4PHp8S56d",
        "colab_type": "text"
      },
      "source": [
        "If we want to transform the `target` tensor in a tensor of labels, we have two options, depending on the strategy or what we use the categorical data for. One is simply to treat labels as an integer vector of scores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ejn1tK0wSwyF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d846776-b3b2-4ad4-f120-af436d656db3"
      },
      "source": [
        "target = wineq[:, -1].long()\n",
        "target"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6, 6,  ..., 7, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDJmU9wYUWH8",
        "colab_type": "text"
      },
      "source": [
        "If targets were string labels, like wine color, assigning an integer number to each string would let us follow the same approach.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCVs0KQjUXuD",
        "colab_type": "text"
      },
      "source": [
        "### One-hot encoding\n",
        "\n",
        "The other approach is to build a one-hot encoding of the scores: that is, encode each of the 10 scores in a vector of 10 elements, with all elements set to 0 but one, at a different index for each score. This way, a score of `1` could be mapped onto the vector `(1,0,0,0,0,0,0,0,0,0)`, a score of `5` onto `(0,0,0,0,1,0,0,0,0,0)`, and so on. Note that the fact that the score corresponds to the index of the nonzero element is purely incidental: we could shuffle the assignment, and nothing would change from a classification standpoint.\n",
        "\n",
        "Keeping wine quality scores in an integer vector of scores induces an ordering on the scores—which might be totally appropriate in this case, since a score of 1 is lower than a score of 4. It also induces some sort of distance between scores: that is, the distance between 1 and 3 is the same as the distance between 2 and 4. If this holds for our quantity, then great. If, on the other hand, scores are purely discrete, like grape variety, one-hot encoding will be a much better fit, as there’s no implied ordering or distance. One-hot encoding is also appropriate for quantitative scores when fractional values in between integer scores, like `2.4`, make no sense for the application—for when the score is either *this* or that.\n",
        "\n",
        "We can achieve one-hot encoding using the `scatter_` method, which fills the tensor with values from a source tensor along the indices provided as arguments:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyVeLBQYT8GT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b5bc3979-c1e6-4615-e3c3-ba563d486db3"
      },
      "source": [
        "target_onehot = torch.zeros(target.shape[0], 10)\n",
        "target_onehot.scatter_(1, target.unsqueeze(1), 1.0)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0.,  ..., 0., 0.],\n",
              "        [0., 0.,  ..., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0.,  ..., 0., 0.],\n",
              "        [0., 0.,  ..., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9l8QSApXOaq",
        "colab_type": "text"
      },
      "source": [
        "Let’s see what `scatter_` does. First, we notice that its name ends with an underscore. As you learned in the previous chapter, this is a convention in PyTorch that indicates the method will not return a new tensor, but will instead modify the tensor in place. The arguments for `scatter_` are as follows:\n",
        "- The dimension along which the following two arguments are specified\n",
        "- A column tensor indicating the indices of the elements to scatter\n",
        "- A tensor containing the elements to scatter or a single scalar to scatter (`1.0` in this case)\n",
        "\n",
        "In other words, the previous invocation reads, “For each row, take the index of the target label (which coincides with the score in our case) and use it as the column index to set the value 1.0.” The end result is a tensor encoding categorical information.\n",
        "The second argument of `scatter_`, the `index` tensor, is required to have the same number of dimensions as the tensor we scatter into. Since `target_onehot` has two dimensions `(4,898 × 10)`, we need to add an extra dummy dimension to target using `unsqueeze`:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYrZdCHNVR-P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2e06883c-7d18-46fa-af1f-722689262b5a"
      },
      "source": [
        "target_unsqueezed = target.unsqueeze(1)\n",
        "target_unsqueezed"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[6],\n",
              "        [6],\n",
              "        ...,\n",
              "        [7],\n",
              "        [6]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIQmlZJpYDyx",
        "colab_type": "text"
      },
      "source": [
        "The call to unsqueeze adds a `singleton` dimension, from a 1D tensor of `4,898` elements to a 2D tensor of size `(4,898 × 1)`, without changing its contents—no extra elements are added; we just decided to use an extra index to access the elements. That is, we access the first element of target as `target[0]` and the first element of its unsqueezed counterpart as `target_unsqueezed[0,0]`.\n",
        "\n",
        "PyTorch allows us to use class indices directly as targets while training neural net- works. However, if we wanted to use the score as a categorical input to the network, we would have to transform it to a one-hot-encoded tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsyHy7liX6cX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}