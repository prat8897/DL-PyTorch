{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyME2kkDasZpTyFD5DPy7sWb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prat8897/DL_PyTorch/blob/master/Chapter5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIo3wZmSc97y",
        "colab_type": "text"
      },
      "source": [
        "# The mechanics of learning\n",
        "\n",
        "This chapter covers\n",
        "- Understanding how algorithms can learn from data\n",
        "- Reframing learning as parameter estimation, using differentiation and gradient descent\n",
        "- Walking through a simple learning algorithm\n",
        "- How PyTorch supports learning with autograd\n",
        "\n",
        "## Learning is just parameter estimation\n",
        "\n",
        "Given input data and the corresponding desired outputs (*ground truth*), as well as initial values for the weights, the model is fed input data (*forward pass*), and a measure of the *error* is evaluated by comparing the resulting outputs to the ground truth. In order to optimize the parameter of the model—its weights—the change in the error following a unit change in weights (that is, the gradient of the error with respect to the parameters) is computed using the chain rule for the derivative of a composite function (*backward pass*). The value of the weights is then updated in the direction that leads to a decrease in the error. The procedure is repeated until the error, evaluated on unseen data, falls below an acceptable level.\n",
        "\n",
        "### A hot problem\n",
        "\n",
        "We just got back from a trip to some obscure location, and we brought back a fancy, wall-mounted analog thermometer. It looks great, and it’s a perfect fit for our living room. Its only flaw is that it doesn’t show units. Not to worry, we’ve got a plan: we’ll build a dataset of readings and corresponding temperature values in our favorite units, choose a model, adjust its weights iteratively until a measure of the error is low enough, and finally be able to interpret the new readings in units we understand.\n",
        "\n",
        "We’ll start by making a note of temperature data in good old Celsius and measurements from our new thermometer, and figure things out. After a couple of weeks, here’s the data [(code/p1ch5/1_parameter_estimation.ipynb)](https://github.com/deep-learning-with-pytorch/dlwpt-code/blob/master/p1ch5/1_parameter_estimation.ipynb):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1o-YxY5a7PG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "torch.set_printoptions(edgeitems=2, linewidth=75)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byJvzgNju2yH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
        "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
        "t_c = torch.tensor(t_c)\n",
        "t_u = torch.tensor(t_u)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0RiTeYDvAsO",
        "colab_type": "text"
      },
      "source": [
        "Here, the `t_c` values are temperatures in Celsius, and the `t_u` values are our unknown units. We can expect noise in both measurements, coming from the devices themselves and from our approximate readings. For convenience, we’ve already put the data into tensors; we’ll use it in a minute.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVMH30YEu9f0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "d76de45d-3cbb-44a8-b26d-4ff291fd8e34"
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.xlabel(\"Measurement\")\n",
        "plt.ylabel(\"Temperature (°Celsius)\")\n",
        "plt.plot(t_u.numpy(), t_c.numpy(), 'o')\n",
        "plt"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'matplotlib.pyplot' from '/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEVxe42rQ2Y4",
        "colab_type": "text"
      },
      "source": [
        "In the absence of further knowledge, we assume the simplest possible model for converting between the two sets of measurements.\n",
        "\n",
        "The two may be linearly related—that is, multiplying `t_u` by a factor and adding a constant, we may get the temperature in Celsius (up to an error that we omit):\n",
        "\n",
        "```\n",
        "t_c = w * t_u + b\n",
        "```\n",
        "\n",
        "(looks just like `y = mx + b`, `m` being the `slope` and `b` being the `coefficient`)\n",
        "\n",
        "We chose to name `w` and `b` after `weight` and `bias`, two very common terms for linear scaling and the additive constant.\n",
        "\n",
        "Now we need to estimate `w` and `b`, the parameters in our model, based on the data we have. We must do it so that temperatures we obtain from running the unknown temperatures `t_u` through the model are close to temperatures we actually measured in Celsius. If that sounds like fitting a line through a set of measurements, well, yes, because that’s exactly what we’re doing. We’ll go through this simple example using PyTorch and realize that training a neural network will essentially involve changing the model for a slightly more elaborate one, with a few (or a metric ton) more parameters.\n",
        "\n",
        "To summarise, we have a model with some unknown parameters, and we need to estimate those parameters so that the error between predicted outputs and measured values is as low as possible. We notice that we still need to exactly define a measure of the error. Such a measure, which we refer to as the loss function, should be high if the error is high and should ideally be as low as possible for a perfect match. Our optimization process should therefore aim at finding `w` and `b` so that the *loss\n",
        "function* is at a minimum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l6iMKiSSEjG",
        "colab_type": "text"
      },
      "source": [
        "## Loss functions\n",
        "\n",
        "A *loss function* (or *cost function*) is a function that computes a single numerical value that the learning process will attempt to minimize. The calculation of loss typically involves taking the difference between the desired outputs for some training samples and the outputs actually produced by the model when fed those samples. In our case, that would be the difference between the predicted temperatures `t_p` output by our model and the actual measurements `t_c` =>  `t_p – t_c`\n",
        "\n",
        "We need to make sure the *loss function* makes the loss positive both when `t_p` is greater than and when it is less than the true `t_c`, since the goal is for `t_p` to match `t_c`. We have a few choices, the most straightforward being `|t_p – t_c|` and `(t_p – t_c)^2`. Based on the mathematical expression we choose, we can emphasize or discount certain errors. Conceptually, a *loss function* is a way of prioritizing which errors to fix from our training samples, so that our parameter updates result in adjustments to the outputs for the highly weighted samples instead of changes to some other samples output that had a smaller loss.\n",
        "\n",
        "\n",
        "Both of the example loss functions have a clear minimum at zero and grow monotonically as the predicted value moves further from the true value in either direction.\n",
        "\n",
        "Because the steepness of the growth also monotonically increases away from the minimum, both of them are said to be *convex*. Since our model is linear, the loss as a function of `w` and `b` is also *convex*. \n",
        "\n",
        "We’ve already created our data tensors, so now let’s write out the model as a Python function:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBjH43BFv7uU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(t_u, w, b):\n",
        "  return (t_u * w) + b"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60abPJhYVskS",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We’re expecting `t_u`, `w`, and `b` to be the *input tensor*, *weight parameter*, and *bias parameter*, respectively.\n",
        "\n",
        "In our model, the parameters will be PyTorch scalars (aka zero-dimensional tensors), and the product operation will use broadcasting to yield\n",
        "the returned tensors.\n",
        "\n",
        "Let's define our loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88cvC5nmUwaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(t_p, t_c):\n",
        "  squared_diffs = (t_p - t_c)**2\n",
        "  return squared_diffs.mean()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRwhHrbFWYIO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Note that we are building a tensor of differences, taking their square element-wise, and finally producing a scalar loss function by averaging all of the elements in the resulting tensor. It is a **mean square loss**.\n",
        "\n",
        "\n",
        "We can now initialize the parameters and invoke the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YALAuHzWTYk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2410fafa-7773-4f3a-ed83-fea0642f799b"
      },
      "source": [
        "w = torch.ones(())\n",
        "b = torch.zeros(())\n",
        "\n",
        "t_p = model(t_u, w, b)\n",
        "t_p"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000,\n",
              "        21.8000, 48.4000, 60.4000, 68.4000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEP5pUguWyAc",
        "colab_type": "text"
      },
      "source": [
        "At this point, `t_p` is the same as `t_u` because all weights are `1` and biases are `0`.\n",
        "\n",
        "This will change once we update `t_p`'s values with the help of our *loss function*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvQmcE2AWp76",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4324ac05-e220-4aca-eede-73a13346f032"
      },
      "source": [
        "loss = loss_fn(t_p, t_c)\n",
        "loss"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1763.8846)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG0az7FpXKrZ",
        "colab_type": "text"
      },
      "source": [
        "## Decreasing loss\n",
        "\n",
        "We’ll optimize the loss function with respect to the parameters using the gradient descent algorithm.\n",
        "\n",
        "The idea of **Gradient Descent** is to compute the rate of change of the loss with respect to each parameter, and modify each parameter in the direction of decreasing loss.\n",
        "\n",
        "We can estimate the rate of change along each parameter by adding a small number to `w` and `b` and seeing how much the loss changes in that neighborhood:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iaa5-cyYKdU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7ac9e6e-ebf8-41d0-d7da-f00816d6aa65"
      },
      "source": [
        "delta = 0.1\n",
        "loss_rate_of_change_w = (loss_fn(model(t_u, w + delta, b), t_c) - loss_fn(model(t_u, w - delta, b), t_c)) / (2.0 * delta)\n",
        "loss_rate_of_change_w"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4517.2979)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdjtmrtseOko",
        "colab_type": "text"
      },
      "source": [
        "This is saying that in the neighborhood of the current values of `w` and `b`, a unit increase in `w` leads to some change in the loss. If the change is negative, then we need to increase `w` to minimize the loss, whereas if the change is positive, we need to decrease `w`. By how much? Applying a change to `w` that is proportional to the rate of change of the loss is a good idea, especially when the loss has several parameters: we apply a change to those that exert a significant change on the loss. It is also wise to change the parameters slowly in general, because the rate of change could be dramatically different at a distance from the neighborhood of the current `w` value. Therefore, we typically should scale the rate of change by a small factor. This scaling factor has many names; the one we use in machine learning is `learning_rate`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0Un6YQ6eLHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 1e-2\n",
        "w = w - learning_rate * loss_rate_of_change_w"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR663jluhaXt",
        "colab_type": "text"
      },
      "source": [
        "We can do the same with `b`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1Ql2LHfhYjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_rate_of_change_b = (loss_fn(model(t_u, w, b + delta), t_c) - loss_fn(model(t_u, w, b - delta), t_c)) / (2.0 * delta)\n",
        "b = b - learning_rate * loss_rate_of_change_b"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB4QaqwihhM8",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "This represents just **one** basic parameter-update step for gradient descent. By reiterating these evaluations (and provided we choose a small enough learning rate), we will converge to an optimal value of the parameters for which the loss computed on the given data is minimal. We’ll show the complete iterative process soon, but the way we just computed our rates of change is rather crude and needs an upgrade before we move on. Let’s see why and how."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McbOk1SZhpo7",
        "colab_type": "text"
      },
      "source": [
        "## Getting analytical\n",
        "\n",
        "\n",
        "Computing the rate of change by using repeated evaluations of the model and loss in order to probe the behavior of the loss function in the neighborhood of `w` and `b` doesn’t scale well to models with many parameters. Also, it is not always clear how large the neighborhood should be. We chose `delta` equal to `0.1` in the previous section, but it all depends on the shape of the loss as a function of `w` and `b`. If the loss changes too quickly compared to delta, we won’t have a very good idea of in which direction the loss is decreasing the most.\n",
        "\n",
        "What if we could make the neighborhood infinitesimally small? That’s exactly what happens when we analytically take the derivative of the loss with respect to a parameter. In a model with two or more parameters like the one we’re dealing with, we compute the individual derivatives of the loss with respect to each parameter and put them in a vector of derivatives: the *gradient*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_jIy4xsttuX",
        "colab_type": "text"
      },
      "source": [
        "### Computing the Derivatives\n",
        "\n",
        "In order to compute the derivative of the loss with respect to a parameter, we can apply the chain rule and compute the derivative of the loss with respect to its input (which is the output of the model), times the derivative of the model with respect to the parameter.\n",
        "\n",
        "Output of model `w * t_u + b` is `t_p`, therefore\n",
        "\n",
        "```\n",
        "d(loss_fn)/dw = d(loss_fn)/d(w * t_u + b) * d(w * t_u + b)/dw\n",
        "```\n",
        "becomes \n",
        "```\n",
        "d(loss_fn)/dw = d(loss_fn)/d(t_p) * d(t_p)/dw\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Recall that our model is a linear function, and our loss is a sum of squares. Let’s figure out the expressions for the derivatives. Recalling the expression for the loss:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def loss_fn(t_p, t_c):\n",
        "  squared_diffs = (t_p - t_c)**2\n",
        "  return squared_diffs.mean()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTkMT7sErfmt",
        "colab_type": "text"
      },
      "source": [
        "Remembering that `d(x^2) / dx = 2x`, we get"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9pQ5_5_heyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dloss_fn(t_p, t_c):\n",
        "  dsq_diffs = 2 * (t_p - t_c) / t_p.size(0) #The division is from the derivative of mean.\n",
        "  return dsq_diffs"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cnho7HvTsfIW",
        "colab_type": "text"
      },
      "source": [
        "### Applying the derivatives to the model\n",
        "\n",
        "For the model, recalling that our model is\n",
        "\n",
        "```\n",
        "def model(t_u, w, b):\n",
        "    return w * t_u + b\n",
        "```\n",
        "\n",
        "we get these derivatives:\n",
        "\n",
        "\n",
        "`d(w * t_u + b) / dw = t_u`\n",
        "\n",
        "`d(w * t_u + b) / db = 1`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggZaGy0csdVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dmodel_dw(t_u, w, b):\n",
        "  return t_u"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tpPuG4is4-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dmodel_db(t_u, w, b):\n",
        "  return 1.0"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WpCsPuntewz",
        "colab_type": "text"
      },
      "source": [
        "### Defining the **Gradient Function**\n",
        "\n",
        "We take derivatives of the loss function with respect to `w` and `b`:\n",
        "\n",
        "`d(loss_fn)/dw = d(loss_fn)/d(t_p) * d(t_p)/dw`\n",
        "\n",
        "`d(loss_fn)/db = d(loss_fn)/d(t_p) * d(t_p)/db`\n",
        "\n",
        "Putting all of this together, the function returning the gradient of the loss with respect to `w` and `b` is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp4Hl6Fks6zG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grad_fn(t_u, t_c, t_p, w, b):\n",
        "  dloss_dtp = dloss_fn(t_p, t_c)\n",
        "  dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)\n",
        "  dloss_db = dloss_dtp * dmodel_db(t_u, w, b)\n",
        "  return torch.stack([dloss_dw.sum(), dloss_db.sum()])"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vCIqj5t5hjJ",
        "colab_type": "text"
      },
      "source": [
        "The summation `dloss_dw.sum()` and `dloss_db.sum()` is the reverse of the broadcasting we implicitly do when applying the parameters to an entire vector of inputs in the model.\n",
        "\n",
        "We’re averaging (that is, summing and dividing by a constant) over all the data points to get a single scalar quantity for each partial derivative of the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIlOJrnQ5oOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}