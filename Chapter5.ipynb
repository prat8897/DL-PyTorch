{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_l6iMKiSSEjG",
        "oG0az7FpXKrZ",
        "g_jIy4xsttuX",
        "Cnho7HvTsfIW",
        "_WpCsPuntewz"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPpIZ4Xq3g3wVoDscfwezSG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prat8897/DL_PyTorch/blob/master/Chapter5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIo3wZmSc97y",
        "colab_type": "text"
      },
      "source": [
        "# The mechanics of learning\n",
        "\n",
        "This chapter covers\n",
        "- Understanding how algorithms can learn from data\n",
        "- Reframing learning as parameter estimation, using differentiation and gradient descent\n",
        "- Walking through a simple learning algorithm\n",
        "- How PyTorch supports learning with autograd\n",
        "\n",
        "## Learning is just parameter estimation\n",
        "\n",
        "Given input data and the corresponding desired outputs (*ground truth*), as well as initial values for the weights, the model is fed input data (*forward pass*), and a measure of the *error* is evaluated by comparing the resulting outputs to the ground truth. In order to optimize the parameter of the model—its weights—the change in the error following a unit change in weights (that is, the gradient of the error with respect to the parameters) is computed using the chain rule for the derivative of a composite function (*backward pass*). The value of the weights is then updated in the direction that leads to a decrease in the error. The procedure is repeated until the error, evaluated on unseen data, falls below an acceptable level.\n",
        "\n",
        "### A hot problem\n",
        "\n",
        "We just got back from a trip to some obscure location, and we brought back a fancy, wall-mounted analog thermometer. It looks great, and it’s a perfect fit for our living room. Its only flaw is that it doesn’t show units. Not to worry, we’ve got a plan: we’ll build a dataset of readings and corresponding temperature values in our favorite units, choose a model, adjust its weights iteratively until a measure of the error is low enough, and finally be able to interpret the new readings in units we understand.\n",
        "\n",
        "We’ll start by making a note of temperature data in good old Celsius and measurements from our new thermometer, and figure things out. After a couple of weeks, here’s the data [(code/p1ch5/1_parameter_estimation.ipynb)](https://github.com/deep-learning-with-pytorch/dlwpt-code/blob/master/p1ch5/1_parameter_estimation.ipynb):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1o-YxY5a7PG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "torch.set_printoptions(edgeitems=2, linewidth=75)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byJvzgNju2yH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
        "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
        "t_c = torch.tensor(t_c)\n",
        "t_u = torch.tensor(t_u)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0RiTeYDvAsO",
        "colab_type": "text"
      },
      "source": [
        "Here, the `t_c` values are temperatures in Celsius, and the `t_u` values are our unknown units. We can expect noise in both measurements, coming from the devices themselves and from our approximate readings. For convenience, we’ve already put the data into tensors; we’ll use it in a minute.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVMH30YEu9f0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "01d7013c-0fd6-4802-fe55-9e8346d05560"
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.xlabel(\"Measurement\")\n",
        "plt.ylabel(\"Temperature (°Celsius)\")\n",
        "plt.plot(t_u.numpy(), t_c.numpy(), 'o')\n",
        "plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'matplotlib.pyplot' from '/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEVxe42rQ2Y4",
        "colab_type": "text"
      },
      "source": [
        "In the absence of further knowledge, we assume the simplest possible model for converting between the two sets of measurements.\n",
        "\n",
        "The two may be linearly related—that is, multiplying `t_u` by a factor and adding a constant, we may get the temperature in Celsius (up to an error that we omit):\n",
        "\n",
        "```\n",
        "t_c = w * t_u + b\n",
        "```\n",
        "\n",
        "(looks just like `y = mx + b`, `m` being the `slope` and `b` being the `coefficient`)\n",
        "\n",
        "We chose to name `w` and `b` after `weight` and `bias`, two very common terms for linear scaling and the additive constant.\n",
        "\n",
        "Now we need to estimate `w` and `b`, the parameters in our model, based on the data we have. We must do it so that temperatures we obtain from running the unknown temperatures `t_u` through the model are close to temperatures we actually measured in Celsius. If that sounds like fitting a line through a set of measurements, well, yes, because that’s exactly what we’re doing. We’ll go through this simple example using PyTorch and realize that training a neural network will essentially involve changing the model for a slightly more elaborate one, with a few (or a metric ton) more parameters.\n",
        "\n",
        "To summarise, we have a model with some unknown parameters, and we need to estimate those parameters so that the error between predicted outputs and measured values is as low as possible. We notice that we still need to exactly define a measure of the error. Such a measure, which we refer to as the loss function, should be high if the error is high and should ideally be as low as possible for a perfect match. Our optimization process should therefore aim at finding `w` and `b` so that the *loss\n",
        "function* is at a minimum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l6iMKiSSEjG",
        "colab_type": "text"
      },
      "source": [
        "## Loss functions\n",
        "\n",
        "A *loss function* (or *cost function*) is a function that computes a single numerical value that the learning process will attempt to minimize. The calculation of loss typically involves taking the difference between the desired outputs for some training samples and the outputs actually produced by the model when fed those samples. In our case, that would be the difference between the predicted temperatures `t_p` output by our model and the actual measurements `t_c` =>  `t_p – t_c`\n",
        "\n",
        "We need to make sure the *loss function* makes the loss positive both when `t_p` is greater than and when it is less than the true `t_c`, since the goal is for `t_p` to match `t_c`. We have a few choices, the most straightforward being `|t_p – t_c|` and `(t_p – t_c)^2`. Based on the mathematical expression we choose, we can emphasize or discount certain errors. Conceptually, a *loss function* is a way of prioritizing which errors to fix from our training samples, so that our parameter updates result in adjustments to the outputs for the highly weighted samples instead of changes to some other samples output that had a smaller loss.\n",
        "\n",
        "\n",
        "Both of the example loss functions have a clear minimum at zero and grow monotonically as the predicted value moves further from the true value in either direction.\n",
        "\n",
        "Because the steepness of the growth also monotonically increases away from the minimum, both of them are said to be *convex*. Since our model is linear, the loss as a function of `w` and `b` is also *convex*. \n",
        "\n",
        "We’ve already created our data tensors, so now let’s write out the model as a Python function:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBjH43BFv7uU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(t_u, w, b):\n",
        "  return (t_u * w) + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60abPJhYVskS",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We’re expecting `t_u`, `w`, and `b` to be the *input tensor*, *weight parameter*, and *bias parameter*, respectively.\n",
        "\n",
        "In our model, the parameters will be PyTorch scalars (aka zero-dimensional tensors), and the product operation will use broadcasting to yield\n",
        "the returned tensors.\n",
        "\n",
        "Let's define our loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88cvC5nmUwaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(t_p, t_c):\n",
        "  squared_diffs = (t_p - t_c)**2\n",
        "  return squared_diffs.mean()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRwhHrbFWYIO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Note that we are building a tensor of differences, taking their square element-wise, and finally producing a scalar loss function by averaging all of the elements in the resulting tensor. It is a **mean square loss**.\n",
        "\n",
        "\n",
        "We can now initialize the parameters and invoke the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YALAuHzWTYk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b6ad3583-098b-4fa8-cf9a-d6544012403d"
      },
      "source": [
        "w = torch.ones(())\n",
        "b = torch.zeros(())\n",
        "\n",
        "t_p = model(t_u, w, b)\n",
        "t_p"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000,\n",
              "        21.8000, 48.4000, 60.4000, 68.4000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEP5pUguWyAc",
        "colab_type": "text"
      },
      "source": [
        "At this point, `t_p` is the same as `t_u` because all weights are `1` and biases are `0`.\n",
        "\n",
        "This will change once we update `t_p`'s values with the help of our *loss function*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvQmcE2AWp76",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f3478aa-561f-4c5e-9124-9a07cda1f8d7"
      },
      "source": [
        "loss = loss_fn(t_p, t_c)\n",
        "loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1763.8846)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG0az7FpXKrZ",
        "colab_type": "text"
      },
      "source": [
        "## Decreasing loss\n",
        "\n",
        "We’ll optimize the loss function with respect to the parameters using the gradient descent algorithm.\n",
        "\n",
        "The idea of **Gradient Descent** is to compute the rate of change of the loss with respect to each parameter, and modify each parameter in the direction of decreasing loss.\n",
        "\n",
        "We can estimate the rate of change along each parameter by adding a small number to `w` and `b` and seeing how much the loss changes in that neighborhood:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iaa5-cyYKdU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "38c596f3-8bc4-47ee-c521-09760d036095"
      },
      "source": [
        "delta = 0.1\n",
        "loss_rate_of_change_w = (loss_fn(model(t_u, w + delta, b), t_c) - loss_fn(model(t_u, w - delta, b), t_c)) / (2.0 * delta)\n",
        "loss_rate_of_change_w"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4517.2979)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdjtmrtseOko",
        "colab_type": "text"
      },
      "source": [
        "This is saying that in the neighborhood of the current values of `w` and `b`, a unit increase in `w` leads to some change in the loss. If the change is negative, then we need to increase `w` to minimize the loss, whereas if the change is positive, we need to decrease `w`. By how much? Applying a change to `w` that is proportional to the rate of change of the loss is a good idea, especially when the loss has several parameters: we apply a change to those that exert a significant change on the loss. It is also wise to change the parameters slowly in general, because the rate of change could be dramatically different at a distance from the neighborhood of the current `w` value. Therefore, we typically should scale the rate of change by a small factor. This scaling factor has many names; the one we use in machine learning is `learning_rate`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0Un6YQ6eLHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 1e-2\n",
        "w = w - learning_rate * loss_rate_of_change_w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR663jluhaXt",
        "colab_type": "text"
      },
      "source": [
        "We can do the same with `b`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1Ql2LHfhYjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_rate_of_change_b = (loss_fn(model(t_u, w, b + delta), t_c) - loss_fn(model(t_u, w, b - delta), t_c)) / (2.0 * delta)\n",
        "b = b - learning_rate * loss_rate_of_change_b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB4QaqwihhM8",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "This represents just **one** basic parameter-update step for gradient descent. By reiterating these evaluations (and provided we choose a small enough learning rate), we will converge to an optimal value of the parameters for which the loss computed on the given data is minimal. We’ll show the complete iterative process soon, but the way we just computed our rates of change is rather crude and needs an upgrade before we move on. Let’s see why and how."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McbOk1SZhpo7",
        "colab_type": "text"
      },
      "source": [
        "## Getting analytical\n",
        "\n",
        "\n",
        "Computing the rate of change by using repeated evaluations of the model and loss in order to probe the behavior of the loss function in the neighborhood of `w` and `b` doesn’t scale well to models with many parameters. Also, it is not always clear how large the neighborhood should be. We chose `delta` equal to `0.1` in the previous section, but it all depends on the shape of the loss as a function of `w` and `b`. If the loss changes too quickly compared to delta, we won’t have a very good idea of in which direction the loss is decreasing the most.\n",
        "\n",
        "What if we could make the neighborhood infinitesimally small? That’s exactly what happens when we analytically take the derivative of the loss with respect to a parameter. In a model with two or more parameters like the one we’re dealing with, we compute the individual derivatives of the loss with respect to each parameter and put them in a vector of derivatives: the *gradient*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_jIy4xsttuX",
        "colab_type": "text"
      },
      "source": [
        "### Computing the Derivatives\n",
        "\n",
        "In order to compute the derivative of the loss with respect to a parameter, we can apply the chain rule and compute the derivative of the loss with respect to its input (which is the output of the model), times the derivative of the model with respect to the parameter.\n",
        "\n",
        "Output of model `w * t_u + b` is `t_p`, therefore\n",
        "\n",
        "```\n",
        "d(loss_fn)/dw = d(loss_fn)/d(w * t_u + b) * d(w * t_u + b)/dw\n",
        "```\n",
        "becomes \n",
        "```\n",
        "d(loss_fn)/dw = d(loss_fn)/d(t_p) * d(t_p)/dw\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Recall that our model is a linear function, and our loss is a sum of squares. Let’s figure out the expressions for the derivatives. Recalling the expression for the loss:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def loss_fn(t_p, t_c):\n",
        "  squared_diffs = (t_p - t_c)**2\n",
        "  return squared_diffs.mean()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTkMT7sErfmt",
        "colab_type": "text"
      },
      "source": [
        "Remembering that `d(x^2) / dx = 2x`, we get"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9pQ5_5_heyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dloss_fn(t_p, t_c):\n",
        "  dsq_diffs = 2 * (t_p - t_c) / t_p.size(0) #The division is from the derivative of mean.\n",
        "  return dsq_diffs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cnho7HvTsfIW",
        "colab_type": "text"
      },
      "source": [
        "### Applying the derivatives to the model\n",
        "\n",
        "For the model, recalling that our model is\n",
        "\n",
        "```\n",
        "def model(t_u, w, b):\n",
        "    return w * t_u + b\n",
        "```\n",
        "\n",
        "we get these derivatives:\n",
        "\n",
        "\n",
        "`d(w * t_u + b) / dw = t_u`\n",
        "\n",
        "`d(w * t_u + b) / db = 1`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggZaGy0csdVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dmodel_dw(t_u, w, b):\n",
        "  return t_u"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tpPuG4is4-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dmodel_db(t_u, w, b):\n",
        "  return 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WpCsPuntewz",
        "colab_type": "text"
      },
      "source": [
        "### Defining the **Gradient Function**\n",
        "\n",
        "We take derivatives of the loss function with respect to `w` and `b`:\n",
        "\n",
        "`d(loss_fn)/dw = d(loss_fn)/d(t_p) * d(t_p)/dw`\n",
        "\n",
        "`d(loss_fn)/db = d(loss_fn)/d(t_p) * d(t_p)/db`\n",
        "\n",
        "Putting all of this together, the function returning the gradient of the loss with respect to `w` and `b` is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp4Hl6Fks6zG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grad_fn(t_u, t_c, t_p, w, b):\n",
        "  dloss_dtp = dloss_fn(t_p, t_c)\n",
        "  dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)\n",
        "  dloss_db = dloss_dtp * dmodel_db(t_u, w, b)\n",
        "  return torch.stack([dloss_dw.sum(), dloss_db.sum()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vCIqj5t5hjJ",
        "colab_type": "text"
      },
      "source": [
        "The summation `dloss_dw.sum()` and `dloss_db.sum()` is the reverse of the broadcasting we implicitly do when applying the parameters to an entire vector of inputs in the model.\n",
        "\n",
        "We’re averaging (that is, summing and dividing by a constant) over all the data points to get a single scalar quantity for each partial derivative of the loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS0IEAMbOZUL",
        "colab_type": "text"
      },
      "source": [
        "## Iterating to fit the model\n",
        "\n",
        "\n",
        "We now have everything in place to optimize our parameters. Starting from a tentative value for a parameter, we can iteratively apply updates to it for a fixed number of iterations, or until `w` and `b` stop changing. There are several stopping criteria; for now, we’ll stick to a fixed number of iterations.\n",
        "\n",
        "### The Training Loop\n",
        "\n",
        "Let’s introduce another piece of terminology. We call a training iteration during which we update the parameters for all of our training samples an **epoch**.\n",
        "\n",
        "The complete training loop looks like this [(code/p1ch5/1_parameter_estimation.ipynb)](https://github.com/deep-learning-with-pytorch/dlwpt-code/blob/master/p1ch5/1_parameter_estimation.ipynb):\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIlOJrnQ5oOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_loop(n_epochs, learning_rate, params, t_u, t_c, print_params=True):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    w, b = params\n",
        "    t_p = model(t_u, w, b) #Forward Pass\n",
        "    loss = loss_fn(t_p, t_c)\n",
        "    grad = grad_fn(t_u, t_c, t_p, w, b) #Backward Pass\n",
        "    params = params - learning_rate * grad\n",
        "\n",
        "    if epoch in {1, 2, 3, 10, 11, 99, 100, 4000, 5000}:  # verbose printing\n",
        "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "            if print_params:\n",
        "                print('    Params:', params)\n",
        "                print('    Grad:  ', grad)\n",
        "    if epoch in {4, 12, 101}:\n",
        "        print('...')\n",
        "\n",
        "    if not torch.isfinite(loss).all():\n",
        "        break  # end of verbose printing\n",
        "\n",
        "  return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnCTuyBAQMHU",
        "colab_type": "text"
      },
      "source": [
        "Let’s invoke our training loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmzlAYg5QLef",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "1934c5bf-8c84-43e7-c8c3-1eea1d4dca04"
      },
      "source": [
        "training_loop(\n",
        "  n_epochs = 100,\n",
        "  learning_rate = 1e-2,\n",
        "  params = torch.tensor([1.0, 0.0]), t_u = t_u,\n",
        "  t_c = t_c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss 1763.884644\n",
            "    Params: tensor([-44.1730,  -0.8260])\n",
            "    Grad:   tensor([4517.2969,   82.6000])\n",
            "Epoch 2, Loss 5802485.500000\n",
            "    Params: tensor([2568.4014,   45.1637])\n",
            "    Grad:   tensor([-261257.4219,   -4598.9712])\n",
            "Epoch 3, Loss 19408035840.000000\n",
            "    Params: tensor([-148527.7344,   -2616.3933])\n",
            "    Grad:   tensor([15109614.0000,   266155.7188])\n",
            "...\n",
            "Epoch 10, Loss 90901154706620645225508955521810432.000000\n",
            "    Params: tensor([3.2144e+17, 5.6621e+15])\n",
            "    Grad:   tensor([-3.2700e+19, -5.7600e+17])\n",
            "Epoch 11, Loss inf\n",
            "    Params: tensor([-1.8590e+19, -3.2746e+17])\n",
            "    Grad:   tensor([1.8912e+21, 3.3313e+19])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.8590e+19, -3.2746e+17])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_MLK1C0QWaf",
        "colab_type": "text"
      },
      "source": [
        "### Overtraining\n",
        "\n",
        "What happened? Our training process literally blew up, leading to losses becoming `inf`. This is a clear sign that params is receiving updates that are too large, and their values start oscillating back and forth as each update overshoots and the next overcorrects even more. The optimization process is unstable: it *diverges* instead of *converging* to a minimum. We want to see smaller and smaller updates to params, not larger.\n",
        "\n",
        "How can we limit the magnitude of `learning_rate * grad`? We could simply choose a smaller `learning_rate`, and indeed, the `learning rate` is one of the things we typically change when training does not go as well as we would like. We usually change learning rates by orders of magnitude, so we might try with `1e-3` or `1e-4`, which would decrease the magnitude of the updates by orders of magnitude. Let’s go with `1e-4` and see how it works out:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpyfpTBFQQ21",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "d2cdbe6f-5dd0-449d-dc1a-9586854ab4de"
      },
      "source": [
        "training_loop(\n",
        "  n_epochs = 100,\n",
        "  learning_rate = 1e-4,\n",
        "  params = torch.tensor([1.0, 0.0]), t_u = t_u,\n",
        "  t_c = t_c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss 1763.884644\n",
            "    Params: tensor([ 0.5483, -0.0083])\n",
            "    Grad:   tensor([4517.2969,   82.6000])\n",
            "Epoch 2, Loss 323.090546\n",
            "    Params: tensor([ 0.3623, -0.0118])\n",
            "    Grad:   tensor([1859.5493,   35.7843])\n",
            "Epoch 3, Loss 78.929634\n",
            "    Params: tensor([ 0.2858, -0.0135])\n",
            "    Grad:   tensor([765.4667,  16.5122])\n",
            "...\n",
            "Epoch 10, Loss 29.105242\n",
            "    Params: tensor([ 0.2324, -0.0166])\n",
            "    Grad:   tensor([1.4803, 3.0544])\n",
            "Epoch 11, Loss 29.104168\n",
            "    Params: tensor([ 0.2323, -0.0169])\n",
            "    Grad:   tensor([0.5781, 3.0384])\n",
            "...\n",
            "Epoch 99, Loss 29.023582\n",
            "    Params: tensor([ 0.2327, -0.0435])\n",
            "    Grad:   tensor([-0.0533,  3.0226])\n",
            "Epoch 100, Loss 29.022669\n",
            "    Params: tensor([ 0.2327, -0.0438])\n",
            "    Grad:   tensor([-0.0532,  3.0226])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.2327, -0.0438])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CCoq4ZXRTKZ",
        "colab_type": "text"
      },
      "source": [
        "Nice—the behavior is now stable. But there’s another problem: the updates to parameters are very small, so the loss decreases very slowly and eventually stalls. We could obviate this issue by making `learning_rate` adaptive: that is, change according to the magnitude of updates.\n",
        "\n",
        "\n",
        "However, there’s another potential troublemaker in the update term: the *gradient* itself. Let’s go back and look at `grad` at `epoch 1` during optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SFHPoA-Ribf",
        "colab_type": "text"
      },
      "source": [
        "### Normalizing inputs\n",
        "\n",
        "We can see that the first-epoch gradient for the weight is about 50 times larger than the gradient for the bias. This means the weight and bias live in differently scaled spaces. If this is the case, a learning rate that’s large enough to meaningfully update one will be so large as to be unstable for the other; and a rate that’s appropriate for the other won’t be large enough to meaningfully change the first. That means we’re not going to be able to update our parameters unless we change something about our formulation of the problem. We could have individual learning rates for each parameter, but for models with many parameters, this would be too much to bother with.\n",
        "\n",
        "There’s a simpler way to keep things in check: changing the inputs so that the gradients aren’t quite so different. We can make sure the range of the input doesn’t get too far from the range of `–1.0 to 1.0`, roughly speaking. In our case, we can achieve something close enough to that by simply multiplying `t_u` by `0.1`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7EL0I-JRJR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_un = 0.1 * t_u"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwj-BuvVSAa3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Here, we denote the normalized version of `t_u` by appending an `n` to the variable name. At this point, we can run the training loop on our normalized input:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5UpIKhRR-Nn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "fa503188-6aed-49bf-fd07-bc6eb3f92d53"
      },
      "source": [
        "training_loop(\n",
        "  n_epochs = 100,\n",
        "  learning_rate = 1e-2, #1e-2 instead of 1e-4\n",
        "  params = torch.tensor([1.0, 0.0]), t_u = t_un, #t_un instead of t_u\n",
        "  t_c = t_c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss 80.364342\n",
            "    Params: tensor([1.7761, 0.1064])\n",
            "    Grad:   tensor([-77.6140, -10.6400])\n",
            "Epoch 2, Loss 37.574917\n",
            "    Params: tensor([2.0848, 0.1303])\n",
            "    Grad:   tensor([-30.8623,  -2.3864])\n",
            "Epoch 3, Loss 30.871077\n",
            "    Params: tensor([2.2094, 0.1217])\n",
            "    Grad:   tensor([-12.4631,   0.8587])\n",
            "...\n",
            "Epoch 10, Loss 29.030487\n",
            "    Params: tensor([ 2.3232, -0.0710])\n",
            "    Grad:   tensor([-0.5355,  2.9295])\n",
            "Epoch 11, Loss 28.941875\n",
            "    Params: tensor([ 2.3284, -0.1003])\n",
            "    Grad:   tensor([-0.5240,  2.9264])\n",
            "...\n",
            "Epoch 99, Loss 22.214186\n",
            "    Params: tensor([ 2.7508, -2.4910])\n",
            "    Grad:   tensor([-0.4453,  2.5208])\n",
            "Epoch 100, Loss 22.148710\n",
            "    Params: tensor([ 2.7553, -2.5162])\n",
            "    Grad:   tensor([-0.4446,  2.5165])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 2.7553, -2.5162])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMb1eiiPTsN0",
        "colab_type": "text"
      },
      "source": [
        "Even though we set our learning rate back to `1e-2`, parameters don’t blow up during iterative updates. Let’s take a look at the gradients: they’re of similar magnitude, so using a single `learning_rate` for both parameters works just fine. We could probably do a better job of normalization than a simple rescaling by a factor of 10, but since doing so is good enough for our needs, we’re going to stick with that for now.\n",
        "\n",
        "The normalization here absolutely helps get the network trained, but you could make an argument that it’s not strictly needed to optimize the parameters for this particular problem. That’s absolutely true! This problem is small enough that there are numerous ways to beat the parameters into submission. However, for larger, more sophisticated problems, normalization is an easy and effective (if not crucial!) tool to use to improve model convergence.\n",
        "\n",
        "Let’s run the loop for enough iterations to see the changes in params get small. We’ll change `n_epochs` to `5,000`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou0jq4cvSKYG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e802caa6-94a9-46d2-8fc8-aebada11ec72"
      },
      "source": [
        "params = training_loop(\n",
        "  n_epochs = 5000,\n",
        "  learning_rate = 1e-2,\n",
        "  params = torch.tensor([1.0, 0.0]),\n",
        "  t_u = t_un,\n",
        "  t_c = t_c,\n",
        "  print_params = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss 80.364342\n",
            "Epoch 2, Loss 37.574917\n",
            "Epoch 3, Loss 30.871077\n",
            "...\n",
            "Epoch 10, Loss 29.030487\n",
            "Epoch 11, Loss 28.941875\n",
            "...\n",
            "Epoch 99, Loss 22.214186\n",
            "Epoch 100, Loss 22.148710\n",
            "...\n",
            "Epoch 4000, Loss 2.927680\n",
            "Epoch 5000, Loss 2.927648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9MN_tzIURXp",
        "colab_type": "text"
      },
      "source": [
        "Good: our loss decreases while we change parameters along the direction of gradient descent. It doesn’t go exactly to zero; this could mean there aren’t enough iterations to converge to zero, or that the data points don’t sit exactly on a line. As we anticipated, our measurements were not perfectly accurate, or there was noise involved in the reading.\n",
        "\n",
        "But look: the values for `w` and `b` look an awful lot like the numbers we need to use to convert Celsius to Fahrenheit (after accounting for our earlier normalization when we multiplied our inputs by `0.1`). The exact values would be `w=5.5556` and `b=- 17.7778`. Our fancy thermometer was showing temperatures in Fahrenheit the whole time. No big discovery, except that our gradient descent optimization process works!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mll38vXGUkUP",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing (again)\n",
        "\n",
        "Let’s revisit something we did right at the start: plotting our data. This is the first thing anyone doing data science should do. Always plot the data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVIWxyklUJws",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "b58a0cf5-c2d8-4acc-ad76-58d818710ccb"
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "t_p = model(t_un, *params)\n",
        "#fig = plt.figure(dpi=600)\n",
        "plt.xlabel(\"Temperature (°Fahrenheit)\")\n",
        "plt.ylabel(\"Temperature (°Celsius)\")\n",
        "plt.plot(t_u.numpy(), t_p.detach().numpy())\n",
        "plt.plot(t_u.numpy(), t_c.numpy(), 'o')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7d00ffd9b0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd5xU1fnH8c9DR8oigojCig07zRVFLCCg2KMxGDVqbGiMP1tMVDSI2Igau0axm9hIorEXQBTFiC6IC1IUFRWkWgApsuw+vz/u3dnZzZYZdmbvzsz3/Xrta+acuXPvc3dn7zP3nnPPMXdHRERyT6OoAxARkWgoAYiI5CglABGRHKUEICKSo5QARERyVJOoA0hGhw4dvFu3blGHISKSUaZNm7bC3TtWrs+oBNCtWzcKCwujDkNEJKOY2VdV1esSkIhIjlICEBHJUUoAIiI5SglARCRHKQGIiOQoJQARkYasaBzctgeMahc8Fo1L2aozqhuoiEhOKRoHL14AxeuC8spvgjJAj2F1Xr3OAEREGqqJo8sP/mWK1wX1KaAEICLSUK1cmFx9kpQAREQaqrwuydUnSQlARKShGjQSmrasWNe0ZVCfAkoAIiINVY9hcNSdkNcVsODxqDtT0gAM6gUkItKw9RiWsgN+ZToDEBHJUUoAIiI5SglARKQBe+/zFVz2ryLWF5ekfN1qAxARaYDWbSih35iJ/Li2GICLhuxE57yWtbwrOUoAIiINzIPvfMF1L8+JlZ89b7+UH/xBCUBEpMH45vu1HHDTpFh5WEEXbjq+Z9q2pwQgIhIxd+fsx6cxYc7SWN0HVw5iyzYt0rpdJQARkQi989lyTnnog1j5L7/ckxP2zq+XbSsBiIhEYO2GjRRcN4G1G4LePdt3bMVrFx5Isyb11zlTCUBEpJ7d+9Z8bnptXqz8/O/707Nru3qPI+0JwMy6Ao8DnQAHxrr7HWY2CjgbWB4uOsLdX0l3PCIiUVmwYg0DbnkrVj55n3yuP3bPyOKpjzOAjcAf3H26mbUBppnZ+PC129z9lnqIQUQkMu7OaY98yORPl8fqCq8aTIfWzSOMqh4SgLsvBhaHz1eb2Rxgm3RvV0SkIZg0bxmnP/JhrHzrsJ4c1yc14/nXVb22AZhZN6A3MBXoD5xvZqcChQRnCT9U8Z7hwHCA/Pz6aRkXEamrn37eSO/Rb1Bc4gDsslUbXvy//WnauOGMwGPuXj8bMmsNvA1c7+7PmlknYAVBu8C1QGd3P6OmdRQUFHhhYWH6gxURqYM7JnzGbRM+jZVf+r/92WObvMjiMbNp7l5Qub5ezgDMrCnwb+AJd38WwN2Xxr3+APBSfcQiIpIuny//iUF/fTtWPr1/N64+avcII6pZffQCMuAhYI673xpX3zlsHwA4FpiV7lhERNKhtNQ5+cGp/PeL72J10/88hPatmkUYVe3q4wygP3AKMNPMZoR1I4ATzawXwSWgBcA59RCLiEhKjZ+9lLMfL780feeJvTm659YRRpS4+ugF9C5gVbykPv8ikrFWrS+mx6g3YuUeXfJ49nf70aQBNfLWRncCi4gk6a9vzOOuN+fHyq9eeAC7dm4bYUSbRglARCRBny1dzZDbJsfK5xy4PVccvmuEEdWNEoCISC1KSp1h9/+XaV+V36o0Y+QQ2m3WsBt5a6MEICJSg1dnLuZ3T0yPle89uQ+H79k5wohSRwlARKQKK9cW03N0eSPvXttuzrhz+tG4UVV9WjKTEoCISCU3vjqH+9/+IlZ+4+ID6d6pTYQRpYcSgIhIaO6SVQy9/Z1Y+fyBO3LpoTtHGFF6KQGISM4rKXWOvXcKRQtXxuo+vvoQ8lo2jTCq9FMCEJGc9uLH3/J/T30UK489ZS8O2X2rCCOqP0oAIpKTflizgd7Xjo+V992+PU+etS+NsqiRtzZKACKSc0a/OJuHp3wZK0+45CB23LJ1hBFFQwlARHLGrEUrOfKud2PliwbvxEWDu0cYUbSUAEQk620sKeXIu95l7pLVADRpZHw0cghtWmR3I29tlABEJKs999FCLn7m41j54d8WcPAunSKMqOFQAhCRhqNoHEwcDSsXQl4XGDQSegzbpFV999PP7HXdhFj5wO4deez0vQnmqBJQAhCRhqJoHLx4ARSvC8orvwnKkHQS+PN/ZvH397+Kld+6dADdOrRKVaRZQwlARBqGiaPLD/5litcF9QkmgKKFP3L03VNi5T8eujO/H7hjKqPMKkoAItIwrFyYXH2c4pJSht4+mc+XrwFgs2aN+fDKwbRqrkNcTfTbEZGGIa9LcNmnqvoajCv8hj/9qyhWfvyMvhzYvWOqo8tKSgAi0jAMGlmxDQCgacugvgrLVq+n7/UTY+XBu27JA6cWqJE3CTUmADNrARwJHABsDawDZgEvu/sniWzAzLoCjwOdAAfGuvsdZtYeeAboBiwAhrn7D9WtR0SyXNl1/gR6AV3+7yKe/rD8bOGdPw2ka/vN6ivSrGHuXvULZtcQHPzfAqYBy4AWQHdgYPj8D+5eVOUKytfTGejs7tPNrE24rl8AvwW+d/cxZnY5sLm7X1bTugoKCrywsDDxvRORrDL96x847t73YuURh+/C8AN3iDCizGBm09y9oHJ9TWcAH7j71dW8dquZbQnk17Zhd18MLA6frzazOcA2wDHAgHCxxwgSTY0JQERy04aNpQy69S2++T64PJTXsinvXzGIls0aRxxZZqs2Abj7y5XrzKwR0NrdV7n7MoKzgoSZWTegNzAV6BQmB4AlBJeIqnrPcGA4QH5+rflGRLLMk1O/ZsRzM8vLZ+3Dfjt2iDCi7FFrI7CZPQmcC5QAHwJtzewOd785mQ2ZWWvg38BF7r4qvqHG3d3MqrwW5e5jgbEQXAJKZpsikrmWrlrPPjeUN/IetsdW3HtyHzXyplAivYB2Cw/YJwOvApcTXMdPOAGYWVOCg/8T7v5sWL3UzDq7++KwnSCpswkRyU7uzhmPfsikectjde9eNpAum6uRN9USSQBNwwP4L4C73b24um/rVbEgXT8EzHH3W+NeegE4DRgTPj6feNgiko2en7GIC5+eEStffdRunN5/uwgjym6JJID7CbppfgxMNrNtgVVJbKM/cAow08zK/rIjCA7848zsTOArYNNGfBKRjLd2w0Z2G/l6hbpZ1xxKa93Jm1bVdgOt8U1mTdx9YxriqZG6gYpkn0vGzeDZ6Yti5VuH9eS4PjXf/SvJ2ZRuoGVvrPo2PBhd56hEJGd9unQ1h9w2OVZu3qQRc68dqkbeepTI+dWauOdldwbPSU84IpLt3J0dRrxCadzFh/EXH8hOndpEF1SOqjUBuPtf48tmdgvwejWLi4hU65+F3/DHuIHbhhV04abje0YYUW7blBaWzQBdoBORhP3080b2uLri98bZow9ls2Zq5I1SIm0AMwkGcQNoDHRE1/9FJEG/f3I6LxctjpXvOrE3R/XcOsKIpEwi6ffIuOcbgaVR9AASkcwy+9tVHH7nO7Fy2xZNKBp1aIQRSWXVJgAza+vuq4DVlV5qa2a4+/fpDU1EMpG7s90Vr1Soe/MPB7F9x9YRRSTVqekM4EmCb//TCC4BxffNcmD7NMYlIhmo8sBtv9k3n+t+sWeEEUlNahoN9MjwUfdhi0iNVq4rpuc1b1Som3vtUFo01XDNDVkijcD9gRnuvsbMfgP0AW5396/THp2INHhnP17I+NlLY+X7ftOHoXt0jjAiSVQijcB/A3qaWU/gD8CDwN+Bg9IZmIg0bEULf+Tou6fEylu2ac4HVw6OMCJJViIJYGM4Xv8xBKOBPhQO4CYiOaiqRt7JfxxI/hYarjnTJJIAVpvZFcBvgAPDWcGapjcsEWmIHntvAVe/8EmsfEb/7Rh51G4RRiR1kUgCOAE4CTjT3ZeYWT5JTAYjIpnvx7Ub6DV6fIW6edcNpXkTNfJmskTGAloC3BpX/hp4PJ1BiUjDcerDHzD50/LZuR46rYBBu1Y5hbdkmJpuBFtN+RAQFV4imMa3bdqiEpHITf/6B467971YOb/9Zkz+08AII5JUq+k+AI3NKpKDSkud7UdUbOSdcvnBbNOuZUQRSbo0SmQhM9vfzE4Pn3cwM90cJpKFHnzniwoH/98N2IEFY47QwT9LJXIj2NVAAbAz8AjQDPgHwVy/IpIFvvvpZ/a6bkKFuk+vO4xmTRL6jigZKpFeQMcCvYHpAO7+rZnp8pBIljjh/v8y9cvysR0fO6MvB3XvGGFEUl8SSQAbwhvBHMDMWqU5JhGpBx8u+J5f3fffWLl7p9a8cbFu8M8liSSAcWZ2P9DOzM4GzgAeSHQDZvYwwaiiy9x9j7BuFHA2UNa3bIS7v1L1GkSkRkXjYOJoWLkQ8rrAoJHQY1i1i5eUBnPyxnv/ikFsldci3ZFKA5PIfQC3mNkQYBVBO8BIdx9fy9viPQrczf/eO3Cbu9+SxHpEpLKicfDiBVC8Liiv/CYoQ5VJ4J5J87n59Xmx8gWDduKSId3rI1JpgGq6D2BHoJO7TwkP+OPD+v3NbAd3/zyRDbj7ZDPrlopgRaSSiaPLD/5litcF9XEJYNnq9fS9fmKFxeZffxhNGquRN5fV9Ne/neBbf2Urw9fq6nwzKzKzh81s8+oWMrPhZlZoZoXLly+vbjGR3LRyYa31x9z9boWD/5Nn7cOCMUfo4C81JoBO7j6zcmVY162O2/0bsAPQC1gM/LW6Bd19rLsXuHtBx47qmSBSQV6Xauvf+3wF3S5/mY8XrgSgZ5c8Fow5gv127FCPAUpDVlMbQLsaXqvTXSHuHps9wsweAF6qy/pEctagkRXbAABv2pILlx/FCw9MjdV9cOUgtmyjRl6pqKYzgMKw108FZnYWwTzBm8zM4qcLOhaYVZf1iWSVonFw2x4wql3wWDSu+mV7DIOj7oS8roCxqvlWXLjmdF4o3R+APx66MwvGHKGDv1SppjOAi4DnzOxkyg/4BQR3Ah+b6AbM7ClgANDBzBYCVwMDzKwXwWBzC4Bzko5cJBsl2aunrH7Jtkez740TYX159ec3HE7jRpbeeCWjmXtVA37GLWA2ENgjLH7i7m+mPapqFBQUeGFhYVSbF0m/2/YIDvqV5XWFi6s+UR56+2TmLlkdK487px99t2ufrgglA5nZNHcvqFyfyH0Ak4BJaYlKRCpKoFdPmcmfLufUhz+Ilft2a8+4c/ulKzLJQjXdB/Ar4GSCyzRPu/sz9RaVSK7K61LNGUB5b5/iklJ2uvLVCi9Pu2owW7Runu7oJMvU1Ah8GXAc8EvgT/UTjkiOGzQSmlbqZNe0ZVAP3PTa3AoH/ysP35UFY45I/OCfTAOzZL2aLgH9g/LhG/5ZD7GISFlDb6WxfRblH0X/y1+usOgXNxxOo2QaeTelgVmyWo2NwOHIn+buP9VfSNVTI7DkogE3T2LBd2tj5WfP248++dXePF+9TWhgluyQdCOwmZm7r6llpea1dSMSkU3y5tylnPFo+ReeA3bqwN/P3GfTV5hEA7PkhpouAU0ys38Dz7v712WVZtYM2B84jaB30KNpjVAkx/y8sYSdr3qtQt1Hfx7C5q2a1W3FCTQwS26pqRF4KFACPGVm35rZbDP7AvgMOBG43d0frYcYRXLGdS/NrnDwH3XUbiwYc0TdD/5QawOz5J5qzwDcfT1wL3CvmTUFOgDr3P3H+gpOJFd8/d1aDry54u02STfy1qaaBmY1AOeuRGYEw92LCUbtFJEU2/eGiSxZVT6Gwwvn96dHl5rGYqyDHsN0wJeYhBKAiKTea7OWcO4/ysdVHLxrJx487X86aoikjRKASD1bX1zCLn+u2Mj78dWHkNeyaUQRSa5KKAGY2bbATu4+wcxaAk3cfXVt7xORikY+P4vH//tVrHzDsXty0j75EUYkuazWBBDOCTAcaE8wi1cX4D5gUHpDE8keX65Yw8Bb3qpYd+PhmGm4ZolOImcAvwf6AlMB3P0zM9syrVGJZJFeo9/gx7XFsfLLF+zP7lvn1f7GonHqsSNplUgC+NndN5R9UzGzJgQjhIpIDV4q+pbzn/woVj5iz87cc3KfxN6scXukHiSSAN42sxFASzMbApwHvJjesEQy17oNJew6smIj78xRh9CmRRKNvBNHV5jnFwjKE0crAUjKJJIALgPOAmYSTN34CvBgOoMSyVSX/auIZwrLh1u4+fge/Kqga/Ir0rg9Ug9qTABm1phgGshdgAfqJySRzDN/2WoG3zo5Vm7cyJh//WGb3sircXukHtSYANy9xMzmmVl+/IBwIhJwd3Yd+Rrri0tjda9fdCA7b9WmbiseNLJiGwBo3B5JuUQuAW0OfGJmHwCx4aHd/ehENmBmDwNHAsvcfY+wrj3wDNANWAAMc/cfkopcJGLPfbSQi5/5OFY+rvc23HpCr9SsXOP2SD2ocUIYADM7qKp6d387oQ2YHQj8BDwelwBuAr539zFmdjmwubtfVtu6NCGMNARrft7I7le/XqHuk2sOpVVz3VgvDVPSE8KUSfRAX8P7J5tZt0rVxwADwuePAW8RNDaLNGgXPzOD5z5aFCvffkIvftF7mwgjEtl0idwJvJryfv/NgKbAGndvW4ftdnL3stFFlwCdatj+cII7kcnP1y3zEo25S1Yx9PZ3YuVWzRoz65pDdSevZLREzgBirVkWfNqPAfZNVQDu7mZW7XUodx8LjIXgElCqtiuSCHdnuyteqVA34ZKD2HHL1hFFJJI6Nc0I9j888B/g0Dpud6mZdQYIH5fVcX0iKTeu8JsKB/9f792VBWOO0MFfskYil4COiys2AgqA9dUsnqgXCOYUHhM+Pl/H9YmkzOr1xew56o0KdXNGD6Vls8YRRSSSHol0Wzgq7vlGgm6bxyS6ATN7iqDBt4OZLQSuJjjwjzOzM4GvAPVtkwbhvCem8crMJbHyPSf14YgenSOMSCR9EkkAD7r7lPgKM+tPgpdt3P3Eal7ScNLSYMxatJIj73o3Vm7fqhnT/zwkwohE0i+RBHAXUHkIw6rqRDJOVY28b106gG4dWkUUkUj9qTYBmFk/YD+go5ldEvdSW0AXQyXj/eP9r7jqP7Ni5dP6bcs1x+wRYUQi9aumM4BmQOtwmfiBTVYBx6czKJF0Wrm2mJ6jKzbyzr12KC2a6nuN5JZqE0B4B/DbZvaou39V3XIimeTMRz9k4tzy5quxp+zFISWT4e6eGnNHck4ibQBrzexmYHegRVmlux+ctqhEUuzjb37kmHvK+zJsndeC964YpJm3JKclkgCeIBi580jgXIJ++8vTGZRIqpSWOtuPqNjI+86fBtK1/WZBQTNvSQ5L5E7gLdz9IaDY3d929zMAffuXBu+RKV9WOPiffcB2LBhzRPnBHzTzluS0RM4AisPHxWZ2BPAt0D59IYnUzQ9rNtD72vEV6uZdN5TmTapo5NXMW5LDEkkA15lZHvAHgv7/bYGL0xqVyCb6zYNTeXf+ilj54d8WcPAu1Q42q5m3JKclMifwTu7+ErASGFgvUYkkadpXP/DLv70XK2/foRVvXjqg9jdq5i3JYYnMCXwicFs9xSOSlJJSZ4dKjbzvXX4wW7drmfhKegzTAV9yUiKXgKaY2d0EPYHi5wSenraoRBIwdvLn3PDK3Fj59wN34I+H7hJhRCKZJZEEUDbL9ei4Okc9gSQiK376mYLrJlSo++z6w2jaOKnpLURyXiIzgum6vzQYv7rvPT5c8EOs/Pcz+3LATh0jjEgkcyUyIUwn4AZga3c/zMx2A/qF9waI1IupX3zHCWPfj5V37dyWVy88IMKIRDJfIpeAHgUeAa4My58StAcoAUjaVdXIO3XEIDq1bVHNO5JQNE69fySnJXLRtIO7jwNKAdx9I1CS1qhEgLvf/KzCwf/iwd1ZMOaI1B38X7wgvAnMy8cAKhpX93WLZIhEzgDWmNkWBA2/mNm+BPcEiKTFslXr6XvDxAp1868/jCapbOTVGEAiCSWASwgmcd/BzKYAHdF8AJImR931LjMXlX+/eOrsfem3wxap35DGABJJqBfQdDM7CNgZMGCeuxfX8jaRpEyZv4KTH5waK/fOb8dz5/VP3wY1BpBIQr2AWgDnAfsTXAZ6x8zuc/f1dd24mS0AVhO0KWx094K6rlMyS3FJKTtd+WqFug+vHEzHNs3Tu2GNASSS0CWgxwkO0neF5ZOAvwO/SlEMA919Re2LSba59Y153Pnm/Fj5sqG78LsBO9TPxjUGkEhCCWAPd98trjzJzGanKyDJfotXrqPfjW9WqPv8hsNp3MjqNxCNASQ5LpEEMN3M9nX39wHMbB+gMEXbd+ANM3Pgfncfm6L1SgM1+Na3mb/sp1j5n+f2Y+9uml5CJAqJJIC9gPfM7OuwnA/MM7OZgLt7jzpsf393X2RmWwLjzWyuu0+OX8DMhgPDAfLz8+uwKYnSpHnLOP2RD2Plfbdvz9PD+0UYkYgkkgCGpmvj7r4ofFxmZs8BfYHJlZYZC4wFKCgo8HTFIumxYWMp3a+q2Mg77arBbNE6zY28IlKrRLqBfmVmmwNd45ev63DQZtYKaOTuq8Pnh1BxxFHJcDe+Oof73/4iVr7qiF0564DtI4xIROIl0g30WuC3wOeEdwOTmuGgOwHPmVlZHE+6+2t1XKc0AAt/WMv+f5lUoe6LGw6nUX038opIjRK5BDQM2MHdN6Ryw+7+BdAzleuU6B1w05t883153/rnztuP3vmbRxiRiFQnkQQwC2gHLEtzLJLBxs9eytmPl3cOO6h7Rx47o2+EEYlIbRJJADcCH5nZLODnskp3PzptUUnG+HljCTtfVfHK3YyRQ2i3WbOIIhKRRCWSAB4D/gLMJBwSWgTgmhc/4ZEpC2Lla4/ZnVP6dYssHhFJTiIJYK2735n2SCR10jzRyVffreGgm9+qUPfljYcTNuiLSIZIJAG8Y2Y3EgwJHX8JqE7dQCVNyiY6KRvkrGyiE0hJEtj7+gksXx37GPDi+fuzZ5e8Oq9XROpfIgmgd/i4b1xdKrqBSjqkaaKTV2cu5ndPlOf8Q3brxNhTNXirSCZL5EawgfURiKRIiic6WV9cwi5/rtjIWzTqENq2aLpJ6xORhqPWOfbMrJOZPWRmr4bl3czszPSHJpukuglNNmGikxHPzaxw8B9z3J4sGHOEDv4iWSKRSVYfBV4Htg7LnwIXpSsgqaNBI4OJTeIlOdHJ58t/otvlL/Pk1K9jdV/eeDi/7qvB+ESySbWXgMysibtvBDq4+zgzuwLA3TeaWUm9RSjJqeNEJ3uOep3V6zfGyq9eeAC7dm6bjkhFJGI1tQF8APQB1pjZFoTjAJnZvsDKGt4nUduEiU6en7GIC5+eESsf1XNr7jqxdw3vEJFMV1MCKOvUfQlBF9AdzGwK0BE4Pt2BSf1Yu2Eju418vULdrGsOpXXzRDqIiUgmq+m/vKOZXRI+fw54hSAp/AwMBorSHJuk2agXPuHR9xbEyn/9VU9+uVfyjcUikplqSgCNgdaUnwmU2Sx94Uh9+PbHdew3pnxO3maNGzHvuqG6k1ckx9SUABa7uyZoySLuzgVPz+DFj7+N1U25/GC2adeyhneJSLZKpA1AssD7X3zHr8e+HyuPPmZ3TtXAbSI5raYEMKjeopC0WV9cQv8xb/LdmmA+n855LZh06QBaNG0ccWQiErVqE4C7f1+fgUjqPfzul4x+aXas/M9z+7F3t/YRRiQiDYn6+mWhynPy/rJPF/46rIrZN9M8bLSINGxKAFnE3Tn3H9N4/ZOlsbqpIwbRqW2L/104zcNGi0jDpwSQJabMX8HJD06NlW84dk9O2qeGsXvSNGy0iGSOSBOAmQ0F7iC45+BBdx8TZTyZaN2GEvreMCE2fk9++82YcMlBNGtSyzh/KR42WkQyT2QJwMwaA/cAQ4CFwIdm9oK7z675nVJm7OTPueGVubHys+ftR5/8zRN7c16X4LJPVfUikhOiPAPoC8x39y8AzOxp4BhACaAWX3+3lgNvLm/k/fXeXRnzyx7JrWTQyIptAJD0sNEiktmiTADbAPFfQRcC+1ReyMyGA8MB8vNzezx6d+fMxwp5c+6yWN0HVw5iyzZVNPLWpo7DRotI5mvwjcDuPhYYC1BQUOARhxOZtz9dzmkPfxAr33R8D4YVdK3bSjdh2GgRyR5RJoBFQPwRrEtYJ3HW/LyRva4bz/riUgB26NiK1y46kKaNE5nMTUSkelEmgA+BncxsO4ID/6+BkyKMp8G5Z9J8bn59Xqz8wvn96dGlXYQRiUg2iSwBhFNLnk8w33Bj4GF3/ySqeBqSL1esYeAtb8XKp+y7Ldf+Yo/oAhKRrBRpG4C7v0Iw0YwApaXOaY98wDufrYjVTbtqMFu0bh5hVCKSrRp8I3CumDhnKWc+Vhgr33ZCT47trT75IpI+SgARW72+mF6jx1NSGnRw2rVzW148vz9N1MgrImmmBBCh28Z/yh0TP4uVX75gf3bfOi/CiEQklygBRGD+sp8YfOvbsfIZ/bdj5FG7RRiRiOQiJYB6VFrqnPjA+0z9snyunY/+PITNWzWLMCoRyVVKAPXk9U+WcM7fp8XKd53Ym6N6bh1hRCKS65QA0mzlumJ6XvNGrNyzSx7Pntefxo0swqhERJQA0urm1+dyz6TPY+XXLjqAXbZqG2FEIiLllADS4NOlqznktsmx8jkHbc8Vh+0aYUQiIv9LCSCFSkqdX933HtO//jFW9/HIQ8jbrGmEUYmIVE0JIEVembmY856YHivf95s+DN2jc4QRiYjUTAmgjn5cu4Feo8fHygXbbs4z5/RTI6+INHhKAHVwwytzGDv5i1h5/MUHslOnNhFGJCKSOCWATTD721Ucfuc7sfL5A3fk0kN3jjAiEZHkZX8CKBqXsnlvN5aUcuy97zFz0cry1Y86hLYt1MgrIpknuxNA0Th48QIoXheUV34TlCHpJPD8jEVc+PSMWPmBUwsYslunVEUqIlLvsjsBTBxdfvAvU7wuqE8wAXy/ZgN9ri1v5O23/RY8cdY+NFIjr4hkuOxOACsXJldfyagXPuHR9xbEyhP/cBA7dGydgsBERKKX3Qkgr0tw2aeq+hrMWrSSI+96N1a+eHB3Lhy8U6qjE6L+mDkAAArbSURBVBGJVHYngEEjK7YBADRtGdRXYWNJKUfe9S5zl6wGoFnjRkwfOYTWzbP71yQiuSmSI5uZjQLOBpaHVSPCCeJTq+w6fwK9gJ6dvpBLxn0cKz/y270ZuMuWKQ9JRKShiPKr7W3ufkvat9JjWI0Nvit++pmC6ybEygd178ijp++NmRp5RSS75fS1jSufm8kTU7+Old+6dADdOrSKMCIRkfoTZQI438xOBQqBP7j7D1UtZGbDgeEA+fn5Kdnwx9/8yDH3TImV/3jozvx+4I4pWbeISKYwd0/Pis0mAFtV8dKVwPvACsCBa4HO7n5GbessKCjwwsLCTY5pw8ZSht4+mS9WrAGgVbPGfHDlYFqpkVdEspiZTXP3gsr1aTvyufvgRJYzsweAl9IVR5lxH37Dn/5dFCs/fkZfDuzeMd2bFRFpsKLqBdTZ3ReHxWOBWenc3rjC8oP/4F078cCpe6mRV0RyXlTXPm4ys14El4AWAOekc2PdO7WhV9d23HVib7q23yydmxIRyRhpawNIh7q2AYiI5KLq2gAaRRGMiIhETwlARCRHKQGIiOQoJQARkRylBCAikqOUAEREcpQSgIhIjlICEBHJURl1I5iZLQe+quKlDgSDy2WDbNmXbNkP0L40RNmyH1A/+7Ktu//P4GcZlQCqY2aFVd3llomyZV+yZT9A+9IQZct+QLT7oktAIiI5SglARCRHZUsCGBt1ACmULfuSLfsB2peGKFv2AyLcl6xoAxARkeRlyxmAiIgkSQlARCRHZVQCMLOuZjbJzGab2SdmdmFY397MxpvZZ+Hj5lHHWhsza2FmH5jZx+G+XBPWb2dmU81svpk9Y2bNoo41UWbW2Mw+MrOXwnLG7YuZLTCzmWY2w8wKw7qM+3wBmFk7M/uXmc01szlm1i8T98XMdg7/HmU/q8zsogzdl4vD//dZZvZUeByI7P8koxIAsBH4g7vvBuwL/N7MdgMuBya6+07AxLDc0P0MHOzuPYFewFAz2xf4C3Cbu+8I/ACcGWGMyboQmBNXztR9GejuveL6Zmfi5wvgDuA1d98F6Enwt8m4fXH3eeHfoxewF7AWeI4M2xcz2wa4AChw9z2AxsCvifL/xN0z9gd4HhgCzAM6h3WdgXlRx5bkfmwGTAf2IbgjsElY3w94Per4EtyHLgT/hAcDLwGWiftCMEd1h0p1Gff5AvKALwk7emTyvlSK/xBgSibuC7AN8A3QnmA+9peAQ6P8P8m0M4AYM+sG9AamAp3cfXH40hKgU0RhJSW8ZDIDWAaMBz4HfnT3jeEiCwk+NJngduBPQGlY3oLM3BcH3jCzaWY2PKzLxM/XdsBy4JHwstyDZtaKzNyXeL8GngqfZ9S+uPsi4Bbga2AxsBKYRoT/JxmZAMysNfBv4CJ3XxX/mgdpNCP6trp7iQentV2AvsAuEYe0SczsSGCZu0+LOpYU2N/d+wCHEVxiPDD+xQz6fDUB+gB/c/fewBoqXSLJoH0BILw2fjTwz8qvZcK+hG0UxxAk562BVsDQKGPKuARgZk0JDv5PuPuzYfVSM+scvt6Z4Bt1xnD3H4FJBKd/7cysSfhSF2BRZIElrj9wtJktAJ4muAx0Bxm4L+G3NNx9GcF15r5k5udrIbDQ3aeG5X8RJIRM3JcyhwHT3X1pWM60fRkMfOnuy929GHiW4H8nsv+TjEoAZmbAQ8Acd7817qUXgNPC56cRtA00aGbW0czahc9bErRlzCFIBMeHi2XEvrj7Fe7exd27EZyiv+nuJ5Nh+2JmrcysTdlzguvNs8jAz5e7LwG+MbOdw6pBwGwycF/inEj55R/IvH35GtjXzDYLj2Vlf5PI/k8y6k5gM9sfeAeYSfm15hEE7QDjgHyC4aKHufv3kQSZIDPrATxG0BOgETDO3Ueb2fYE36LbAx8Bv3H3n6OLNDlmNgC41N2PzLR9CeN9Liw2AZ509+vNbAsy7PMFYGa9gAeBZsAXwOmEnzUyb19aERxAt3f3lWFdxv1dwu7eJxD0aPwIOIvgmn8k/ycZlQBERCR1MuoSkIiIpI4SgIhIjlICEBHJUUoAIiI5SglARCRHKQFInZjZFnGjNC4xs0Vx5QY1+qeZDTCz/dK4/pZm9raZNQ7LF5vZdDM7IW6ZkkojW3arZl3dzGxWmuL8rZndneR7HgwHXsTMRsTVNzOzyXE3MkkGUQKQOnH377x8pMb7CEY17BX+bKjveGo5EA0AkkoASR7YzgCedfeScLiSvQnuJD4pbpl1cb+fXu6+IJl46hjfJnP3s9x9dlgcEVe/gWAQwBOqfKM0aEoAknJmtlf4TXiamb0ed7v+W2Z2m5kVhuPT721mz4bjuV8XLtPNgvHrnwiX+ZeZbZbAem+3YPz+C83sqHB89Y/MbIKZdQq/aZ8LXBx+8z7AzB41s+Pj4v4pfBxgZu+Y2QvA7HDQvpvN7EMzKzKzc6rZ9ZMpv4vTwscab7Qxs9ZmNjE8U5hpZsfEvdzYzB6wYPz4N8I7xqva35p+L3+xYN6JT83sgLh1b21mr4W/+5vi4jnEzP4bxvPPMJGVravAzMYALcPf4RPh2/4T7rtkmqiHSNVP9vwAo4A/Au8BHcO6E4CHw+dvAX8Jn18IfEswjG9zgrFrtgC6ERw0+4fLPQxcCjStZb33xsWxOeU3OZ4F/DUuvkvjlnsUOD6u/FP4OIBg8LTtwvJw4KrweXOgsOy1uPc2A5ZUqruC4M7Ok+LqSoAZ4c9zBHcctw1f6wDMJ0ge3QjuFu0VvjaO4A7RCvubwO+lbN8PByaEz39LcGdwHtCC4C7aruH2JwOtwuUuA0bGrasg/vcUt0+NgeVRf/70k/yPrttJqjUH9gDGmxkEB4fFca+/ED7OBD7xcDhfM/uC4CD0I/CNu08Jl/sHwSQar9Wy3mfinncBngm/CTcjGBc/WR+4e9n7DgF6xJ0t5AE7VVpvhzD2GHe/Ebix0nrXeXC5DIgNbniDBaOOlhIMC1A2rPGX7j4jfD6NICmUKdvfnan591I2YGLl90/08iEVZgPbAu2A3YAp4bqaAf+lFh5c8tpgZm3cfXVty0vDoQQgqWYEB/Z+1bxeNsZJadzzsnLZ57HyZRNPYL1r4p7fBdzq7i9YMDbRqGres5HwMqiZNSI44FW1PgP+z91fr2Y9AOsIvk0n62SgI7CXuxdbMKJq2Xrifz8lQMsq4kv0911Cxf/3yutuEq5rvLufmOxOECT+9ZvwPomQ2gAk1X4GOppZPwi+4ZrZ7kmuI7/s/QQNqO8SzP6U6HrzKB9S97S4+tVAm7jyAoIpBiEYZ75pNet7Hfhd+G0dM+tuweBkMe7+A8E1+2STQB7BXArFZjaQ4Jt4MpL5vdTmfaC/me0YrquVmXWvYrnist9FuNwWwAoPhjiWDKIEIKlWSjC07V/M7GOCa93Jdr2cRzAZyxyC6/l/86C3SaLrHQX808ymEUy3V+ZF4NiyRmDgAeCgcH39qPitP96DBMP2Trega+b9VH32/Aawf+K7CcATQIGZzQROBeYm8+Ykfy+1rWs5QfvAU2ZWRHD5p6pJisYCRXGNwAOBlzdlmxItjQYqDUrYW+clDybNzihm1ge42N1PiTqW+mRmzwKXu/unUcciydEZgEiKuPt0YJKFN4LlAgtu9vuPDv6ZSWcAIiI5SmcAIiI5SglARCRHKQGIiOQoJQARkRylBCAikqP+H/9tNRqFazb8AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4MLBwRyVR9z",
        "colab_type": "text"
      },
      "source": [
        "We are using a Python trick called argument unpacking here: `*params` means to pass the elements of `params` as individual arguments. In Python, this is usually done with lists or tuples, but we can also use argument unpacking with PyTorch tensors, which are split along the leading dimension. So here, `model(t_un, *params)` is equivalent to `model(t_un, params[0], params[1])`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNOJ-G7jVtPO",
        "colab_type": "text"
      },
      "source": [
        "## PyTorch’s autograd: Backpropagating all things\n",
        "\n",
        "\n",
        "In our little adventure, we just saw a simple example of backpropagation: we computed the gradient of a composition of functions—the model and the loss—with respect to their innermost parameters (w and b) by propagating derivatives backward using the chain rule. The basic requirement here is that all functions we’re dealing with can be differentiated analytically. If this is the case, we can compute the gradient—what we earlier called “the rate of change of the loss”—with respect to the parameters in one sweep.\n",
        "Even if we have a complicated model with millions of parameters, as long as our model is differentiable, computing the gradient of the loss with respect to the parameters amounts to writing the analytical expression for the derivatives and evaluating\n",
        "them once. Granted, writing the analytical expression for the derivatives of a very deep 9\n",
        "composition of linear and nonlinear functions is not a lot of fun. It isn’t particularly quick, either."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T73dLxhDWeB9",
        "colab_type": "text"
      },
      "source": [
        "### Computing the gradient automatically\n",
        "\n",
        "\n",
        "This is when PyTorch tensors come to the rescue, with a PyTorch component called **autograd**. Chapter 3 presented a comprehensive overview of what tensors are and what functions we can call on them. We left out one very interesting aspect, however: PyTorch tensors can remember where they come from, in terms of the operations and parent tensors that originated them, and they can automatically provide the chain of derivatives of such operations with respect to their inputs. This means we won’t need to derive our model by hand; given a forward expression, no matter how nested, PyTorch will automatically provide the gradient of that expression with respect to its input parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppXBUjwqWmog",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Applying Autograd\n",
        "\n",
        "At this point, the best way to proceed is to rewrite our thermometer calibration code, this time using autograd, and see what happens. First, we recall our model and loss function [(code/p1ch5/2_autograd.ipynb)](https://github.com/deep-learning-with-pytorch/dlwpt-code/blob/master/p1ch5/2_autograd.ipynb):\n",
        "\n",
        "```\n",
        "def model(t_u, w, b):\n",
        "  return w * t_u + b\n",
        "\n",
        "def loss_fn(t_p, t_c):\n",
        "  squared_diffs = (t_p - t_c)**2\n",
        "  return squared_diffs.mean()\n",
        "```\n",
        "\n",
        "Let’s initialise a parameters tensor:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtMERIK2U5vO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = torch.tensor([1.0, 0.0], requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fiMUYj_SgnR",
        "colab_type": "text"
      },
      "source": [
        "### Using the `requires_grad` attribute\n",
        "\n",
        "Notice the `requires_grad=True` argument to the tensor constructor? That argument is telling PyTorch to track the entire family tree of tensors resulting from operations on params. In other words, any tensor that will have params as an ancestor will have access to the chain of functions that were called to get from params to that tensor. In case these functions are differentiable (and most PyTorch tensor operations will be), the value of the derivative will be automatically populated as a grad attribute of the params tensor.\n",
        "\n",
        "\n",
        "In general, all PyTorch tensors have an attribute named grad. Normally, it’s None:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrIGGJtcSdLs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e6c2c43a-8847-48a8-d586-a5d3fc24cae3"
      },
      "source": [
        "params.grad is None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbNub_tIS4sr",
        "colab_type": "text"
      },
      "source": [
        "All we have to do to populate it is to start with a tensor with `requires_grad` set to `True`, then call the model and compute the loss, and then call backward on the loss tensor:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQirjYblS235",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42b8595a-c748-4d7d-dc6d-abc12385c5e1"
      },
      "source": [
        "loss = loss_fn(model(t_u, *params), t_c)\n",
        "loss.backward()\n",
        "\n",
        "params.grad"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4517.2969,   82.6000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U90Y1hSsUXTO",
        "colab_type": "text"
      },
      "source": [
        "At this point, the `grad` attribute of `params` contains the derivatives of the loss with respect to each element of `params`.\n",
        "\n",
        "\n",
        "When we compute our loss while the parameters `w` and `b` require gradients, in addition to performing the actual computation, PyTorch creates the autograd graph with the operations. When we call `loss.backward()`, PyTorch traverses this graph in the reverse direction to compute the gradients.\n",
        "\n",
        "Only those tensors with `requires_grad` set to `True` will get their `grad` values populated.\n",
        "\n",
        "### Accumulating Grad Functions\n",
        "\n",
        "We could have any number of tensors with requires_grad set to True and any composition of functions. In this case, PyTorch would compute the derivatives of the loss throughout the chain of functions (the computation graph) and accumulate their values in the grad attribute of those tensors (the leaf nodes of the graph).\n",
        "\n",
        "Alert! **Big gotcha ahead**. This is something PyTorch newcomers—and a lot of more experienced folks, too—trip up on regularly. We just wrote *accumulate*, not *store*. Calling `backward()` will lead derivatives to *accumulate* at leaf nodes. We need to zero the gradient explicitly after using it for parameter updates.\n",
        "\n",
        "\n",
        "So if `backward()` was called earlier, the loss is evaluated again, `backward()` is called again (as in any training loop), and the gradient at each leaf is accumulated (that is, summed) on top of the one computed at the previous iteration, which leads to an incorrect value for the gradient. In order to prevent this from occurring, we need to `zero` the gradient explicitly at each iteration. We can do this easily using the in-place `zero_` method:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMR9fBp-TRV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if params.grad is not None:\n",
        "  params.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oqMlEgXXJAC",
        "colab_type": "text"
      },
      "source": [
        "There is a reason why zeroing the gradient is a required step instead of zeroing happening automatically whenever we call backward. Doing it this way provides more flexibility and control when working with gradients in complicated models.\n",
        "\n",
        "Now, let’s see what our autograd-enabled training code looks like, start to finish:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCCemdlJXEt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    if params.grad is not None: #This could be done at any point in the loop prior to calling loss.backward().\n",
        "      params.grad.zero_()\n",
        "\n",
        "    t_p = model(t_u, *params)\n",
        "    loss = loss_fn(t_p, t_c)\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      params -= learning_rate * params.grad\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "      print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "\n",
        "  return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgyXDoVHZuHI",
        "colab_type": "text"
      },
      "source": [
        "Note that our code updating params is not quite as straightforward as we might have expected. There are two particularities. First, we are encapsulating the update in a `no_grad` context using the Python `with` statement. This means within the `with` block, the PyTorch autograd mechanism should look away: that is, not add edges to the forward graph. In fact, when we are executing this bit of code, the forward graph that PyTorch records is consumed when we call backward, leaving us with the params leaf node. But now we want to change this leaf node before we start building a fresh forward graph on top of it.\n",
        "\n",
        "Second, we update `params` in place. This means we keep the same `params` tensor around but subtract our update from it. When using autograd, we usually avoid in- place updates because PyTorch’s autograd engine might need the values we would be modifying for the backward pass. Here, however, we are operating without autograd, and it is beneficial to keep the `params` tensor. Not replacing the parameters by assigning new tensors to their variable name will become crucial when we register our parameters with the optimizer.\n",
        "\n",
        "Let’s invoke our loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANi1IjhTZqtt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4d467b7d-3566-4f92-b319-73dd4894cbb4"
      },
      "source": [
        "training_loop(\n",
        "  n_epochs = 5000,\n",
        "  learning_rate = 1e-2,\n",
        "  params = torch.tensor([1.0, 0.0],requires_grad=True), #notice requires_grad=True\n",
        "  t_u = t_un, #normalised units\n",
        "  t_c = t_c\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 500, Loss 7.860116\n",
            "Epoch 1000, Loss 3.828538\n",
            "Epoch 1500, Loss 3.092191\n",
            "Epoch 2000, Loss 2.957697\n",
            "Epoch 2500, Loss 2.933134\n",
            "Epoch 3000, Loss 2.928648\n",
            "Epoch 3500, Loss 2.927830\n",
            "Epoch 4000, Loss 2.927679\n",
            "Epoch 4500, Loss 2.927652\n",
            "Epoch 5000, Loss 2.927647\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  5.3671, -17.3012], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8eloW2hdf1n",
        "colab_type": "text"
      },
      "source": [
        "The result is the same as we got previously. Good for us! It means that while we are capable of computing derivatives by hand, we no longer need to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX_2ZA0edkYM",
        "colab_type": "text"
      },
      "source": [
        "## Optimizers\n",
        "\n",
        "In the example code, we used *vanilla* gradient descent for optimization, which worked fine for our simple case. Needless to say, there are several optimization strategies and tricks that can assist convergence, especially when models get complicated.\n",
        "We’ll dive deeper into this topic in later chapters, but now is the right time to introduce the way PyTorch abstracts the optimization strategy away from user code: that is, the training loop we’ve examined. This saves us from the boilerplate busywork of having to update each and every parameter to our model ourselves. The torch module has an `optim` submodule where we can find classes implementing different optimization algorithms. Here’s an abridged list\n",
        "[(code/p1ch5/3_optimizers.ipynb)](https://github.com/deep-learning-with-pytorch/dlwpt-code/blob/master/p1ch5/3_optimizers.ipynb) :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdYsJJ_AdT3z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "34c40840-8a08-4028-e590-24364d4eba36"
      },
      "source": [
        "import torch.optim as optim\n",
        "dir(optim)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ASGD',\n",
              " 'Adadelta',\n",
              " 'Adagrad',\n",
              " 'Adam',\n",
              " 'AdamW',\n",
              " 'Adamax',\n",
              " 'LBFGS',\n",
              " 'Optimizer',\n",
              " 'RMSprop',\n",
              " 'Rprop',\n",
              " 'SGD',\n",
              " 'SparseAdam',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " 'lr_scheduler']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sbN_5eFfve1",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Every optimizer constructor takes a list of parameters (aka PyTorch tensors, typically with `requires_grad` set to `True`) as the first input. All parameters passed to the optimizer are retained inside the optimizer object so the optimizer can update their values after each `backward()` call and access their `grad` attribute.\n",
        "\n",
        "\n",
        "Each optimizer exposes two methods: `zero_grad` and `step`. `zero_grad` zeroes the `grad` attribute of all the parameters passed to the optimizer upon construction. `step` updates the value of those parameters according to the optimization strategy implemented by the specific optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbCX1M8lhybI",
        "colab_type": "text"
      },
      "source": [
        "### Using a Gradient Descent Optimizer\n",
        "\n",
        "Let’s create `params` and instantiate a gradient descent optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_oMxPVsfVqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 1e-5\n",
        "optimizer = optim.SGD([params], lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmMtF11xvtG8",
        "colab_type": "text"
      },
      "source": [
        "Here **SGD** stands for *stochastic gradient descent*. Actually, the optimizer itself is exactly a vanilla gradient descent (as long as the momentum argument is set to `0.0`, which is the default). The term stochastic comes from the fact that the gradient is typically obtained by averaging over a random subset of all input samples, called a *minibatch*. However, the optimizer does not know if the loss was evaluated on all the samples (*vanilla*) or a random subset of them (*stochastic*), so the algorithm is literally the same in the two cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRq9tsgDu6Bx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "09fa4da9-d966-4910-801c-206966383c3b"
      },
      "source": [
        "t_p = model(t_u, *params)\n",
        "loss = loss_fn(t_p, t_c)\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "params"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 9.5483e-01, -8.2600e-04], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPHNJCjkw1Pw",
        "colab_type": "text"
      },
      "source": [
        "The value of params is updated upon calling `step()` without us having to touch it ourselves! What happens is that the optimizer looks into `params.grad` and updates `params`, subtracting `learning_rate * grad` from it, exactly as in our former handwritten code.\n",
        "\n",
        "\n",
        "Ready to stick this code in a training loop? Nope! The big gotcha almost got us— we forgot to zero out the gradients. Had we called the previous code in a loop, gradients would have accumulated in the leaves at every call to backward, and our gradient descent would have been all over the place! Here’s the loop-ready code, with the extra `zero_grad` at the correct spot (right before the call to backward):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVDu0NcOwyP5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e9ca8564-5279-4c63-c6db-5fa506d1ad8b"
      },
      "source": [
        "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 1e-2\n",
        "optimizer = optim.SGD([params], lr=learning_rate)\n",
        "\n",
        "t_p = model(t_un, *params)\n",
        "loss = loss_fn(t_p, t_c)\n",
        "optimizer.zero_grad() #zero_grad\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "params"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.7761, 0.1064], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBIe5zOKxmPM",
        "colab_type": "text"
      },
      "source": [
        "Perfect! See how the `optim` module helps us abstract away the specific optimization scheme? All we have to do is provide a list of `params` to it (that list can be extremely long, as is needed for very deep neural network models).\n",
        "\n",
        "Let’s update our training loop accordingly:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohaCZBuuxdEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    t_p = model(t_u, *params)\n",
        "    loss = loss_fn(t_p, t_c)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "      print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "\n",
        "  return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djdeLYLtyE1-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e756d4f9-4912-498c-a4ec-fa53ed499314"
      },
      "source": [
        "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 1e-2\n",
        "optimizer = optim.SGD([params], lr=learning_rate)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    t_u = t_un,\n",
        "    t_c = t_c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 500, Loss 7.860118\n",
            "Epoch 1000, Loss 3.828538\n",
            "Epoch 1500, Loss 3.092191\n",
            "Epoch 2000, Loss 2.957697\n",
            "Epoch 2500, Loss 2.933134\n",
            "Epoch 3000, Loss 2.928648\n",
            "Epoch 3500, Loss 2.927830\n",
            "Epoch 4000, Loss 2.927680\n",
            "Epoch 4500, Loss 2.927651\n",
            "Epoch 5000, Loss 2.927648\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  5.3671, -17.3012], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70ua50ZUyUN8",
        "colab_type": "text"
      },
      "source": [
        "### Testing other Optimizers\n",
        "\n",
        "In order to test more optimizers, all we have to do is instantiate a different optimizer, say Adam, instead of SGD. The rest of the code stays as it is.\n",
        "\n",
        "We won’t go into much detail about Adam; suffice to say that it is a more sophisticated optimizer in which the learning rate is set adaptively. In addition, it is a lot less sensitive to the scaling of the parameters—so insensitive that we can go back to using the original (non-normalized) input `t_u`, and even increase the learning rate to `1e-1`, and Adam won’t even blink:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVNnXfaEyOB0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "8d9951f4-d59e-4ccd-e289-04a327cbad19"
      },
      "source": [
        "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 1e-1\n",
        "optimizer = optim.Adam([params], lr = learning_rate)\n",
        "\n",
        "training_loop(n_epochs=2000,\n",
        "              optimizer=optimizer,\n",
        "              params=params,\n",
        "              t_u = t_u,\n",
        "              t_c = t_c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 500, Loss 7.612903\n",
            "Epoch 1000, Loss 3.086700\n",
            "Epoch 1500, Loss 2.928578\n",
            "Epoch 2000, Loss 2.927646\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  0.5367, -17.3021], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXCwi6nYzrWK",
        "colab_type": "text"
      },
      "source": [
        "Notice how the first element of params is `0.5367` instead of the usual `5.3671`. This is because we used `t_u` instead of the normalized `t_un`.\n",
        "\n",
        "The optimizer is not the only flexible part of our training loop. Let’s turn our attention to the model. In order to train a neural network on the same data and the same loss, all we would need to change is the model function. It wouldn’t make particular sense in this case, since we know that converting Celsius to Fahrenheit amounts to a linear transformation.\n",
        "\n",
        " We’ll see quite soon that neural networks allow us to remove our arbitrary assumptions about the shape of the function we should be approximating. Even so, we’ll see how neural networks manage to be trained even when the underlying processes are highly nonlinear (such in the case of describing an image with a sentence, as we saw in chapter 2).\n",
        "\n",
        "\n",
        "We have touched on a lot of the essential concepts that will enable us to train complicated deep learning models while knowing what’s going on under the hood: backpropagation to estimate gradients, autograd, and optimizing weights of models using gradient descent or other optimizers.\n",
        "\n",
        "\n",
        "Next, we’re going to offer an aside on how to split our samples, because that sets up a perfect use case for learning how to better control autograd.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYt_Ln0Y1d4v",
        "colab_type": "text"
      },
      "source": [
        "## Training, validation, and overfitting\n",
        "\n",
        "We need to keep a part of the data on the side so that we can validate our models on independent observations. This is a vital thing to do, especially when the model we adopt could potentially approximate functions of any shape, as in the case of neural networks. In other words, a highly adaptable model will tend to use its many parameters to make sure the loss is minimal *at* the data points, but we’ll have no guarantee that the model behaves well *away from or in between* the data points.\n",
        "\n",
        "After all, that’s what we’re asking the optimizer to do: minimize the loss *at* the data points. Sure enough, if we had independent data points that we didn’t use to evaluate our loss or descend along its negative gradient, we would soon find out that evaluating the loss at those independent data points would yield higher-than-expected loss. We have already mentioned this phenomenon, called **overfitting**.\n",
        "\n",
        "The first action we can take to combat overfitting is recognizing that it might hap- pen. In order to do so, as Kepler figured out in 1600, we must take a few data points out of our dataset (the *validation* set) and only fit our model on the remaining data points (the *training* set). Then, while we’re fitting the model, we can evaluate the loss once on the training set and once on the validation set. When we’re trying to decide if we’ve done a good job of fitting our model to the data, we must look at both!\n",
        "\n",
        "A deep neural network can potentially approximate complicated functions, provided that the number of neurons, and therefore parameters, is high enough. The fewer the number of parameters, the simpler the shape of the function our network will be able to approximate.\n",
        "\n",
        "\n",
        "So, **rule 1**: if the training loss is not decreasing, chances are the model is too simple for the data. The other possibility is that our data just doesn’t contain meaningful information that lets it explain the output: if the nice folks at the shop sell us a barometer instead of a thermometer, we will have little chance of predicting temperature in Celsius from just pressure.\n",
        "\n",
        "**rule 2**: if the training loss and the validation loss diverge, we’re overfitting. If the loss evaluated in the validation set doesn’t decrease along with the training set, it means our model is improving its fit of the samples it is seeing during training, but it is not generalizing to samples outside this precise set.If we evaluate the model at new, previously unseen points, the values of the loss function can be poor.\n",
        "\n",
        "We’ve got some nice trade-offs here. On the one hand, we need the model to have enough capacity for it to fit the training set. On the other, we need the model to avoid overfitting. Therefore, in order to choose the right size for a neural network model in terms of parameters, the process is based on two steps: increase the size until it fits, and then scale it down until it stops overfitting.\n",
        "\n",
        "Let’s get back to our example and see how we can split the data into a training set and a validation set. We’ll do it by shuffling `t_u` and `t_c` the same way and then splitting the resulting shuffled tensors into two parts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCfThecnbCZy",
        "colab_type": "text"
      },
      "source": [
        "### Splitting a Dataset\n",
        "\n",
        "Shuffling the elements of a tensor amounts to finding a permutation of its indices. The `randperm` function does exactly this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMQyIC-Izbr6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e44c9e06-a8e6-4e28-be6b-b6e10b17eb2f"
      },
      "source": [
        "n_samples = t_u.shape[0] #size of whole sample set\n",
        "n_val = int(0.2 * n_samples) #size of validation set\n",
        "shuffled_indices = torch.randperm(n_samples) #shuffle\n",
        "train_indices = shuffled_indices[:-n_val] #training set\n",
        "val_indices = shuffled_indices[-n_val:] #validation set\n",
        "train_indices, val_indices\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 3,  0,  2,  9,  8, 10,  4,  1,  6]), tensor([5, 7]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwuhFtI5ctCP",
        "colab_type": "text"
      },
      "source": [
        "We just got index tensors that we can use to build training and validation sets starting\n",
        "from the data tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StsMcXcEbzz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_t_u = t_u[train_indices]\n",
        "train_t_c = t_c[train_indices]\n",
        "\n",
        "val_t_u = t_u[val_indices]\n",
        "val_t_c = t_c[val_indices]\n",
        "\n",
        "train_t_un = 0.1 * train_t_u #normalised\n",
        "val_t_un = 0.1 * val_t_u #normalised"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sIvJifndRFI",
        "colab_type": "text"
      },
      "source": [
        "Our training loop doesn’t really change. We just want to additionally evaluate the validation loss at every epoch, to have a chance to recognize whether we’re overfitting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVC-t3DtdPtf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_loop(n_epochs, optimizer, params, train_t_u,\n",
        "                  val_t_u, train_t_c, val_t_c):\n",
        "  \n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        train_t_p = model(train_t_u, *params)\n",
        "        train_loss = loss_fn(train_t_p, train_t_c)\n",
        "                             \n",
        "        val_t_p = model(val_t_u, *params)\n",
        "        val_loss = loss_fn(val_t_p, val_t_c)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch <= 3 or epoch % 500 == 0:\n",
        "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
        "                  f\" Validation loss {val_loss.item():.4f}\")\n",
        "            \n",
        "    return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrfa1w76dzCz",
        "colab_type": "text"
      },
      "source": [
        "Note that there is no `val_loss.backward()` here because we don't want to train our model on our validation data.\n",
        "\n",
        "Since we’re using Stochastic Gradient Descent again, we’re back to using normalized inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm3nNYH1eHk_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "e510d54b-5b72-4c65-c425-9a465c15eedb"
      },
      "source": [
        "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 1e-2\n",
        "optimizer = optim.SGD([params], lr=learning_rate)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 3000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    train_t_u = train_t_un,\n",
        "    val_t_u = val_t_un,\n",
        "    train_t_c = train_t_c,\n",
        "    val_t_c = val_t_c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Training loss 92.9048, Validation loss 23.9323\n",
            "Epoch 2, Training loss 32.4664, Validation loss 36.4383\n",
            "Epoch 3, Training loss 25.9949, Validation loss 46.3535\n",
            "Epoch 500, Training loss 9.8406, Validation loss 11.4031\n",
            "Epoch 1000, Training loss 5.1131, Validation loss 2.0225\n",
            "Epoch 1500, Training loss 3.6758, Validation loss 0.9233\n",
            "Epoch 2000, Training loss 3.2388, Validation loss 1.5555\n",
            "Epoch 2500, Training loss 3.1059, Validation loss 2.2806\n",
            "Epoch 3000, Training loss 3.0655, Validation loss 2.7949\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  5.5415, -18.3099], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj1NtZagepgM",
        "colab_type": "text"
      },
      "source": [
        "Here we are not being entirely fair to our model. The validation set is really small, so the validation loss will only be meaningful up to a point. In any case, we note that the validation loss is **lesser** (in the textbook, we get a higher validation loss) than our training loss, although not by an order of magnitude.\n",
        "\n",
        "We expect a model to perform better on the training set, since the model parameters are being shaped by the training set. Our main goal is to also see both the training loss and the validation loss decreasing. While ideally both losses would be roughly the same value, as long as the validation loss stays reasonably close to the training loss, we know that our model is continuing to learn generalized things about our data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNxe7acEiMfO",
        "colab_type": "text"
      },
      "source": [
        "### Autograd nits and switching it off\n",
        "\n",
        "From the previous training loop, we can appreciate that we only ever call backward on `train_loss`. Therefore, errors will only ever backpropagate based on the training set—the validation set is used to provide an independent evaluation of the accuracy of the model’s output on data that wasn’t used for training.\n",
        "\n",
        "\n",
        "The curious reader will have a question at this point. The model is evaluated twice—once on train_t_u and once on val_t_u—and then backward is called. Won’t this confuse autograd? Won’t backward be influenced by the values generated during the pass on the validation set?\n",
        "\n",
        "\n",
        "Luckily for us, this isn’t the case. The first line in the training loop evaluates model on `train_t_u` to produce `train_t_p`. Then `train_loss` is evaluated from `train_t_p`. This creates a computation graph that links `train_t_u` to `train_t_p` to `train_loss`.\n",
        "\n",
        "When model is evaluated again on `val_t_u`, it produces `val_t_p` and `val_loss`. In this case, a separate computation graph will be created that links `val_t_u` to `val_t_p` to `val_loss`. Separate tensors have been run through the same functions, `model` and `loss_fn`, generating separate computation graphs.\n",
        "\n",
        "\n",
        "The only tensors these two graphs have in common are the parameters. When we call backward on `train_loss`, we run backward on the first graph. In other words, we accumulate the derivatives of `train_loss` with respect to the parameters based on the computation generated from `train_t_u`.\n",
        "\n",
        "If we (incorrectly) called backward on `val_loss` as well, we would accumulate the derivatives of `val_loss` with respect to the parameters on the same leaf nodes. Remember the `zero_grad` thing, whereby gradients are accumulated on top of each other every time we call backward unless we zero out the gradients explicitly? Well, here something very similar would happen: calling backward on `val_loss` would lead to gradients accumulating in the `params` tensor, on top of those generated during the `train_loss.backward()` call. In this case, we would effectively train our model on the whole dataset (both training and validation), since the gradient would depend on both - which is effectively the same as what we did earlier.\n",
        "\n",
        "Since we’re not ever calling `backward()` on `val_loss`, why are we building the graph in the first place? We could in fact just call `model` and `loss_fn` as plain functions, without tracking the computation. However optimized, building the autograd graph comes with additional costs that we could totally forgo during the validation pass, especially when the model has millions of parameters.\n",
        "\n",
        "In order to address this, PyTorch allows us to switch off autograd when we don’t need it, using the `torch.no_grad` context manager. We won't see any meaningful advantage in terms of speed or memory consumption on our small problem. However, for larger models, the differences can add up. We can make sure this works by checking the value of the `requires_grad` attribute on the `val_loss` tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWRHbjZteYzl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_loop(n_epochs, optimizer, params, train_t_u,\n",
        "                  val_t_u, train_t_c, val_t_c):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        train_t_p = model(train_t_u, *params)\n",
        "        train_loss = loss_fn(train_t_p, train_t_c)\n",
        "\n",
        "        with torch.no_grad(): #Context Manager here\n",
        "            val_t_p = model(val_t_u, *params)\n",
        "            val_loss = loss_fn(val_t_p, val_t_c)\n",
        "            assert val_loss.requires_grad == False #Checks that our output requires_grad args are forced to False inside this block\n",
        "            \n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5ihm1J2lEnX",
        "colab_type": "text"
      },
      "source": [
        "Using the related `set_grad_enabled` context, we can also condition the code to run with `autograd` enabled or disabled, according to a Boolean expression —typically indicating whether we are running in *training* or *inference* mode. We could, for instance, define a `calc_forward` function that takes data as input and runs `model` and `loss_fn` with or without autograd according to a Boolean `train_is` argument:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJfQo7fxlD1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_forward(t_u, t_c, is_train):\n",
        "  with torch.set_grad_enabled(is_train):\n",
        "    t_p = model(t_u, *params)\n",
        "    loss = loss_fn(t_p, t_c)\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0feJZJcmb1z",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "We started this chapter with a big question: how is it that a machine can learn from examples? We spent the rest of the chapter describing the mechanism with which a model can be optimized to fit data. We chose to stick with a simple model in order to see all the moving parts without unneeded complications.\n",
        "\n",
        "Now that we’ve had our fill of appetizers, in chapter 6 we’ll finally get to the main course: using a neural network to fit our data. We’ll work on solving the same thermometer problem, but with the more powerful tools provided by the `torch.nn` module. We’ll adopt the same spirit of using this small problem to illustrate the larger uses of PyTorch. The problem doesn’t need a neural network to reach a solution, but it will allow us to develop a simpler understanding of what’s required to train a neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4AhxzUbpq-3",
        "colab_type": "text"
      },
      "source": [
        "#Exercises\n",
        "\n",
        "1. Redefine the model to be `w2 * t_u ** 2 + w1 * t_u + b`.\n",
        "\n",
        "a. What parts of the training loop, and so on, need to change to accommodate this redefinition?\n",
        "\n",
        " We see that the model is a polynomial function, instead of linear in our last example. This means we have more paramaters to backpropogate - `w2`, `w1`, and `b`. Therefore we will have to change the `params` tensor in the training loop.\n",
        "\n",
        " b.  What parts are agnostic to swapping out the model?\n",
        "\n",
        " The `learning_rate` and the numbers of `epoch`'s will be agnostic to swapping out the model.\n",
        "\n",
        " c. Is the resulting loss higher or lower after training?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F02onctvp4t4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "07f1327b-a969-408f-e21d-b9940200744f"
      },
      "source": [
        "def exercise_model(t_u, w1, w2, b):\n",
        "  return w2 * (t_u ** 2) + w1 * t_u + b\n",
        "\n",
        "def exercise_training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    t_p = exercise_model(t_u, *params)\n",
        "    loss = loss_fn(t_p, t_c)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "      print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "\n",
        "  return params\n",
        "\n",
        "exercise_params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True) #w1, w2, b\n",
        "learning_rate = 1e-2\n",
        "optimizer = optim.Adam([exercise_params], lr=learning_rate)\n",
        "\n",
        "exercise_training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = exercise_params,\n",
        "    t_u = t_u,\n",
        "    t_c = t_c)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 500, Loss 6.108696\n",
            "Epoch 1000, Loss 5.984359\n",
            "Epoch 1500, Loss 5.809034\n",
            "Epoch 2000, Loss 5.586318\n",
            "Epoch 2500, Loss 5.320904\n",
            "Epoch 3000, Loss 5.021268\n",
            "Epoch 3500, Loss 4.701310\n",
            "Epoch 4000, Loss 4.380754\n",
            "Epoch 4500, Loss 4.083410\n",
            "Epoch 5000, Loss 3.832654\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0698,  0.0054, -1.2251], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3GsXqp9xvEO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "3b0474ca-41aa-46f1-b533-aac30b813040"
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "t_p = exercise_model(t_u, *exercise_params)\n",
        "#fig = plt.figure(dpi=600)\n",
        "plt.xlabel(\"Temperature (°Fahrenheit)\")\n",
        "plt.ylabel(\"Temperature (°Celsius)\")\n",
        "plt.plot(t_u.numpy(), t_p.detach().numpy())\n",
        "plt.plot(t_u.numpy(), t_c.numpy(), 'o')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7e4243e240>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIoc-4Ict0DJ",
        "colab_type": "text"
      },
      "source": [
        "The resulting loss is higher after training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cPcSrz1uBl-",
        "colab_type": "text"
      },
      "source": [
        "c. Is the actual result better or worse?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7Vb9ATJuHE6",
        "colab_type": "text"
      },
      "source": [
        "The actual result is worse. This is expected because we are using a polynomial function to fit a linear model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIC5QRhPuVMJ",
        "colab_type": "text"
      },
      "source": [
        "# Summary\n",
        "\n",
        "- Linear models are the simplest reasonable model to use to fit data.\n",
        "- Convex optimization techniques can be used for linear models, but they do not generalize to neural networks, so we focus on stochastic gradient descent for\n",
        "parameter estimation.\n",
        "- Deep learning can be used for generic models that are not engineered for solving a specific task, but instead can be automatically adapted to specialize themselves on the problem at hand.\n",
        "- Learning algorithms amount to optimizing parameters of models based on observations. A loss function is a measure of the error in carrying out a task, such as the error between predicted outputs and measured values. The goal is to get the loss function as low as possible.\n",
        "- The rate of change of the loss function with respect to the model parameters can be used to update the same parameters in the direction of decreasing loss.\n",
        "- The `optim` module in PyTorch provides a collection of ready-to-use optimizers for updating parameters and minimizing loss functions.\n",
        "- Optimizers use the `autograd` feature of PyTorch to compute the gradient for each parameter, depending on how that parameter contributes to the final output. This allows users to rely on the dynamic computation graph during complex forward passes.\n",
        "-Context managers like with `torch.no_grad()`: can be used to control `autograd`’s behavior.\n",
        "-Data is often split into separate sets of *training* samples and *validation* samples. This lets us evaluate a model on data it was not trained on.\n",
        "-Overfitting a model happens when the model’s performance continues to improve on the training set but degrades on the validation set. This is usually due to the model not generalizing, and instead memorizing the desired outputs for the training set."
      ]
    }
  ]
}